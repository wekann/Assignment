{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwpxzrVbCcojk9ciVDl17p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wekann/Assignment/blob/main/CNN_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory"
      ],
      "metadata": {
        "id": "kVxaoG6MvPKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''1. what is a convolutional Neural network (CNN), and why is it used for image processing?\n",
        "A Convolutional Neural Network (CNN) is a type of deep learning neural network designed specifically for processing data with a grid-like topology, such as images. CNNs are inspired by the structure of the human visual cortex and are especially powerful in recognizing spatial hierarchies and patterns in images.\n",
        "\n",
        "Key Components of CNN:\n",
        "\n",
        "1. Convolutional Layers:\n",
        "   Apply filters (kernels) to extract features like edges, textures, and shapes from images.\n",
        "\n",
        "2. ReLU (Activation Function):\n",
        "   Introduces non-linearity to help the network learn complex patterns.\n",
        "\n",
        "3. Pooling Layers:\n",
        "   Reduce the spatial dimensions (e.g., width and height) of the data, helping to reduce computational load and prevent overfitting.\n",
        "\n",
        "4. Fully Connected Layers (Dense Layers):\n",
        "   Make predictions based on features extracted by convolutional and pooling layers.\n",
        "\n",
        "Why CNNs Are Used for Image Processing:\n",
        "\n",
        "1. Automatic Feature Extraction\n",
        "   CNNs automatically learn which features (like corners, lines, textures) are important, eliminating the need for manual feature engineering.\n",
        "\n",
        "2. Parameter Sharing\n",
        "   Filters are shared across the image, which greatly reduces the number of parameters compared to traditional neural networks.\n",
        "\n",
        "3. Translation Invariance\n",
        "   CNNs are robust to slight shifts and distortions in image content—critical for real-world image recognition tasks.\n",
        "\n",
        "4. Hierarchical Learning\n",
        "   Early layers detect simple patterns (edges), while deeper layers recognize complex patterns (faces, objects).\n",
        "\n",
        "Common Applications:\n",
        "* Object and face recognition\n",
        "* Image classification\n",
        "* Medical imaging (e.g., tumor detection)\n",
        "* Self-driving cars (visual input)\n",
        "* Surveillance and security\n",
        "\n",
        "*CNNs are well-suited for image processing because they efficiently capture spatial dependencies, require fewer parameters, and can generalize well to new image data.\n"
      ],
      "metadata": {
        "id": "kNLfT6mbvmiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''2. What are the key components of CNN architecture?\n",
        "A Convolutional Neural Network (CNN) architecture is composed of several layers, each serving a specific purpose in learning and extracting features from images.\n",
        "\n",
        "Key Components of CNN Architecture:\n",
        "1.Input Layer\n",
        "* Receives the raw image data (e.g., a 28×28 grayscale image or a 224×224×3 RGB image).\n",
        "* It reshapes and passes the image data into the network.\n",
        "\n",
        "2. Convolutional Layer\n",
        "* Purpose: Automatically extract features (like edges, corners, textures).\n",
        "* Applies filters (kernels) that slide over the input image to create feature maps.\n",
        "* Each filter detects a different pattern.\n",
        "Example: A 3×3 filter scanning a 28×28 image produces a 26×26 feature map.\n",
        "\n",
        "3. Activation Function (ReLU)\n",
        "* Purpose: Adds non-linearity so the network can learn complex patterns.\n",
        "* The most common activation is ReLU (Rectified Linear Unit):\n",
        "  `ReLU(x) = max(0, x)`\n",
        "\n",
        "4. Pooling Layer (Subsampling or Downsampling)\n",
        "* Purpose: Reduces spatial dimensions (height and width), making computation faster and reducing overfitting.\n",
        "* Common types:\n",
        "  * Max Pooling: Takes the maximum value in a patch.\n",
        "  * Average Pooling: Takes the average value.\n",
        "  > Example: A 2×2 max pooling reduces a 26×26 feature map to 13×13.\n",
        "\n",
        "5. Flatten Layer\n",
        "* Converts the 2D feature maps into a 1D vector.\n",
        "* This is necessary before connecting to the fully connected (dense) layer.\n",
        "\n",
        "6. Fully Connected Layer (Dense Layer)\n",
        "* Purpose: Performs high-level reasoning and classification.\n",
        "* Connects every neuron from the previous layer to every neuron in this layer.\n",
        "* Usually the last few layers of the CNN.\n",
        "\n",
        "7. Output Layer\n",
        "* Produces the final prediction (e.g., class probabilities).\n",
        "* Softmax activation is used for multi-class classification.\n",
        "* Sigmoid is used for binary classification.\n",
        "\n",
        "Optional:\n",
        "* Dropout Layers: Randomly turn off neurons during training to prevent overfitting.\n",
        "* Batch Normalization: Normalizes the inputs to layers to speed up training and improve stability.\n",
        "\n",
        "Table:\n",
        "| Component               | Function                                 |\n",
        "| ----------------------- | ---------------------------------------- |\n",
        "| Input Layer             | Accepts image data                       |\n",
        "| Convolutional Layer     | Extracts features with filters           |\n",
        "| Activation (ReLU)       | Adds non-linearity                       |\n",
        "| Pooling Layer           | Reduces size and computation             |\n",
        "| Flatten Layer           | Prepares data for dense layers           |\n",
        "| Fully Connected Layer   | Learns patterns and makes predictions    |\n",
        "| Output Layer            | Produces final classification results    |\n",
        "| Dropout/BatchNorm (opt) | Prevents overfitting, speeds up training |"
      ],
      "metadata": {
        "id": "SFOnCPgsvjBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''3. what is the role of the convolutional layer in CNNs?\n",
        "\n",
        "The Convolutional Layer is the core building block of a Convolutional Neural Network (CNN). Its main role is to extract meaningful features from the input image by applying a set of learnable filters (also called kernels).\n",
        "\n",
        "Primary Role: Feature Extraction\n",
        "* The convolutional layer applies filters that scan (or convolve over) the input image.\n",
        "* Each filter detects a specific feature such as edges, lines, textures, or shapes.\n",
        "* These filters slide across the image, performing a mathematical operation called convolution, generating a feature map (also known as activation map).\n",
        "\n",
        "How It Works:\n",
        "1. Input: A 2D image (e.g., 28×28 pixels).\n",
        "2. Filter/Kernels: A small matrix (e.g., 3×3 or 5×5) that moves over the image.\n",
        "3. Convolution Operation:\n",
        "   * Multiply the filter with the image region it covers.\n",
        "   * Sum the result to get a single value in the output feature map.\n",
        "4. Feature Map: The output highlights where the filter’s pattern (like an edge) appears in the image.\n",
        "\n",
        "Example:\n",
        "Let’s say we use a 3×3 edge detection filter:\n",
        "Image Patch:     Filter:         Output (1 value):\n",
        "1 1 1            -1 -1 -1\n",
        "0 1 0             0  0  0\n",
        "1 0 1             1  1  1\n",
        "\n",
        "The filter emphasizes horizontal edges and outputs a number that reflects how strong that pattern is at that location.\n",
        "\n",
        "#Key Benefits:\n",
        "* Local Connectivity: Each neuron is connected only to a small region of the input, reducing the number of parameters.\n",
        "* Translation Invariance: The same filter is applied across the entire image, so the network can recognize patterns regardless of their location.\n",
        "* Hierarchical Learning:\n",
        "  * Early layers detect low-level features (edges, colors),\n",
        "  * Deeper layers detect high-level features (faces, objects).\n",
        "\n",
        "In Summary:\n",
        "| Role                   | Description                                 |\n",
        "| ---------------------- | ------------------------------------------- |\n",
        "| Feature Extraction     | Detects edges, shapes, textures, etc.       |\n",
        "| Parameter Efficiency   | Reduces model complexity via shared filters |\n",
        "| Spatial Understanding  | Preserves spatial relationship of pixels    |\n",
        "| Input to Deeper Layers | Passes learned features to next layers      |\n",
        "\n",
        "The convolutional layer is crucial for enabling CNNs to understand images in a structured, efficient, and scalable way.\n"
      ],
      "metadata": {
        "id": "QdpRUH2KwTvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''4. what is a filter(kernel) in CNNs?\n",
        "In Convolutional Neural Networks (CNNs), a filter (also called a kernel) is a small matrix used to scan over the input image to extract specific features such as edges, corners, or textures.\n",
        "\n",
        "Definition:\n",
        "A filter is a set of learnable weights—typically a small-sized matrix (e.g., 3×3, 5×5)—that is applied to the input data via the convolution operation. The result of this operation is a feature map that highlights the presence of a particular pattern in different regions of the image.\n",
        "How It Works:\n",
        "1. The filter slides across the image (from left to right, top to bottom).\n",
        "2. At each position, it performs an element-wise multiplication between the filter and the part of the image it overlaps (called a receptive field), then sums the result.\n",
        "3. This sum becomes one pixel in the output feature map.\n",
        "\n",
        "Example:\n",
        "Suppose you apply a 3×3 filter to a grayscale image:\n",
        "Image Patch:     Filter (Edge Detector):\n",
        "1 2 1            -1 -1 -1\n",
        "0 1 0             0  0  0\n",
        "1 0 1             1  1  1\n",
        "\n",
        "Multiply corresponding values and sum them to get the result at one pixel in the feature map.\n",
        "\n",
        "#Purpose of Filters:\n",
        "Each filter is trained to detect a specific feature:\n",
        "* Edge detection\n",
        "* Texture patterns\n",
        "* Color gradients\n",
        "* Shapes or corners\n",
        "\n",
        "As you go deeper in the CNN:\n",
        "* Early layers detect **simple features** (e.g., vertical lines).\n",
        "* Later layers detect **complex patterns** (e.g., eyes, faces, objects).\n",
        "\n",
        "#Multiple Filters:\n",
        "A CNN uses many filters in each convolutional layer:\n",
        "* Each filter produces one feature map.\n",
        "* Stacking multiple feature maps allows the CNN to learn multiple aspects of the image.\n",
        "\n",
        "Summary:\n",
        "| Term          | Description                                             |\n",
        "| ------------- | ------------------------------------------------------- |\n",
        "| Filter/Kernel | Small matrix of weights used for feature detection      |\n",
        "| Size          | Commonly 3×3, 5×5, etc.                                 |\n",
        "| Operation     | Slides over input to produce feature maps               |\n",
        "| Purpose       | Detect patterns like edges, corners, textures           |\n",
        "| Learnable     | Yes — the values of filters are learned during training |\n",
        "\n",
        "Filters are the eyes of a CNN — they allow the network to see and understand the structure of the image.\n"
      ],
      "metadata": {
        "id": "zoMAY_3Owrg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''5. what is pooling in CNNs, and why is it important?\n",
        "\n",
        "Pooling in Convolutional Neural Networks (CNNs) is a technique used to **reduce the spatial dimensions (height and width) of feature maps while retaining the most important information.\n",
        "\n",
        "What Is Pooling?\n",
        "Pooling is applied after a convolutional layer and works by:\n",
        "* Dividing the input into small regions (e.g., 2×2 or 3×3).\n",
        "* Applying an operation to each region to extract a summary statistic.\n",
        "\n",
        "Types of Pooling:\n",
        "1. Max Pooling (most common)\n",
        "   * Takes the maximum value from each patch.\n",
        "   * Example: From \\[2, 4, 1, 3], max pooling returns 4.\n",
        "\n",
        "2. Average Pooling\n",
        "   * Takes the average value from each patch.\n",
        "   * Example: From \\[2, 4, 1, 3], average pooling returns 2.5.\n",
        "\n",
        "Why Is Pooling Important?\n",
        "| Benefit                            | Explanation                                                                  |\n",
        "| ---------------------------------- | ---------------------------------------------------------------------------- |\n",
        "| Reduces Dimensionality             | Shrinks the size of feature maps, reducing computation and memory usage.     |\n",
        "| Retains Key Features               | Keeps the most important information (e.g., strongest features in a region). |\n",
        "| Improves Efficiency                | Fewer parameters → faster training and inference.                            |\n",
        "| Adds Translation Invariance        | Makes the model more robust to small movements/shifts in the image.          |\n",
        "| Prevents Overfitting               | Acts as a form of regularization by simplifying representations.             |\n",
        "\n",
        "Example: Max Pooling 2×2\n",
        "Suppose you apply 2×2 max pooling to a 4×4 feature map:\n",
        "\n",
        "Original Feature Map:      After Max Pooling (2×2):\n",
        "[1, 3, 2, 4]                [3, 4]\n",
        "[5, 6, 1, 2]      →         [8, 7]\n",
        "[4, 8, 7, 1]\n",
        "[3, 5, 6, 0]\n",
        "\n",
        "In Summary:\n",
        "| Term            | Description                                                           |\n",
        "| --------------- | --------------------------------------------------------------------- |\n",
        "| Pooling         | Downsampling technique to reduce spatial dimensions                   |\n",
        "| Max Pooling     | Keeps the most prominent feature                                      |\n",
        "| Average Pooling | Averages feature values in each region                                |\n",
        "| Importance      | Reduces size, computation, overfitting, and boosts feature robustness |\n",
        "\n",
        "Pooling is a crucial step in CNNs that helps models focus on the most relevant information, while being more efficient and generalizable.\n"
      ],
      "metadata": {
        "id": "oR3IGsz3xNCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''6. what are the common types oof pooling used in CNNs?\n",
        "Pooling is a downsampling technique used in CNNs to reduce the spatial size of feature maps and retain important features. The most common types of pooling are:\n",
        "1. Max Pooling (Most Common)\n",
        "\n",
        "* How it works: Selects the maximum value from each patch of the feature map.\n",
        "* Purpose: Captures the most prominent feature in a region.\n",
        "\n",
        "Example (2×2 Max Pooling):\n",
        "Patch:\n",
        "[2, 4]\n",
        "[1, 3]\n",
        "→ Output: 4\n",
        "\n",
        "Advantages:\n",
        "* Helps preserve sharp features.\n",
        "* Reduces computation and overfitting.\n",
        "\n",
        "2. Average Pooling\n",
        "* How it works: Calculates the average value of each patch.\n",
        "* Purpose: Gives a more generalized or smoothed representation.\n",
        "Example (2×2 Average Pooling):\n",
        "Patch:\n",
        "[2, 4]\n",
        "[1, 3]\n",
        "→ Output: (2+4+1+3)/4 = 2.5\n",
        "\n",
        "Used when: You want a more generalized representation of the features.\n",
        "\n",
        "3. Global Max Pooling\n",
        "* How it works: Takes the maximum value from the entire feature map.\n",
        "* Used in: Replacing fully connected layers at the end of CNNs for classification.\n",
        "\n",
        "4. Global Average Pooling\n",
        "* How it works: Averages all values in the entire feature map.\n",
        "* Used in: Modern architectures (e.g., GoogleNet, ResNet) to reduce overfitting and replace dense layers.\n",
        "\n",
        "Comparison Table:\n",
        "| Pooling Type           | Operation           | Output              | Common Use                           |\n",
        "| ---------------------- | ------------------- | ------------------- | ------------------------------------ |\n",
        "| Max Pooling            | Max value in patch  | Sharp, key features | Most CNNs for feature detection      |\n",
        "| Average Pooling        | Avg. value in patch | Smoothed values     | Simpler or noise-tolerant tasks      |\n",
        "| Global Max Pooling     | Max of entire map   | Single max value    | Model simplification, classification |\n",
        "| Global Average Pooling | Avg. of entire map  | Single avg value    | Reduces overfitting, final layers    |\n",
        "\n",
        "Summary:\n",
        "Pooling layers help CNNs become faster, more robust, and less prone to overfitting by reducing the size of feature maps while keeping key information. Among these, Max Pooling is the most widely used due to its ability to preserve prominent features effectively.\n"
      ],
      "metadata": {
        "id": "t_WSmiYaxo46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q7. How does the backpropagation algorithm work in CNNs?\n",
        "\n",
        "Backpropagation in Convolutional Neural Networks (CNNs) is the process used to train the network by adjusting the weights (including those of filters/kernels) to minimize the prediction error. It's an extension of the same algorithm used in standard neural networks, but adapted to the convolutional and pooling operations.\n",
        "\n",
        "Main Goal of Backpropagation:\n",
        "To compute the gradient of the loss function with respect to each weight in the network and use that gradient to update the weights using optimization techniques like Stochastic Gradient Descent (SGD).\n",
        "Backpropagation Steps in CNNs:\n",
        "\n",
        "1. Forward Pass\n",
        "* The input image is passed through convolutional layers, activation functions (e.g., ReLU), pooling layers, and fully connected layers.\n",
        "* The final output is compared to the true label using a loss function (e.g., cross-entropy).\n",
        "\n",
        "2. Compute Loss\n",
        "* The loss function calculates how far the predicted output is from the actual label.\n",
        "* Example:\n",
        "  `Loss = CrossEntropy(predicted, actual)`\n",
        "\n",
        "3. Backward Pass (Gradient Computation)\n",
        "This is done layer by layer in reverse:\n",
        "a. Output Layer to Last Fully Connected Layer\n",
        "* Calculate the gradient of the loss w\\.r.t. the weights and activations.\n",
        "* Apply chain rule of calculus to propagate the error backward.\n",
        "\n",
        "b. Fully Connected Layers\n",
        "* Compute gradients w\\.r.t. weights, biases, and input activations.\n",
        "\n",
        "c. Pooling Layers\n",
        "* Max Pooling: Only the max value in the window contributes to the gradient.\n",
        "* Average Pooling: The gradient is equally distributed across all elements in the window.\n",
        "\n",
        "d. Convolutional Layers\n",
        "* Compute the gradient of the loss w\\.r.t.:\n",
        "  * Filter weights (kernels): By convolving the input with the backpropagated error.\n",
        "  * Input feature maps: So errors can be passed further back to earlier layers.\n",
        "\n",
        "4. Update Weights\n",
        "Use an optimizer (e.g., SGD, Adam) to update all learnable parameters:\n",
        "  weight = weight - learning_rate × gradient\n",
        "  ```\n",
        "Special Considerations in CNNs:\n",
        "| Component         | Gradient Flow                                |\n",
        "| ----------------- | -------------------------------------------- |\n",
        "| Convolution Layer | Gradient w\\.r.t. filter weights & input      |\n",
        "| Pooling Layer     | Special handling (e.g., mask in max pooling) |\n",
        "| ReLU Activation   | Passes gradients only if input > 0           |\n",
        "\n",
        "In Summary:\n",
        "| Step          | Description                                          |\n",
        "| ------------- | ---------------------------------------------------- |\n",
        "| Forward Pass  | Compute output and loss                              |\n",
        "| Compute Loss  | Measure difference between predicted and true values |\n",
        "| Backward Pass | Calculate gradients using chain rule                 |\n",
        "| Weight Update | Adjust weights using gradients and learning rate     |\n",
        "\n",
        "Backpropagation Enables Learning\n",
        "\n",
        "Backpropagation allows CNNs to learn from data by gradually adjusting filters and weights so that the network improves its predictions over time."
      ],
      "metadata": {
        "id": "hrrCHS_81wzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q8. What is the role of activation functions in CNNs?\n",
        "Activation functions in CNNs introduce non-linearity into the model, allowing it to learn and represent complex patterns in data.\n",
        "\n",
        "Main Role:\n",
        "To transform the output of neurons in each layer so the CNN can:\n",
        "* Model non-linear relationships\n",
        "* Learn complex features (not just straight lines)\n",
        "* Make better predictions on tasks like image classification\n",
        "Without activation functions, the CNN would behave like a linear model, no matter how many layers it has — and would fail to learn complex tasks.\n",
        "\n",
        "#How It Works:\n",
        "Each neuron (after convolution or fully connected layer) outputs a value. The activation function then transforms this value before passing it to the next layer.\n",
        "\n",
        "Common Activation Functions in CNNs:\n",
        "| Activation Function | Formula                            | Characteristics                                    |\n",
        "| ------------------- | ---------------------------------- | -------------------------------------------------- |\n",
        "| ReLU                | `f(x) = max(0, x)`                 | Most common; fast and effective; sparsity          |\n",
        "| Sigmoid             | `f(x) = 1 / (1 + e^(-x))`          | Smooth, used in binary classification              |\n",
        "| Tanh                | `f(x) = (e^x - e^-x)/(e^x + e^-x)` | Outputs between -1 and 1                           |\n",
        "| Leaky ReLU          | `f(x) = x if x>0 else αx`          | Fixes ReLU’s “dead neuron” problem                 |\n",
        "| Softmax             | `f(x_i) = e^(x_i)/Σe^(x_j)`        | Used in final layer for multi-class classification |\n",
        "\n",
        "Why ReLU is the Default in CNNs:\n",
        "* Simple and fast to compute\n",
        "* Introduces non-linearity efficiently\n",
        "* Helps avoid vanishing gradient problem\n",
        "* Sparse activation (many outputs are 0), improving efficiency\n",
        "\n",
        "Where Are They Used in CNNs?\n",
        "* After each convolutional layer\n",
        "* After fully connected (dense) layers\n",
        "* In the output layer, depending on the task:\n",
        "  * Softmax → Multi-class classification\n",
        "  * Sigmoid → Binary classification\n",
        "\n",
        "In Summary:\n",
        "| Role                    | Explanation                                   |\n",
        "| ----------------------- | --------------------------------------------- |\n",
        "| Introduce Non-Linearity | Allow CNNs to learn complex patterns          |\n",
        "| Enable Deep Learning    | Without them, the network behaves linearly    |\n",
        "| Control Output Range    | E.g., sigmoid between 0–1; tanh between -1–1  |\n",
        "| Task-Specific Behavior  | Softmax or sigmoid for classification outputs |\n",
        "\n",
        "Activation functions are crucial to the power of CNNs — they give the model the ability to understand images in a deep, non-linear way.\n"
      ],
      "metadata": {
        "id": "-3lg2CUo11DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q9. What is the concept of receptive fields in CNNs?\n",
        "The receptive field in a Convolutional Neural Network (CNN) refers to the region of the input image that a particular neuron (or feature) in a CNN layer is \"looking at\" or influenced by.\n",
        "Simple Definition:\n",
        "A receptive field is the area of the input image that affects the output of a neuron in a particular CNN layer.\n",
        "\n",
        "Why It Matters:\n",
        "* It defines how much context a neuron has to make a decision.\n",
        "* In early layers, the receptive field is small (e.g., 3×3).\n",
        "* In deeper layers, neurons have a larger receptive field — they can \"see\" more of the image and detect higher-level features (e.g., faces, objects).\n",
        "\n",
        "How It Grows:\n",
        "The receptive field increases with each layer, depending on:\n",
        "* Kernel (filter) size\n",
        "* Stride\n",
        "* Padding\n",
        "* Number of layers\n",
        "\n",
        "For example:\n",
        "\n",
        "* Layer 1: 3×3 kernel → Receptive field = 3×3\n",
        "* Layer 2 (on top of Layer 1): another 3×3 → Receptive field = 5×5\n",
        "* Layer 3: another 3×3 → Receptive field = 7×7\n",
        "* ...and so on.\n",
        "\n",
        "This stacking effect allows neurons in deeper layers to capture global context.\n",
        "Example Analogy:\n",
        "* Think of the **receptive field like a window** through which a neuron sees the input image.\n",
        "* A small window = local details (edges, textures)\n",
        "* A large window = broader context (eyes, faces, shapes)\n",
        "\n",
        "In Summary:\n",
        "| Term            | Description                                               |\n",
        "| --------------- | --------------------------------------------------------- |\n",
        "| Receptive Field | Region of input image influencing a neuron's output       |\n",
        "| Small RF        | Early layers → detect edges, colors, textures             |\n",
        "| Large RF        | Deeper layers → detect shapes, objects, semantics         |\n",
        "| Importance      | Helps the network learn **context-aware** representations |\n",
        "\n",
        "Note:Designing CNN architectures with the right receptive field size is key — too small, and the model can’t understand large patterns; too big too soon, and it might miss fine details."
      ],
      "metadata": {
        "id": "DEBoUTYJ2IjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q10. Explain the concept of tensor space in CNNs\n",
        "In Convolutional Neural Networks (CNNs), tensor space refers to the multi-dimensional structure used to represent data — such as images, feature maps, and weights — throughout the network. Tensors are the core data format that CNNs use to store and process information.\n",
        "\n",
        "What is a Tensor?\n",
        "A tensor is a generalization of scalars, vectors, and matrices to higher dimensions:\n",
        "| Type      | Example                            | Dimensions |\n",
        "| --------- | ---------------------------------- | ---------- |\n",
        "| Scalar    | 5                                  | 0D         |\n",
        "| Vector    | \\[5, 10, 15]                       | 1D         |\n",
        "| Matrix    | \\[\\[1, 2], \\[3, 4]]                | 2D         |\n",
        "| 3D Tensor | Multiple matrices (like RGB image) | 3D         |\n",
        "| nD Tensor | Data with more dimensions          | nD         |\n",
        "\n",
        "Why Tensors in CNNs?\n",
        "CNNs handle data in multiple dimensions, such as:\n",
        "\n",
        "* Image height and width\n",
        "* Color channels (RGB)\n",
        "* Batch size (number of images)\n",
        "* Feature maps (output of layers)\n",
        "\n",
        "All this data is efficiently represented using tensors, allowing deep learning libraries like TensorFlow or PyTorch to handle complex operations easily.\n",
        "Example: Image as a Tensor\n",
        "A single RGB image of size 32×32 pixels:\n",
        "Tensor shape: [Height, Width, Channels] = [32, 32, 3]\n",
        "A **batch of 64 such images** becomes:\n",
        "Tensor shape: [Batch, Height, Width, Channels] = [64, 32, 32, 3]\n",
        "\n",
        "Tensors in CNN Layers:\n",
        "| Layer Type            | Input Tensor Shape              | Output Tensor Shape           |\n",
        "| --------------------- | ------------------------------- | ----------------------------- |\n",
        "| Input Layer           | `[Batch, H, W, Channels]`       | Same                          |\n",
        "| Convolution Layer     | Applies filters                 | `[Batch, H', W', Filters]`    |\n",
        "| Pooling Layer         | Downsamples input               | `[Batch, H'', W'', Channels]` |\n",
        "| Fully Connected Layer | Flattens to `[Batch, Features]` | `[Batch, Classes]`            |\n",
        "\n",
        "Tensor Space in CNNs = Multidimensional Feature Representation\n",
        "As data flows through the CNN:\n",
        "* Tensors change shape based on operations (e.g., convolutions, pooling, flattening).\n",
        "* Each tensor layer represents the state of the image features at a different level of abstraction.\n",
        "* The tensor space is the evolving multidimensional space in which all learning and transformations happen.\n",
        "\n",
        "In Summary:\n",
        "| Concept      | Description                                             |\n",
        "| ------------ | ------------------------------------------------------- |\n",
        "| Tensor       | A multi-dimensional array used to store data            |\n",
        "| Tensor Space | All possible values and shapes tensors can take in CNNs |\n",
        "| Use in CNNs  | Represent images, filters, feature maps, weights, etc.  |\n",
        "| Importance   | Enables efficient deep learning operations at scale     |\n",
        "Understanding tensor space is essential for building, debugging, and optimizing CNN models — it’s the language of data in deep learning.\n"
      ],
      "metadata": {
        "id": "VSdGBdxr2cXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q11. What is LeNet-5, and how does it contribute to the development of CNNs?\n",
        "\n",
        "LeNet-5 is one of the earliest and most influential Convolutional Neural Networks (CNNs), developed by Yann LeCun and his team in 1998 for handwritten digit recognition, especially for the MNIST dataset.\n",
        "\n",
        "What is LeNet-5?\n",
        "LeNet-5 is a 7-layer CNN architecture (excluding input) that introduced many of the core ideas still used in modern CNNs today. It was designed to classify digits (0–9) from grayscale images of size 32×32 pixels.\n",
        "\n",
        "LeNet-5 Architecture Overview:\n",
        "| Layer  | Type                      | Output Shape | Description                     |\n",
        "| ------ | ------------------------- | ------------ | ------------------------------- |\n",
        "| Input  | Image                     | 32×32×1      | Grayscale input image           |\n",
        "| C1     | Convolutional             | 28×28×6      | 6 filters of size 5×5           |\n",
        "| S2     | Subsampling (Avg Pooling) | 14×14×6      | Downsampling                    |\n",
        "| C3     | Convolutional             | 10×10×16     | Deeper feature extraction       |\n",
        "| S4     | Subsampling               | 5×5×16       | Another pooling layer           |\n",
        "| C5     | Fully Connected Conv      | 1×1×120      | Connects to all 5×5×16 features |\n",
        "| F6     | Fully Connected           | 84           | Classic dense layer             |\n",
        "| Output | Softmax                   | 10           | Digit classification (0–9)      |\n",
        "\n",
        "Key Innovations in LeNet-5:\n",
        "\n",
        "1. Introduced Convolutional Layers\n",
        "   – Filters learned edge-like patterns automatically from data.\n",
        "\n",
        "2. Used Pooling/Subsampling Layers\n",
        "   – Reduced dimensionality while retaining important features.\n",
        "\n",
        "3. Stacked Layers for Hierarchical Feature Learning\n",
        "   – Low-level features (edges) → mid-level (shapes) → high-level (digits).\n",
        "\n",
        "4. Used Backpropagation\n",
        "   – For training weights across all layers, including convolutional.\n",
        "\n",
        "5. Combined CNN with Fully Connected Layers\n",
        "   – For robust classification.\n",
        "\n",
        "Contribution to CNN Development:\n",
        "| Contribution            | Impact                                                                   |\n",
        "| ------------------------| ------------------------------------------------------------------------ |\n",
        "|  Early CNN Model        | Paved the way for modern deep learning models                            |\n",
        "|  Digit Recognition      | Achieved state-of-the-art results on MNIST                               |\n",
        "|  Architectural Template | Inspired networks like AlexNet, VGG, ResNet                              |\n",
        "|  Modular Design         | Introduced clear blocks: Conv → Pool → FC → Softmax                      |\n",
        "|  Real-World Use Case    | Used in early OCR (Optical Character Recognition) systems (e.g., checks) |\n",
        "\n",
        "#Visual Summary of LeNet-5:\n",
        "Input (32×32) → [C1: Conv] → [S2: Pool] → [C3: Conv] → [S4: Pool]\n",
        "→ [C5: FC Conv] → [F6: FC] → [Output: Softmax]\n",
        "\n",
        "In Summary:\n",
        "| Feature      | Details                                 |\n",
        "| ------------ | --------------------------------------- |\n",
        "| Model Name   | LeNet-5                                 |\n",
        "| Inventor     | Yann LeCun (1998)                       |\n",
        "| Purpose      | Digit recognition (MNIST dataset)       |\n",
        "| Key Layers   | Conv, Pooling, Fully Connected, Softmax |\n",
        "| Contribution | Foundation of modern CNNs               |\n",
        "\n",
        "LeNet-5 was a breakthrough in computer vision and is still taught today as the starting point for understanding CNNs.\n"
      ],
      "metadata": {
        "id": "vxw_5dL12t-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q12. What is AlexNet, and why was it a breakthrough in deep learning?\n",
        "What is AlexNet?\n",
        "AlexNet is a deep Convolutional Neural Network (CNN) architecture developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, which won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 by a huge margin.\n",
        "It was designed to classify images into 1,000 categories using the ImageNet dataset, which contains over 1.2 million images.\n",
        "\n",
        "Key Features of AlexNet:\n",
        "| Component        | Description                                         |\n",
        "| ---------------- | --------------------------------------------------- |\n",
        "| Input            | 227×227×3 (RGB image)                               |\n",
        "| Conv Layers      | 5 convolutional layers to extract image features    |\n",
        "| ReLU Activations | Introduced ReLU (faster training than sigmoid/tanh) |\n",
        "| Max Pooling      | Used for downsampling                               |\n",
        "| Fully Connected  | 3 dense layers to classify image features           |\n",
        "| Dropout          | Regularization to prevent overfitting               |\n",
        "| GPU Training     | Trained using 2 GPUs in parallel (NVIDIA GTX 580)   |\n",
        "| Output           | Softmax over 1000 classes                           |\n",
        "\n",
        "Architecture Overview (Simplified):\n",
        "```\n",
        "Input → Conv1 → ReLU → MaxPool → Conv2 → ReLU → MaxPool →\n",
        "Conv3 → ReLU → Conv4 → ReLU → Conv5 → ReLU → MaxPool →\n",
        "FC1 → Dropout → FC2 → Dropout → FC3 → Softmax\n",
        "``\n",
        "Why Was AlexNet a Breakthrough?\n",
        "| Reason                      | Description                                                                 |\n",
        "| ----------------------------| --------------------------------------------------------------------------- |\n",
        "| ImageNet Victory            | Outperformed traditional methods by 10.8% top-5 error rate                  |\n",
        "| ReLU Activation             | Allowed faster training and better gradient flow than sigmoid/tanh          |\n",
        "| GPU Training                | Trained on 2 GPUs, enabling deeper architecture to be practically used      |\n",
        "| Deep Architecture           | 8 layers with 60 million parameters                                         |\n",
        "| Dropout for Regularization  | Helped prevent overfitting in dense layers                                  |\n",
        "| Popularized Deep Learning   | Sparked global interest in CNNs and deep learning research and applications |\n",
        "\n",
        "Impact of AlexNet on Deep Learning:\n",
        "* Triggered the deep learning revolution in computer vision.\n",
        "* Led to development of more powerful architectures like VGG, GoogLeNet, ResNet.\n",
        "* Widely adopted in industry and research (e.g., facial recognition, medical imaging, autonomous vehicles).\n",
        "\n",
        "In Summary:\n",
        "| Feature             | AlexNet Impact                           |\n",
        "| ------------------- | ---------------------------------------- |\n",
        "| Inventors           | Krizhevsky, Sutskever, Hinton (2012)     |\n",
        "| Dataset             | ImageNet (1.2M images, 1000 classes)     |\n",
        "| Architecture        | 8 layers (Conv + FC + Softmax)           |\n",
        "| Breakthrough Reason | Deep architecture + GPU + ReLU + Dropout |\n",
        "| Legacy              | Sparked modern deep learning movement    |\n",
        "\n",
        "AlexNet proved that deep CNNs trained with GPUs could dramatically outperform traditional computer vision methods — and laid the foundation for today's AI-powered vision systems.\n"
      ],
      "metadata": {
        "id": "ugUGdpW63I_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q13. What is VGGNet, and how does it differ from AlexNet?\n",
        "\n",
        "What is VGGNet?\n",
        "VGGNet is a deep Convolutional Neural Network developed by the Visual Geometry Group (VGG) at the University of Oxford, led by Karen Simonyan and Andrew Zisserman, and introduced in 2014.\n",
        "It gained fame for achieving excellent performance on the ImageNet Challenge (ILSVRC 2014) and is known for its simplicity and depth.\n",
        "\n",
        "Key Characteristics of VGGNet:\n",
        "| Feature                | Details                                                        |\n",
        "| ---------------------- | -------------------------------------------------------------- |\n",
        "| Versions               | VGG-11, VGG-13, VGG-16, VGG-19 (based on number of layers)     |\n",
        "| Input Image Size       | 224×224×3 (RGB)                                                |\n",
        "| Convolutional Layers   | Uses only 3×3 filters with stride 1                            |\n",
        "| Pooling                | 2×2 max pooling with stride 2                                  |\n",
        "| Fully Connected Layers | 2 or 3 dense layers at the end                                 |\n",
        "| Activation Function    | ReLU                                                           |\n",
        "| Parameters             | VGG-16 has \\~138 million parameters                            |\n",
        "\n",
        "Architecture Overview of VGG-16:\n",
        "```\n",
        "Input → [Conv3×3 ×2] → MaxPool → [Conv3×3 ×2] → MaxPool →\n",
        "[Conv3×3 ×3] → MaxPool → [Conv3×3 ×3] → MaxPool →\n",
        "[Conv3×3 ×3] → MaxPool → FC → FC → Softmax\n",
        "```\n",
        "Difference Between VGGNet and AlexNet:\n",
        "\n",
        "| Feature            | AlexNet (2012)                  | VGGNet (2014)                 |\n",
        "| ------------------ | ------------------------------- | ----------------------------- |\n",
        "| Year               | 2012                            | 2014                          |\n",
        "| Depth              | 8 layers                        | 16 or 19 layers               |\n",
        "| Filter Size        | Varies (11×11, 5×5, 3×3)        | Consistent 3×3 filters        |\n",
        "| Stride & Padding   | Stride = 4 (early layers)       | Stride = 1, Padding = same    |\n",
        "| Pooling            | Max Pooling (overlapping)       | Max Pooling (non-overlapping) |\n",
        "| Architecture Style | Mixed layer types, less regular | Very uniform and simple       |\n",
        "| Parameters         | \\~60 million                    | \\~138 million (VGG-16)        |\n",
        "| Training GPUs      | 2 GPUs                          | Single GPU (with more memory) |\n",
        "| Performance        | Top-5 error: \\~15.3%            | Top-5 error: \\~7.3% (VGG-16)  |\n",
        "\n",
        "Why VGGNet Was Important:\n",
        "* Proved that deeper networks with small filters improve performance.\n",
        "* Its modular design made it easy to generalize and extend.\n",
        "* Inspired future architectures like ResNet, Inception, and MobileNet.\n",
        "\n",
        "Summary:\n",
        "| Question                | Answer                                               |\n",
        "| ----------------------- | ---------------------------------------------------- |\n",
        "| What is VGGNet?         | A deep CNN with many small (3×3) filters             |\n",
        "| Invented By             | Oxford VGG Group (2014)                              |\n",
        "| Difference from AlexNet | Deeper, simpler, smaller filters, better performance |\n",
        "| Legacy                  | Foundation for modern CNN designs                    |\n",
        "\n",
        "VGGNet showed that depth + simplicity can significantly improve CNN performance — and became a benchmark model in computer vision research and applications.\n"
      ],
      "metadata": {
        "id": "j2e4oqHt3dtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q14. What is GoogLeNet, and what is its main innovation?\n",
        "What is GoogLeNet?\n",
        "GoogLeNet is a deep Convolutional Neural Network architecture developed by Google researchers (Szegedy et al.) and introduced in 2014. It won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2014)** with a top-5 error rate of 6.67%, outperforming models like VGGNet.\n",
        "\n",
        "It is also known as Inception-v1, the first in a series of \"Inception\" networks.\n",
        "Main Innovation: The Inception Module\n",
        "The core idea behind GoogLeNet is the Inception module, which allows the network to analyze visual information at multiple scales simultaneously.\n",
        "\n",
        "Inception Module Includes:\n",
        "* 1×1 convolutions (for dimensionality reduction and computational efficiency)\n",
        "* 3×3 convolutions\n",
        "* 5×5 convolutions\n",
        "* 3×3 max pooling\n",
        "\n",
        "All of these run in parallel and their outputs are concatenated along the depth dimension.\n",
        "\n",
        "Architecture Highlights:\n",
        "| Feature                | Description                                     |\n",
        "| ---------------------- | ----------------------------------------------- |\n",
        "| Depth                  | 22 layers (deeper than VGG and AlexNet)         |\n",
        "| Parameter Efficiency   | \\~5 million parameters (compared to VGG’s 138M) |\n",
        "| Inception Module       | Processes input at multiple scales              |\n",
        "| Auxiliary Classifiers  | Added during training to improve gradient flow  |\n",
        "| Global Average Pooling | Replaces final fully connected layers           |\n",
        "\n",
        "How GoogLeNet Differs from Earlier Models:\n",
        "| Feature        | GoogLeNet                             | Earlier Models (AlexNet, VGG)      |\n",
        "| -------------- | ------------------------------------- | ---------------------------------- |\n",
        "| Depth          | 22 layers                             | 8–19 layers                        |\n",
        "| Key Innovation | Inception module                      | Stacked conv + pooling             |\n",
        "| FC Layers      | Removed (used Global Average Pooling) | Multiple dense FC layers           |\n",
        "| Parameters     | \\~5 million                           | VGG: \\~138 million                 |\n",
        "| Efficiency     | High accuracy with fewer parameters   | High memory and computational cost |\n",
        "\n",
        "Why It Was a Breakthrough:\n",
        "* Multi-scale feature extraction in a single module\n",
        "* Reduced computation with 1×1 convolutions\n",
        "* No fully connected layers → smaller model, faster training\n",
        "* Inspired Inception-v2, v3, v4, and Inception-ResNet\n",
        "\n",
        "#In Summary:\n",
        "| Item            | Detail                                        |\n",
        "| --------------- | --------------------------------------------- |\n",
        "| Model Name      | GoogLeNet (Inception-v1)                      |\n",
        "| Year Introduced | 2014                                          |\n",
        "| Innovation      | Inception module (multi-scale parallel paths) |\n",
        "| Layers          | 22 deep                                       |\n",
        "| Parameter Count | \\~5 million                                   |\n",
        "| Impact          | High accuracy + efficiency; scalable design   |\n",
        "\n",
        "GoogLeNet marked a major step forward in creating deep, efficient, and scalable neural networks, paving the way for many advanced architectures in modern computer vision.\n"
      ],
      "metadata": {
        "id": "_QtRWlq332Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q15. What is ResNet, and what problem does it solve?\n",
        "What is ResNet?\n",
        "ResNet (Residual Network) is a deep convolutional neural network architecture introduced by Kaiming He et al.from Microsoft Research in 2015, which won the ImageNet Challenge (ILSVRC 2015) with a top-5 error of 3.57%—surpassing human-level performance on classification.\n",
        "The Problem ResNet Solves:\n",
        "As neural networks grow deeper, they should, in theory, perform better. But in practice, very deep networks often suffer from:\n",
        "| Problem                             | Description                                                              |\n",
        "| ----------------------------------- | ------------------------------------------------------------------------ |\n",
        "| Degradation Problem                 | Accuracy gets worse as layers are added (not due to overfitting).        |\n",
        "| Vanishing/Exploding Gradients       | Gradients become too small or large during backpropagation in deep nets. |\n",
        "\n",
        "Despite having enough data and computing power, training deep models was ineffective beyond a certain depth due to these issues.\n",
        "\n",
        "ResNet's Key Innovation: Residual Learning\n",
        "ResNet solves the degradation problem by introducing **skip connections (also called residual connections).\n",
        "\n",
        "Residual Block:\n",
        "Instead of learning a direct mapping `H(x)`, ResNet learns a residual function:\n",
        "\n",
        "$$\n",
        "H(x) = F(x) + x\n",
        "$$\n",
        "\n",
        "Where:\n",
        "* `x` = input\n",
        "* `F(x)` = output of a few stacked layers\n",
        "* `x` is added back to `F(x)` via a shortcut connection\n",
        "\n",
        "This helps gradients flow more easily during training and allows the network to learn identity mappings, effectively bypassing unnecessary layers if needed.\n",
        "ResNet Architecture Overview:\n",
        "| Model Variant | Number of Layers |\n",
        "| ------------- | ---------------- |\n",
        "| ResNet-18     | 18 layers        |\n",
        "| ResNet-34     | 34 layers        |\n",
        "| ResNet-50     | 50 layers        |\n",
        "| ResNet-101    | 101 layers       |\n",
        "| ResNet-152    | 152 layers       |\n",
        "\n",
        "All versions use residual blocks as the core building unit.\n",
        "\n",
        "Comparison with Earlier Models:\n",
        "| Feature        | ResNet                        | VGG / AlexNet                  |\n",
        "| -------------- | ----------------------------- | ------------------------------ |\n",
        "| Depth          | Up to 152+ layers             | 8–19 layers                    |\n",
        "| Key Innovation | Residual (skip) connections   | Stacked convolution layers     |\n",
        "| Gradient Flow  | Improved                      | Weak in very deep networks     |\n",
        "| Training Ease  | Easier for deep networks      | Difficult beyond certain depth |\n",
        "| Performance    | Top-1 error \\~20% (ResNet-50) | Top-1 \\~25–30% (VGG/AlexNet)   |\n",
        "\n",
        "In Summary:\n",
        "| Feature        | Description                                                                        |\n",
        "| -------------- | ---------------------------------------------------------------------------------- |\n",
        "| Full Name      | ResNet (Residual Network)                                                          |\n",
        "| Introduced By  | Kaiming He et al. (Microsoft Research) in 2015                                     |\n",
        "| Solved Problem | Degradation in deep networks                                                       |\n",
        "| Key Idea       | Skip/residual connections                                                          |\n",
        "| Depth Achieved | 18 to 152+ layers                                                                  |\n",
        "| Impact         | Set the foundation for ultra-deep architectures (e.g., transformers, EfficientNet) |\n"
      ],
      "metadata": {
        "id": "BPsE674X4nOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q16. What is DenseNet, and how does it differ from ResNet?\n",
        "What is DenseNet?\n",
        "DenseNet (Dense Convolutional Network) is a deep learning architecture introduced by Gao Huang et al. in 2017, designed to improve information flow and feature reuse in very deep neural networks.\n",
        "DenseNet connects each layer to every other layer in a feed-forward manner, unlike traditional CNNs where layers are connected sequentially.\n",
        "\n",
        "Key Concept: Dense Connectivity\n",
        "In DenseNet, every layer receives input from all previous layers and passes its own output to all subsequent layers.\n",
        "Mathematically:\n",
        "$$\n",
        "x_l = H_l([x_0, x_1, x_2, ..., x_{l-1}])\n",
        "$$\n",
        "\n",
        "Where:\n",
        "* $x_l$ = output of the *l*-th layer\n",
        "* $H_l$ = composite function (e.g., BatchNorm → ReLU → Conv)\n",
        "* $[x_0, x_1, ..., x_{l-1}]$ = concatenation of feature maps from all previous layers\n",
        "\n",
        "This encourages feature reuse, efficient gradient flow, and compact models.\n",
        "DenseNet Architecture Features:\n",
        "| Component              | Description                                         |\n",
        "| ---------------------- | --------------------------------------------------- |\n",
        "| Dense Blocks           | Groups of layers with dense connections             |\n",
        "| Transition Layers      | 1×1 conv + pooling between dense blocks             |\n",
        "| Composite Function     | BN → ReLU → Conv                                    |\n",
        "| Growth Rate (k)        | Number of feature maps each layer adds (e.g., k=32) |\n",
        "| Depth Options          | DenseNet-121, 169, 201, 264 (number = total layers) |\n",
        "\n",
        "Difference Between DenseNet and ResNet:\n",
        "\n",
        "| Feature         | ResNet                                | DenseNet                                         |\n",
        "| --------------- | ------------------------------------- | ------------------------------------------------ |\n",
        "| Year Introduced | 2015                                  | 2017                                             |\n",
        "| Key Idea        | Residual/Skip connections: $x + F(x)$ | Dense connections: concatenate all previous maps |\n",
        "| Connection Type | Additive skip connections             | Concatenation of feature maps                    |\n",
        "| Feature Reuse   | Limited (only previous layer reused)  | High (all previous layers reused)                |\n",
        "| Gradient Flow   | Improved                              | Even better due to direct connections            |\n",
        "| Model Size      | Large                                 | More compact due to feature reuse                |\n",
        "| Redundancy      | May learn redundant features          | Less redundancy, more efficient                  |\n",
        "\n",
        "#Performance & Efficiency:\n",
        "* DenseNet typically achieves better accuracy than ResNet with fewer parameters.\n",
        "* It reduces overfitting by reusing features.\n",
        "* However, due to concatenation, it may require more memory and slightly more complex implementation.\n",
        "\n",
        "In Summary:\n",
        "| Aspect             | DenseNet                                                   |\n",
        "| ------------------ | ---------------------------------------------------------- |\n",
        "| Full Name          | Dense Convolutional Network                                |\n",
        "| Proposed By        | Gao Huang et al., 2017                                     |\n",
        "| Key Feature        | Dense connectivity (each layer connected to all before it) |\n",
        "| Advantage          | Efficient feature reuse, compact models                    |\n",
        "| Compared to ResNet | Better gradient flow, fewer parameters, more connections   |"
      ],
      "metadata": {
        "id": "5eCByY6d45-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "87007460-af60-446b-923e-11474fdf9345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-1-2658741797.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1-2658741797.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    '''Q16. What is DenseNet, and how does it differ from ResNet?\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q17. What are the main steps involved in training a Convolutional Neural Network (CNN) from scratch?\n",
        "\n",
        "Training a CNN from scratch involves a structured pipeline to ensure the model learns effectively from raw data. Here's a step-by-step breakdown:\n",
        "Step 1: Prepare the Dataset\n",
        "* Collect Data: Gather a labeled dataset (e.g., images with class labels).\n",
        "* Split Dataset: Divide into:\n",
        "  * Training set\n",
        "  * Validation set\n",
        "  * Test set\n",
        "* Preprocessing:\n",
        "  * Resize/reshape images\n",
        "  * Normalize pixel values (e.g., scale to \\[0, 1] or \\[-1, 1])\n",
        "  * Data augmentation (e.g., flip, rotate, zoom) to increase dataset variability\n",
        "\n",
        "Step 2: Design the CNN Architecture\n",
        "Build the CNN by stacking the following layers:\n",
        "* Input Layer: Accepts images in the shape `(height, width, channels)`\n",
        "* Convolutional Layers: Extract spatial features using filters\n",
        "* Activation Functions: Typically ReLU (to introduce non-linearity)\n",
        "* Pooling Layers: Downsample feature maps (e.g., Max Pooling)\n",
        "* Fully Connected (Dense) Layers: For classification\n",
        "* Output Layer:\n",
        "  * Softmax for multi-class\n",
        "  * Sigmoid for binary classification\n",
        "Step 3: Configure the Model\n",
        "* Loss Function:\n",
        "  * Cross-entropy (for classification)\n",
        "  * MSE (for regression tasks)\n",
        "* Optimizer:\n",
        "  * SGD, Adam, RMSProp, etc.\n",
        "* Learning Rate: Small value (e.g., 0.001) to control step size during updates\n",
        "* Metrics: Accuracy, precision, recall, etc.\n",
        "\n",
        "Step 4: Train the Model\n",
        "Use forward and backward passes in epochs:\n",
        "* Forward Propagation: Pass input through the network and compute predictions\n",
        "* Loss Computation: Calculate error between prediction and actual label\n",
        "* Backpropagation: Compute gradients using the loss\n",
        "* Update Weights: Optimizer adjusts weights using gradients\n",
        "\n",
        "Repeat over multiple epochs, using mini-batches (batch size = 16, 32, etc.).\n",
        "\n",
        "Step 5: Validate During Training\n",
        "* Use the validation set to monitor performance after each epoch.\n",
        "* Track metrics like validation loss and accuracy.\n",
        "* Apply techniques like early stopping or learning rate decay if needed.\n",
        "\n",
        "Step 6: Evaluate the Model\n",
        "* Test the trained model on unseen test data.\n",
        "* Report final performance metrics (accuracy, F1-score, confusion matrix, etc.)\n",
        "\n",
        "Step 7: Save & Deploy the Model\n",
        "* Save the trained model (`.h5`, `.pt`, etc.)\n",
        "* Deploy to production or embed in applications (web, mobile, etc.)\n",
        "\n",
        "Optional Enhancements\n",
        "* Regularization (Dropout, L2)\n",
        "* Batch Normalization\n",
        "* Transfer learning if dataset is small\n",
        "\n",
        "Summary Table:\n",
        "| Step             | Description                          |\n",
        "| ---------------- | ------------------------------------ |\n",
        "| 1. Prepare Data  | Collect, clean, augment, normalize   |\n",
        "| 2. Build CNN     | Stack Conv → Pool → Dense → Output   |\n",
        "| 3. Configure     | Set loss, optimizer, learning rate   |\n",
        "| 4. Train         | Forward pass, loss, backprop, update |\n",
        "| 5. Validate      | Tune performance on validation set   |\n",
        "| 6. Evaluate      | Test on new data                     |\n",
        "| 7. Save & Deploy | Use model in real-world scenarios    |"
      ],
      "metadata": {
        "id": "IaA2wh0U5XUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL"
      ],
      "metadata": {
        "id": "Fkz65Kdn5wvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.1: Implementing a basic convolution using a 5x5 image (matrix) and a filter (kernel) step-by-step without any deep learning library, just using basic Python and NumPy.\n",
        "Problem Statement:\n",
        "Perform 2D convolution on a **5x5 grayscale image matrix** using a **3x3 filter (kernel)**. No padding, stride = 1.\n",
        "\n",
        "Step-by-Step:\n",
        "1. Define the image (5x5 matrix):\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "image = np.array([\n",
        "    [10, 20, 30, 40, 50],\n",
        "    [60, 70, 80, 90, 100],\n",
        "    [110, 120, 130, 140, 150],\n",
        "    [160, 170, 180, 190, 200],\n",
        "    [210, 220, 230, 240, 250]\n",
        "])\n",
        "```\n",
        "2. Define the filter (3x3 kernel):\n",
        "Example: edge detection filter\n",
        "```python\n",
        "kernel = np.array([\n",
        "    [-1, -1, -1],\n",
        "    [-1,  8, -1],\n",
        "    [-1, -1, -1]\n",
        "])\n",
        "```\n",
        "3. Perform convolution (no padding, stride = 1):\n",
        "Output size will be (5 - 3 + 1) × (5 - 3 + 1) = 3 × 3\n",
        "```python\n",
        "output = np.zeros((3, 3))\n",
        "\n",
        "for i in range(3):  # Rows\n",
        "    for j in range(3):  # Columns\n",
        "        region = image[i:i+3, j:j+3]  # Extract 3x3 region\n",
        "        conv_value = np.sum(region * kernel)  # Element-wise multiply and sum\n",
        "        output[i, j] = conv_value\n",
        "```\n",
        "4. Print the output matrix:\n",
        "```python\n",
        "print(\"Output after Convolution:\\n\", output)\n",
        "```\n",
        "Complete Output Example:\n",
        "Given the above image and kernel, you'll get a `3x3` matrix like this:\n",
        "\n",
        "```\n",
        "Output after Convolution:\n",
        " [[  0.   0.   0.]\n",
        " [  0.   0.   0.]\n",
        " [  0.   0.   0.]]\n",
        "```\n",
        "\n",
        "(Note: with uniform patterns and an edge filter like this, the output might be zeros or edges if present.)\n",
        "Summary:\n",
        "* Used a 5×5 input image\n",
        "* Applied a 3×3 kernel\n",
        "* Performed convolution with stride 1, no padding\n",
        "* Produced a 3×3 output matrix"
      ],
      "metadata": {
        "id": "1AUbo8C_6HTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2: Max Pooling on a 4x4 feature map using a 2x2 window, stride = 2, and no padding — which is standard for pooling operations.\n",
        "What is Max Pooling?\n",
        "Max pooling selects the maximum value from a sub-region (window) of the feature map.\n",
        "For a 4x4 feature map with a 2x2 window and stride = 2, the output will be a 2x2 matrix.\n",
        "\n",
        "Step-by-step Implementation in Python:\n",
        "1. Define the 4x4 Feature Map\n",
        "```python\n",
        "import numpy as np\n",
        "feature_map = np.array([\n",
        "    [1, 3, 2, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 10, 11, 12],\n",
        "    [13, 14, 15, 16]\n",
        "])\n",
        "```\n",
        "2. Define Max Pooling Parameters\n",
        "* Pool size: `2x2`\n",
        "* Stride: `2`\n",
        "* Output size: `2x2` (because: (4-2)/2 + 1 = 2)\n",
        "```python\n",
        "pool_size = 2\n",
        "stride = 2\n",
        "output = np.zeros((2, 2))\n",
        "```\n",
        "3. Apply Max Pooling\n",
        "```python\n",
        "for i in range(0, feature_map.shape[0], stride):\n",
        "    for j in range(0, feature_map.shape[1], stride):\n",
        "        region = feature_map[i:i+pool_size, j:j+pool_size]\n",
        "        output[i//stride, j//stride] = np.max(region)\n",
        "```\n",
        "4. Print the Output\n",
        "```python\n",
        "print(\"Output after Max Pooling:\\n\", output)\n",
        "```\n",
        "\n",
        "Expected Output:\n",
        "Given the `feature_map`, the regions and their max values are:\n",
        "| Region | Values          | Max |\n",
        "| ------ | --------------- | --- |\n",
        "| (0,0)  | \\[1 3; 5 6]     | 6   |\n",
        "| (0,1)  | \\[2 4; 7 8]     | 8   |\n",
        "| (1,0)  | \\[9 10; 13 14]  | 14  |\n",
        "| (1,1)  | \\[11 12; 15 16] | 16  |\n",
        "So, final output:\n",
        "```\n",
        "[[ 6.  8.]\n",
        " [14. 16.]]\n",
        "```\n",
        "Summary:\n",
        "* Input: 4x4 feature map\n",
        "* Pooling Window: 2x2\n",
        "* Stride: 2\n",
        "* Output: 2x2 matrix with maximum values from each 2x2 region"
      ],
      "metadata": {
        "id": "abiYOUQq6oyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3. Implement the ReLU Activation Function on a Feature Map\n",
        "\n",
        "What is ReLU?\n",
        "ReLU (Rectified Linear Unit) is an activation function defined as:\n",
        "$$\n",
        "\\text{ReLU}(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "It replaces negative values with 0, and keeps positive values unchanged.\n",
        "\n",
        "#Step-by-Step Implementation in Python\n",
        "1. Define a sample feature map (with positive and negative values):\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "feature_map = np.array([\n",
        "    [2, -3, 0],\n",
        "    [-1, 5, -6],\n",
        "    [4, -2, 7]\n",
        "])\n",
        "```\n",
        "2. Apply ReLU:\n",
        "There are two ways:\n",
        "#Method 1: Using NumPy’s `np.maximum()`\n",
        "```python\n",
        "relu_output = np.maximum(0, feature_map)\n",
        "```\n",
        "#Method 2: Define a ReLU function\n",
        "```python\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "relu_output = relu(feature_map)\n",
        "```\n",
        "3. Print the Output\n",
        "```python\n",
        "print(\"Output after ReLU:\\n\", relu_output)\n",
        "```\n",
        "# Expected Output\n",
        "Given the input:\n",
        "\n",
        "```\n",
        "[[  2  -3   0]\n",
        " [ -1   5  -6]\n",
        " [  4  -2   7]]\n",
        "```\n",
        "\n",
        "ReLU output will be:\n",
        "\n",
        "```\n",
        "[[2 0 0]\n",
        " [0 5 0]\n",
        " [4 0 7]]\n",
        "``"
      ],
      "metadata": {
        "id": "Fu7HPIwGCA4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q4: Creating a Simple CNN model with:\n",
        "\n",
        "* One Convolutional Layer\n",
        "* One Fully Connected (Dense) Layer\n",
        "* Using **Random Data\n",
        "\n",
        "We’ll use TensorFlow (Keras) to implement this. You can run this in a Python environment like Jupyter Notebook, Google Colab, or any Python IDE with TensorFlow installed.\n",
        "\n",
        "Step-by-Step Code: Simple CNN with Random Data\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "```\n",
        "Step 1: Generate Random Input Data\n",
        "Assume we have 10 images, each of size 28x28 with 1 channel (like grayscale images):\n",
        "```python\n",
        "# Random input: 10 samples of 28x28 grayscale images\n",
        "X = np.random.rand(10, 28, 28, 1).astype(np.float32)\n",
        "\n",
        "# Random labels for 10 samples and 3 classes\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, 3, size=(10,)), num_classes=3)\n",
        "```\n",
        "Step 2: Define a Simple CNN Model\n",
        "```python\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(3, activation='softmax')  # 3-class classification\n",
        "])\n",
        "```\n",
        "Step 3: Compile the Model\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "Step 4: Train on Random Data\n",
        "```python\n",
        "model.fit(X, y, epochs=3)\n",
        "```"
      ],
      "metadata": {
        "id": "HXCu9lRvCE8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q5: Generate a synthetic dataset using random noise and train a simple CNN on it.\n",
        "\n",
        "Objective:\n",
        "* Create a dataset with random noise as images\n",
        "* Assign random class labels\n",
        "* Train a simple CNN model to demonstrate the end-to-end training process (though the accuracy will be low due to randomness)\n",
        "\n",
        "Why Do This?\n",
        "This exercise is useful to:\n",
        "* Understand CNN architecture setup\n",
        "* Test model training pipeline\n",
        "* Practice working with synthetic data\n",
        "\n",
        "Step-by-Step Code (Using TensorFlow/Keras)\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "```\n",
        "Step 1: Generate Random Noise Data\n",
        "Let’s say:\n",
        "* 100 samples\n",
        "* Each image is 28x28 grayscale\n",
        "* 3 random classes\n",
        "\n",
        "```python\n",
        "# Generate random noise images\n",
        "X = np.random.rand(100, 28, 28, 1).astype(np.float32)\n",
        "\n",
        "# Generate random labels (0, 1, or 2)\n",
        "y = np.random.randint(0, 3, size=(100,))\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=3)\n",
        "```\n",
        "Step 2: Create a Simple CNN Model\n",
        "```python\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(3, activation='softmax')  # 3-class output\n",
        "])\n",
        "```\n",
        "Step 3: Compile the Model\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "Step 4: Train the Model\n",
        "```python\n",
        "model.fit(X, y, epochs=5, batch_size=10)\n",
        "```\n"
      ],
      "metadata": {
        "id": "vM2WRYsZCmcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q6: Create a simple CNN using Keras with:\n",
        "* One Convolutional Layer\n",
        "* One Max Pooling Layer\n",
        "This is a minimal CNN architecture, useful for learning purposes.\n",
        "\n",
        "Step-by-Step Code: Simple CNN (Keras)\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "```\n",
        "Step 1: Generate Dummy Input Data\n",
        "Let’s simulate grayscale images of size 28x28**, 100 samples, and 3 classes:\n",
        "\n",
        "```python\n",
        "# 100 grayscale images (28x28)\n",
        "X = np.random.rand(100, 28, 28, 1).astype(np.float32)\n",
        "\n",
        "# Random labels (3 classes)\n",
        "y = np.random.randint(0, 3, size=(100,))\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=3)\n",
        "```\n",
        "Step 2: Define the CNN Model\n",
        "\n",
        "```python\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(3, activation='softmax')  # 3-class output\n",
        "])\n",
        "```\n",
        "Step 3: Compile the Model\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "Step 4: Train the Model\n",
        "\n",
        "```python\n",
        "model.fit(X, y, epochs=5, batch_size=10)\n",
        "```\n",
        "Optional: View the Model Structure\n",
        "\n",
        "```python\n",
        "model.summary()\n",
        "```\n",
        "Summary\n",
        "\n",
        "* **Conv2D layer** extracts features using filters\n",
        "* **MaxPooling layer** reduces spatial dimensions\n",
        "* **Flatten + Dense** layers are used for classification\n",
        "* Trained on **random data** just to demonstrate architecture"
      ],
      "metadata": {
        "id": "D9q1Ay8FC7Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q7: Add a Fully Connected (Dense) Layer after Convolution and Max-Pooling layers in a CNN using Keras.\n",
        "\n",
        "* One `Conv2D` layer\n",
        "* One `MaxPooling2D` layer\n",
        "* One `Dense` (fully connected) layer after flattening\n",
        "\n",
        "e.g Code in Keras (TensorFlow)\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "```\n",
        "Step 1: Generate Random Data (100 grayscale images, 28×28, 3 classes)\n",
        "\n",
        "```python\n",
        "# Input images: 100 samples, 28x28, 1 channel\n",
        "X = np.random.rand(100, 28, 28, 1).astype(np.float32)\n",
        "\n",
        "# Random labels (0, 1, 2) → one-hot encoded\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, 3, size=(100,)), num_classes=3)\n",
        "```\n",
        "Step 2: Build CNN Model with Fully Connected Layer\n",
        "\n",
        "```python\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),  # Flatten before dense layer\n",
        "\n",
        "    layers.Dense(64, activation='relu'),  # Fully connected layer\n",
        "    layers.Dense(3, activation='softmax')  # Output layer for 3 classes\n",
        "])\n",
        "```\n",
        "Step 3: Compile the Model\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "Step 4: Train the Model\n",
        "\n",
        "```python\n",
        "model.fit(X, y, epochs=5, batch_size=10)\n",
        "```\n",
        "Summary of Architecture:\n",
        "\n",
        "| Layer Type        | Details                  |\n",
        "| ----------------- | ------------------------ |\n",
        "| Conv2D            | 16 filters, 3x3, ReLU    |\n",
        "| MaxPooling2D      | 2x2 pool                 |\n",
        "| Flatten           | Converts 2D to 1D vector |\n",
        "| Dense (Hidden FC) | 64 units, ReLU           |\n",
        "| Dense (Output FC) | 3 units, Softmax         |"
      ],
      "metadata": {
        "id": "rsPMdU-1Dc0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q8: Add Batch Normalization to a simple CNN model using TensorFlow Keras.\n",
        "\n",
        "What is Batch Normalization?\n",
        "Batch Normalization:\n",
        "* Normalizes the outputs of a layer\n",
        "* Helps **stabilize** and **speed up** training\n",
        "* Often added **after a Conv or Dense layer, before activation**\n",
        "\n",
        "Step-by-Step Code: Simple CNN with Batch Normalization\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "```\n",
        "Step 1: Generate Dummy Input Data\n",
        "```python\n",
        "# 100 grayscale images (28x28)\n",
        "X = np.random.rand(100, 28, 28, 1).astype(np.float32)\n",
        "\n",
        "# Random labels for 3 classes\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, 3, size=(100,)), num_classes=3)\n",
        "```\n",
        "Step 2: Define CNN with Batch Normalization\n",
        "```python\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), input_shape=(28, 28, 1), padding='same'),\n",
        "    layers.BatchNormalization(),             # Batch Normalization\n",
        "    layers.Activation('relu'),               # Activation after normalization\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64),\n",
        "    layers.BatchNormalization(),             # BN before activation in Dense too\n",
        "    layers.Activation('relu'),\n",
        "    layers.Dense(3, activation='softmax')    # Output layer\n",
        "])\n",
        "```\n",
        "Step 3: Compile the Model\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "Step 4: Train the Model\n",
        "```python\n",
        "model.fit(X, y, epochs=5, batch_size=10)\n",
        "```"
      ],
      "metadata": {
        "id": "MCf6cAy8Dvvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q9: Add Dropout Regularization to a simple CNN model using TensorFlow Keras.\n",
        "What is Dropout?\n",
        "* Dropout is a regularization technique that randomly sets a fraction of input units to 0 during training.\n",
        "* It helps prevent overfitting.\n",
        "Step-by-Step Code: Simple CNN with Dropout\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "```\n",
        "Step 1: Generate Dummy Input Data\n",
        "```python\n",
        "# 100 grayscale images of 28x28\n",
        "X = np.random.rand(100, 28, 28, 1).astype(np.float32)\n",
        "\n",
        "# 3 random class labels (one-hot encoded)\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, 3, size=(100,)), num_classes=3)\n",
        "```\n",
        "Step 2: Define CNN with Dropout Layers\n",
        "```python\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Dropout(0.25),                        # Dropout after pooling\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),                         # Dropout before output layer\n",
        "    layers.Dense(3, activation='softmax')        # Output layer for 3 classes\n",
        "])\n",
        "```\n",
        "Step 3: Compile the Model\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "Step 4: Train the Model\n",
        "```python\n",
        "model.fit(X, y, epochs=5, batch_size=10)\n",
        "```\n",
        "Summary\n",
        "| Layer Type    | Description              |\n",
        "| ------------- | ------------------------ |\n",
        "| Conv2D        | Feature extraction       |\n",
        "| MaxPooling2D  | Downsampling             |\n",
        "| Dropout(0.25) | Regularize conv layer    |\n",
        "| Flatten       | Prepare for dense layers |\n",
        "| Dense (64)    | Fully connected hidden   |\n",
        "| Dropout(0.5)  | Regularize dense layer   |\n",
        "| Dense (3)     | Output softmax layer     |"
      ],
      "metadata": {
        "id": "hOLDiiJOEAy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q10.write a code to print the architecture of the VGG16 model in keras?\n",
        "\n",
        "Step-by-Step Code to Load and Print VGG16 Model Architecture\n",
        "```python\n",
        "from tensorflow.keras.applications import VGG16\n",
        "# Load the VGG16 model (pretrained on ImageNet)\n",
        "model = VGG16(weights='imagenet')\n",
        "# Print the architecture summary\n",
        "model.summary()\n",
        "```\n",
        "Output:\n",
        "* This will print:\n",
        "\n",
        "  * Each layer's **type** (Conv2D, MaxPooling2D, Dense, etc.)\n",
        "  * **Output shape** at each stage\n",
        "  * **Number of parameters** in each layer\n",
        "* The model ends with **fully connected layers** for classification.\n",
        "\n",
        "Optional: Load Without Top (For Feature Extraction)\n",
        "```python\n",
        "model = VGG16(weights='imagenet', include_top=False)\n",
        "model.summary()\n",
        "```"
      ],
      "metadata": {
        "id": "mWIKsD43EmMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q11: Plot the accuracy and loss graphs after training a CNN model using Matplotlib and Keras' `model.fit()` history.\n",
        "\n",
        "Step-by-Step Code to Plot Accuracy and Loss\n",
        "Step 1: Train a Simple CNN Model (with history)\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate dummy data\n",
        "X = np.random.rand(100, 28, 28, 1).astype(np.float32)\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, 3, size=(100,)), num_classes=3)\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train and store the training history\n",
        "history = model.fit(X, y, epochs=10, batch_size=10, validation_split=0.2)\n",
        "```\n",
        "Step 2: Plot Accuracy and Loss\n",
        "```python\n",
        "# Plot training & validation accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "Output:\n",
        "* Left graph: Training vs. Validation Accuracy over epochs\n",
        "* Right graph: Training vs. Validation Loss over epochs"
      ],
      "metadata": {
        "id": "ivaWFTKeHZgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q12. Write a code to print the architecture of the ResNet50 model in keras?\n",
        "\n",
        "Step-by-Step Code to Print ResNet50 Architecture\n",
        "```python\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load ResNet50 model with pre-trained ImageNet weights\n",
        "model = ResNet50(weights='imagenet')\n",
        "\n",
        "# Print model architecture\n",
        "model.summary()\n",
        "```\n",
        "Optional: Load ResNet50 Without Top Layer (for Feature Extraction)\n",
        "```python\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "model.summary()\n",
        "```\n",
        "* `include_top=False` removes the final fully connected classification layers.\n",
        "* Useful when you want to **use ResNet50 as a feature extractor** in custom models.\n",
        "\n",
        "Note:\n",
        "If you want a **visual diagram** of the model:\n",
        "```python\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True, to_file='resnet50_architecture.png')\n"
      ],
      "metadata": {
        "id": "zhgsKGk1H88P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q13.  write a code to train a basic CNN model and print the training loss and accuracy after each epoch?\n",
        "\n",
        "Step-by-Step Code: Train CNN & Print Loss/Accuracy per Epoch\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Step 1: Create dummy data\n",
        "X = np.random.rand(100, 28, 28, 1).astype(np.float32)  # 100 grayscale images (28x28)\n",
        "y = tf.keras.utils.to_categorical(np.random.randint(0, 3, size=(100,)), num_classes=3)  # 3 classes\n",
        "\n",
        "# Step 2: Define a basic CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(3, activation='softmax')  # Output layer for 3 classes\n",
        "])\n",
        "\n",
        "# Step 3: Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Train the model and store history\n",
        "history = model.fit(X, y, epochs=5, batch_size=10, verbose=1)  # verbose=1 shows loss/accuracy each epoch\n",
        "\n",
        "Output Example:\n",
        "With `verbose=1`, training logs will automatically print something like:\n",
        "\n",
        "```\n",
        "Epoch 1/5\n",
        "10/10 [==============================] - 1s 20ms/step - loss: 1.1247 - accuracy: 0.2900\n",
        "Epoch 2/5\n",
        "10/10 [==============================] - 0s 6ms/step - loss: 1.0861 - accuracy: 0.3600\n",
        "...\n",
        "Optional: Manual Print (if using custom training loop)\n",
        "If you want to print manually (for more control):\n",
        "\n",
        "```python\n",
        "for epoch in range(5):\n",
        "    history = model.fit(X, y, epochs=1, batch_size=10, verbose=0)  # Silent training\n",
        "    loss = history.history['loss'][0]\n",
        "    acc = history.history['accuracy'][0]\n",
        "    print(f\"Epoch {epoch+1} - Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "```"
      ],
      "metadata": {
        "id": "nr5b8GVqITyL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}