{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKeK/C8rOe7RLG3ry0OkzJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wekann/Assignment/blob/main/Deep_Learning_Frameworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning Framework"
      ],
      "metadata": {
        "id": "c_QaXYiGolBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q1: What is TensorFlow 2.0, and how is it different from TensorFlow 1.x?\n",
        "\n",
        "What is TensorFlow 2.0?\n",
        "\n",
        "**TensorFlow 2.0** is an open-source deep learning framework developed by Google, released in **September 2019**. It is a major upgrade from **TensorFlow 1.x**, designed to make machine learning model development:\n",
        "\n",
        "Easier to use\n",
        "More Pythonic\n",
        "Faster for prototyping\n",
        "More flexible and powerful\n",
        "\n",
        "It integrates seamlessly with Keras and emphasizes eager execution, allowing you to write code and see results immediately.\n",
        "\n",
        "Key Differences Between TensorFlow 2.0 and 1.x\n",
        "\n",
        "| Feature                    | TensorFlow 1.x                         | TensorFlow 2.0                                                          |\n",
        "| -------------------------- | -------------------------------------- | ----------------------------------------------------------------------- |\n",
        "| Execution Model            | Graph-based (static computation graph) | Eager execution by default (dynamic computation graph)                  |\n",
        "| Ease of Use                | Complex APIs, steep learning curve     | Simpler, user-friendly API with tight Keras integration                 |\n",
        "| Keras Support              | Optional and inconsistent              | Built-in high-level API (`tf.keras`)                                    |\n",
        "| Code Readability           | Verbose and fragmented                 | Clean, Pythonic, and concise                                            |\n",
        "| tf.function Decorator      | Not available                          | Introduced to convert eager code to graph for performance               |\n",
        "| Variables and Sessions     | Manual session handling (`tf.Session`) | Sessions removed; variables are auto-managed                            |\n",
        "| Compatibility              | Backward incompatible                  | Designed for forward simplicity; limited 1.x support via `tf.compat.v1` |\n",
        "### 🛠️ Example:\n",
        "\n",
        "TensorFlow 1.x:\n",
        "```python\n",
        "sess = tf.Session()\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = x * 2\n",
        "result = sess.run(y, feed_dict={x: 3})\n",
        "```\n",
        "\n",
        "TensorFlow 2.0:\n",
        "\n",
        "```python\n",
        "x = tf.constant(3.0)\n",
        "y = x * 2\n",
        "print(y.numpy())\n",
        "```\n",
        "Why Use TensorFlow 2.0?\n",
        "* Simplified API (especially with `tf.keras`)\n",
        "* Eager execution helps debugging\n",
        "* Better support for **TF Lite**, **TF Serving**, and **TF.js**\n",
        "* Performance optimizations with AutoGraph & distribution strategies"
      ],
      "metadata": {
        "id": "zlALbU5XpEaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2:How do you install TensorFlow 2.0?\n",
        "Soln: You can install TensorFlow 2.0 easily using `pip` in your Python environment.\n",
        "\n",
        "#Basic Installation (CPU version)\n",
        "Open your terminal or command prompt and run:\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```\n",
        "\n",
        "This will install the latest stable version of TensorFlow (currently 2.x series). If you want to explicitly install TensorFlow 2.0 (initial release):\n",
        "\n",
        "```bash\n",
        "pip install tensorflow==2.0.0\n",
        "```\n",
        "#Installation with GPU Support\n",
        "If your system has an NVIDIA GPU and is properly configured (with CUDA and cuDNN), install:\n",
        "```bash\n",
        "pip install tensorflow-gpu==2.0.0\n",
        "```\n",
        "\n",
        ">Note: Newer versions of TensorFlow (>=2.1) have unified packages for CPU and GPU, so this command is mostly for legacy systems.\n",
        "\n",
        "#Using Virtual Environment (Recommended)\n",
        "To avoid conflicts with other Python packages:\n",
        "```bash\n",
        "# Create and activate a virtual environment\n",
        "python -m venv tf_env\n",
        "source tf_env/bin/activate       # For Linux/Mac\n",
        ".\\tf_env\\Scripts\\activate        # For Windows\n",
        "\n",
        "# Install TensorFlow\n",
        "pip install tensorflow==2.0.0\n",
        "```\n",
        "#Verify Installation\n",
        "After installing, check TensorFlow version in Python:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "You should see:\n",
        "```\n",
        "2.0.0\n",
        "```\n"
      ],
      "metadata": {
        "id": "c5QE-SckpNED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3: What is primary function of the tf.function in Tensorflow 2.0?\n",
        "soln: Primary Function of `tf.function`:\n",
        "`tf.function` is used to convert a Python function into a high-performance TensorFlow graph. This enables TensorFlow to run your code faster by optimizing it for efficient execution.\n",
        "Why is it important?\n",
        "In TensorFlow 2.0, code runs eagerly by default (like normal Python), which is great for debugging but slower for large-scale training.\n",
        "\n",
        "By using `@tf.function`, you:\n",
        "Compile your function into a static computation graph\n",
        "Get the performance benefits of graph execution\n",
        "Can deploy your model in production or on mobile with tools like TensorFlow Lite\n",
        "\n",
        "How It Works\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def multiply(x, y):\n",
        "    return x * y\n",
        "\n",
        "print(multiply(tf.constant(3), tf.constant(4)))\n",
        "```\n",
        "Under the hood:\n",
        "* TensorFlow traces your Python code\n",
        "* Builds a static computation graph\n",
        "* Executes the graph for faster performance\n",
        "\n",
        "Key Benefits\n",
        "* Improves performance\n",
        "* Enables graph-based features (e.g. optimization, serialization)\n",
        "* Required for deploying models in production or on edge devices\n",
        "\n",
        "Note:\n",
        "Not all Python code can be converted. Avoid using:\n",
        "* Regular Python loops with non-TensorFlow values\n",
        "* Side effects (like `print` or `append` outside `tf.print`)\n",
        "* Python objects that aren’t Tensor-compatible"
      ],
      "metadata": {
        "id": "V3XcYIkOpecL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q4: What is the purpose of the model class in TensorFlow 2.0?\n",
        "soln:Purpose of the `Model` Class:\n",
        "In TensorFlow 2.0, the `Model` class (from `tf.keras.Model`) serves as the base class for building and training custom models. It provides a flexible and structured way to:\n",
        "* Define complex neural network architectures\n",
        "* Handle forward pass logic (`call` method)\n",
        "* Automatically integrate with training, evaluation, and inference workflows\n",
        "\n",
        "Where It Comes From\n",
        "```python\n",
        "from tensorflow.keras import Model\n",
        "```\n",
        "Or when subclassing:\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(10)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        return self.dense2(x)\n",
        "```\n",
        "Main Purposes of the `Model` Class\n",
        "\n",
        "| Purpose           | Description                                                             |\n",
        "| ----------------- | ----------------------------------------------------------------------- |\n",
        "| Encapsulation     | Encapsulates layers, forward pass, and model behavior in one object     |\n",
        "| Custom Logic      | Allows custom training steps, loss functions, and metrics               |\n",
        "| Integration       | Works smoothly with `fit()`, `evaluate()`, and `predict()` methods      |\n",
        "| Serialization     | Supports saving/loading weights and model architectures                 |\n",
        "| Subclassing       | Useful when building dynamic or complex models (e.g. GANs, custom RNNs) |\n",
        "\n",
        " Two Main Ways to Use a Model\n",
        "\n",
        "1. Sequential / Functional API:\n",
        "\n",
        "    For standard feed-forward models\n",
        "\n",
        "   ```python\n",
        "   model = tf.keras.Sequential([\n",
        "       tf.keras.layers.Dense(64, activation='relu'),\n",
        "       tf.keras.layers.Dense(10)\n",
        "   ])\n",
        "   ```\n",
        "\n",
        "2. Subclassing `tf.keras.Model`:\n",
        "\n",
        "   * For custom behavior and architectures\n",
        "\n",
        "   ```python\n",
        "   class MyModel(tf.keras.Model):\n",
        "       ...\n",
        "   ```\n",
        "# Summary\n",
        "\n",
        "The `Model` class in TensorFlow 2.0 provides:\n",
        "\n",
        "* A powerful base for custom architectures\n",
        "* High-level API features for training & evaluation\n",
        "* Flexibility for advanced deep learning tasks\n"
      ],
      "metadata": {
        "id": "l9SbRdYypzAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q5: How do you create a neural network using TensorFlow 2.0?\n",
        "\n",
        "In TensorFlow 2.0, you can create a neural network using Keras API, which is built-in and simplifies model creation. There are three main approaches:\n",
        "\n",
        "1. Sequential API (Best for simple, stackable models)\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a Sequential model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),  # Input layer\n",
        "    tf.keras.layers.Dense(64, activation='relu'),                       # Hidden layer\n",
        "    tf.keras.layers.Dense(10, activation='softmax')                    # Output layer\n",
        "])\n",
        "```\n",
        "2. Functional API (Good for multi-input/output or shared layers)\n",
        "\n",
        "```python\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# Define input\n",
        "inputs = tf.keras.Input(shape=(784,))\n",
        "x = layers.Dense(128, activation='relu')(inputs)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "```\n",
        "3. Subclassing `tf.keras.Model` (Best for custom or dynamic models)\n",
        "\n",
        "```python\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.d1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.d2 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        return self.out(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MyModel()\n",
        "```\n",
        "Compile, Train, and Evaluate the Model**\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Example: Training on dummy data\n",
        "# model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "```\n",
        "Summary\n",
        "\n",
        "| Step                    | Action                              |\n",
        "| ----------------------- | ----------------------------------- |\n",
        "| 1. Define the model     | Sequential, Functional, or Subclass |\n",
        "| 2. Compile              | Set optimizer, loss, and metrics    |\n",
        "| 3. Fit                  | Train the model with `.fit()`       |\n",
        "| 4. Evaluate             | Test the model with `.evaluate()`   |\n",
        "| 5. Predict              | Make predictions with `.predict()`  |\n"
      ],
      "metadata": {
        "id": "E7k0sI_PqJxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q6: what is the importance of Tensor Space in Tensorflow?\n",
        "\n",
        "What is a Tensor in TensorFlow?\n",
        "\n",
        "In TensorFlow, a tensor is the core data structure — it's a generalization of scalars, vectors, and matrices to higher dimensions. Think of it as a multidimensional array (similar to NumPy arrays).\n",
        "\n",
        "What is \"Tensor Space\"?\n",
        "\n",
        "Tensor Space refers to the mathematical and computational domain in which all tensor operations happen — including:\n",
        "\n",
        "* Arithmetic operations\n",
        "* Transformations (e.g. reshape, transpose)\n",
        "* Data flow in deep learning models\n",
        "\n",
        "Importance of Tensor Space in TensorFlow**\n",
        "\n",
        "| Reason                           | Description                                                                                                                                |\n",
        "| -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| 1. Core of Data Flow             | All inputs, outputs, weights, and activations in TensorFlow models are tensors. The tensor space is where these values \"live\" and operate. |\n",
        "| 2. Unified Data Structure        | Tensors allow consistent representation of data for computation on CPU, GPU, or TPU.                                                       |\n",
        "| 3. Efficient Computation         | TensorFlow performs tensor operations in tensor space using optimized backends for speed and scalability.                                  |\n",
        "| 4. Flexibility in Dimensions     | Supports 0D (scalar), 1D (vector), 2D (matrix), up to nD arrays, enabling complex model architectures.                                     |\n",
        "| 5. Automatic Differentiation     | Gradients are calculated within tensor space using operations tracked on tensors (important for backpropagation).                          |\n",
        "| 6. Device Independence           | Tensor space abstracts hardware—same code can run on CPU/GPU/TPU without modification.                                                     |\n",
        "\n",
        "Example: Tensor Types\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Scalar (0D)\n",
        "scalar = tf.constant(5)\n",
        "\n",
        "# Vector (1D)\n",
        "vector = tf.constant([1.0, 2.0, 3.0])\n",
        "\n",
        "# Matrix (2D)\n",
        "matrix = tf.constant([[1, 2], [3, 4]])\n",
        "\n",
        "# 3D Tensor\n",
        "tensor3d = tf.constant([[[1], [2]], [[3], [4]]])\n",
        "```\n",
        "Summary\n",
        "\n",
        "* Tensors are the building blocks of data in TensorFlow.\n",
        "* The \"tensor space\" is where all learning, operations, and computation happen.\n",
        "* Understanding tensor shapes, ranks, and broadcasting is essential to mastering TensorFlow.\n"
      ],
      "metadata": {
        "id": "fHTXstv0qcW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q7: How can TensorBoard be integrated with TensorFlow 2.0?\n",
        "What is TensorBoard?\n",
        "\n",
        "TensorBoard is TensorFlow’s built-in visualization tool. It helps you:\n",
        "\n",
        "* Monitor training metrics (loss, accuracy)\n",
        "* Visualize model graphs\n",
        "* Track histograms, distributions, and more\n",
        "\n",
        "Steps to Integrate TensorBoard with TensorFlow 2.0\n",
        "Step 1: Import Required Libraries\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "```\n",
        "Step 2: Define and Compile a Model\n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "Step 3: Create a TensorBoard Callback\n",
        "\n",
        "```python\n",
        "# Set the log directory with a timestamp\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Create the callback\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "```\n",
        "Step 4: Train the Model with the Callback\n",
        "\n",
        "```python\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(x_val, y_val),\n",
        "          callbacks=[tensorboard_callback])\n",
        "```\n",
        "Step 5: Launch TensorBoard\n",
        "\n",
        "In your terminal, run:\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir=logs/fit\n",
        "```\n",
        "\n",
        "Then open your browser and go to:\n",
        "\n",
        "```\n",
        "http://localhost:6006/\n",
        "```\n",
        "What You Can See in TensorBoard\n",
        "\n",
        "| Feature        | Description                               |\n",
        "| -------------- | ----------------------------------------- |\n",
        "| Scalars        | Track loss, accuracy, learning rate, etc. |\n",
        "| Graphs         | Visualize model architecture              |\n",
        "| Histograms     | Weights and activations over time         |\n",
        "| Images         | Input samples, filters                    |\n",
        "| Projector      | Embedding visualizations                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "bw9pMkYVqyaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q8: What is the purpose of TensorFlow Playground?\n",
        " What is TensorFlow Playground?\n",
        "\n",
        "TensorFlow Playground is a browser-based interactive tool that allows users to experiment with neural networks directly in their web browser — without any coding.\n",
        "Purpose of TensorFlow Playground\n",
        "\n",
        "| Purpose                           | Description                                                                                                                             |\n",
        "| --------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| 1. Educational Tool               | Helps beginners visually understand how neural networks learn and make decisions.                                                       |\n",
        "| 2. Experimentation                | Lets users tweak hyperparameters (like learning rate, number of layers, activation functions) and instantly see the effect on training. |\n",
        "| 3. Intuition Building             | Teaches concepts like overfitting, underfitting, non-linearity, and decision boundaries through live visualization.                     |\n",
        "| 4. No Installation Needed         | Works entirely in the browser—great for quick demos, learning, and teaching.                                                            |\n",
        "| 5. Understand Backpropagation     | Shows how weights adjust during training via visual animations.                                                                         |\n",
        "\n",
        "What Can You Control in TensorFlow Playground?\n",
        "\n",
        "* Input features (X1, X2, sin(X1), etc.)\n",
        "* Number of layers and neurons\n",
        "* Activation functions (ReLU, tanh, sigmoid)\n",
        "* Learning rate\n",
        "* Batch size & epochs\n",
        "* Regularization (L1, L2)\n",
        "* Noise in data\n",
        "* Problem type (classification vs regression)\n",
        "\n",
        "Visual Feedback Includes:\n",
        "\n",
        "* Real-time loss reduction\n",
        "* Training accuracy\n",
        "* Weight updates\n",
        "* Decision boundaries forming\n"
      ],
      "metadata": {
        "id": "SMdWgsXQrLc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q9: What is Netron, and how is it useful for deep learning models?\n",
        "What is Netron?\n",
        "\n",
        "Netron is a visual model viewer used to analyze and inspect deep learning models. It provides an interactive graphical interface to explore model architecture, layers, and parameters.\n",
        "Key Features of Netron\n",
        "\n",
        "| Feature                     | Description                                                                               |\n",
        "| --------------------------- | ----------------------------------------------------------------------------------------- |\n",
        "| Model Visualization         | Shows a clear graphical structure of your model’s layers and connections                  |\n",
        "| Layer Details               | Click on layers to view input/output shapes, parameters, and attributes                   |\n",
        "| Multi-Framework Support     | Supports models from TensorFlow, Keras, PyTorch, ONNX, CoreML, TFLite, Caffe, MXNet, etc. |\n",
        "| Lightweight & Fast          | Opens models instantly in a browser or standalone app                                     |\n",
        "| Drag & Zoom                 | Explore large models with an interactive, zoomable interface                              |\n",
        "\n",
        "Supported File Formats\n",
        "Netron supports a wide range of model formats:\n",
        "\n",
        "* `.pb`, `.h5`, `.tflite` (TensorFlow / Keras / TF Lite)\n",
        "* `.onnx` (Open Neural Network Exchange)\n",
        "* `.pt`, `.pth`, `.pkl` (PyTorch)\n",
        "* `.mlmodel` (Apple CoreML)\n",
        "* `.caffemodel` (Caffe)\n",
        "* `.json` (MXNet)\n",
        "\n",
        "Why is Netron Useful in Deep Learning?**\n",
        "\n",
        "| Use Case                       | Benefit                                                               |\n",
        "| ------------------------------ | --------------------------------------------------------------------- |\n",
        "| Model Debugging                | Quickly check if the model architecture is as expected                |\n",
        "| Architecture Understanding     | Visualize layers, input/output shapes, and flow                       |\n",
        "| Model Sharing                  | Share visual model insights with teams or clients                     |\n",
        "| Education & Documentation      | Helps students or stakeholders grasp deep learning models intuitively |\n",
        "\n"
      ],
      "metadata": {
        "id": "4J5GyHygsE9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q10: what is the difference between TensorFlow & PyTorch?\n",
        "\n",
        "TensorFlow and PyTorch are the two most widely used deep learning frameworks. Both are powerful, open-source, and supported by major tech companies (TensorFlow by Google, PyTorch by Meta/Facebook). However, they differ in syntax, execution, and use cases.\n",
        "Key Differences Between TensorFlow and PyTorch\n",
        "\n",
        "| Feature                    |     TensorFlow                                    | PyTorch                                 |\n",
        "| -------------------------- | ------------------------------------------------- | ------------------------------------------- |\n",
        "| Developer                  | Google                                            | Meta (Facebook)                             |\n",
        "| Release Year               | 2015                                              | 2016                                        |\n",
        "| Execution Model            | Graph-based (static by default)                   | Eager (dynamic by default)                  |\n",
        "| Ease of Debugging          | More complex (better with TF 2.x & `tf.function`) | Easier (feels like regular Python)          |\n",
        "| Syntax Style               | More verbose                                      | More Pythonic and intuitive                 |\n",
        "| Dynamic Computation        | Supported via `tf.function`, `tf.GradientTape`    | Native support (built-in dynamic graphs)    |\n",
        "| Model Deployment           | Excellent (TensorFlow Serving, Lite, JS, etc.)    | Improving (TorchServe, ONNX support)        |\n",
        "| Community Support          | Very large and established                        | Rapidly growing and active                  |\n",
        "| Visualization Tool         | TensorBoard (native, powerful)                    | TensorBoard (via `torch.utils.tensorboard`) |\n",
        "| Mobile/Edge Support        | TensorFlow Lite, TensorFlow\\.js                   | PyTorch Mobile (newer and less mature)      |\n",
        "| Popularity in Research     | More in production/industry                       | More popular in academia/research           |\n",
        "\n",
        "High-Level Summary\n",
        "\n",
        "| Use Case                        | Recommended Framework      |\n",
        "| ------------------------------- | -------------------------- |\n",
        "| Rapid Prototyping               | PyTorch                    |\n",
        "| Production-Scale Deployment     | TensorFlow                 |\n",
        "| Research & Experimentation      | PyTorch                    |\n",
        "| Mobile or Web Apps              | TensorFlow (via TFLite/JS) |\n",
        "\n",
        "Example Comparison\n",
        "\n",
        "PyTorch:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "```\n",
        "\n",
        "TensorFlow:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "```\n",
        "Final Verdict\n",
        "\n",
        "* PyTorch = Best for beginners, flexible research, dynamic experimentation\n",
        "* TensorFlow = Best for scalable production systems, cross-platform deployment, and industry use\n"
      ],
      "metadata": {
        "id": "LZR6HGMPsSnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q11: How do you install PyTorch?\n",
        "Step-by-Step Guide to Installing PyTorch\n",
        "\n",
        "You can install PyTorch using pip, conda, or by selecting options from the [official website](https://pytorch.org/get-started/locally/).\n",
        "\n",
        "1. Install with `pip` (Recommended for most users)\n",
        "```bash\n",
        "pip install torch torchvision torchaudio\n",
        "```\n",
        "\n",
        "For GPU (NVIDIA CUDA 11.8):\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "```\n",
        "\n",
        "> You must have compatible CUDA drivers and NVIDIA GPU for GPU support.\n",
        "\n",
        "2. Install with `conda` (if you use Anaconda)\n",
        "\n",
        "For CPU-only:\n",
        "```bash\n",
        "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
        "```\n",
        "\n",
        "For GPU (CUDA 11.8):\n",
        "```bash\n",
        "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "```\n",
        "3. Verify Installation\n",
        "After installation, open Python and run:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "```"
      ],
      "metadata": {
        "id": "-uHSUZuZsugh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q12: What is the basic structure of PyTorch neural network?\n",
        "\n",
        "Overview\n",
        "\n",
        "In PyTorch, a neural network is built by creating a class that inherits from `torch.nn.Module`, defining the layers in `__init__()`, and specifying the forward pass logic in the `forward()` method.\n",
        "\n",
        "Basic Structure of a PyTorch Neural Network\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a custom neural network\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(784, 128)    # Input layer\n",
        "        self.fc2 = nn.Linear(128, 64)     # Hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)      # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))           # Activation after layer 1\n",
        "        x = F.relu(self.fc2(x))           # Activation after layer 2\n",
        "        x = self.fc3(x)                   # Output (no softmax needed if using CrossEntropyLoss)\n",
        "        return x\n",
        "```\n",
        "How to Use This Model\n",
        "```python\n",
        "# Create model instance\n",
        "model = NeuralNet()\n",
        "\n",
        "# Choose optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Forward pass example\n",
        "dummy_input = torch.randn(32, 784)       # Batch of 32 samples\n",
        "output = model(dummy_input)              # Forward pass\n",
        "```\n",
        "Model Components Summary\n",
        "\n",
        "| Component    | Description                           |\n",
        "| ------------ | ------------------------------------- |\n",
        "| `__init__()` | Define network layers here            |\n",
        "| `forward()`  | Define how input flows through layers |\n",
        "| `nn.Linear`  | Fully connected layer                 |\n",
        "| `F.relu`     | Activation function                   |\n",
        "| `nn.Module`  | Base class for all models in PyTorch  |\n",
        "\n",
        "Example Use Case\n",
        "This structure is commonly used for tasks like:\n",
        "\n",
        "* Image classification (e.g. MNIST, CIFAR-10)\n",
        "* Text classification (with embeddings)\n",
        "* Regression problems (with final layer having no activation)"
      ],
      "metadata": {
        "id": "Ix-6xwbqs51c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q13: What is the significance of tensors in PyTorch?\n",
        "What is a Tensor in PyTorch?\n",
        "\n",
        "A tensor in PyTorch is a multi-dimensional array, similar to a NumPy array, but with added power for GPU acceleration and automatic differentiation. Tensors are the core data structure used in all deep learning computations in PyTorch.\n",
        "Why Tensors Are Important in PyTorch\n",
        "\n",
        "|   Purpose                  | Explanation                                                                                                         |\n",
        "| -------------------------- | ------------------------------------------------------------------------------------------------------------------- |\n",
        "| Data Representation        | Inputs (images, text, audio), weights, and outputs are all represented as tensors.                                  |\n",
        "| Core of Computation        | All operations like matrix multiplication, reshaping, convolution, and activation functions are applied on tensors. |\n",
        "| GPU Acceleration           | PyTorch tensors can be moved to GPU to speed up training: `tensor.to('cuda')`.                                      |\n",
        "| Autograd System            | Tensors support **automatic differentiation** with `requires_grad=True`, which is crucial for backpropagation.      |\n",
        "| Interoperability           | PyTorch tensors can easily convert to and from NumPy arrays using `.numpy()` and `torch.from_numpy()`.              |\n",
        "\n",
        "Example\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Create a tensor\n",
        "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "\n",
        "# Perform operations\n",
        "y = x * 2 + 1\n",
        "z = y.mean()\n",
        "\n",
        "# Backward pass (compute gradients)\n",
        "z.backward()\n",
        "\n",
        "# View gradient of x\n",
        "print(x.grad)\n",
        "```\n",
        "Types of Tensors You’ll Use\n",
        "\n",
        "| Tensor Type      | Shape Example                                       | Use Case         |\n",
        "| ---------------- | --------------------------------------------------- | ---------------- |\n",
        "| Scalar (0D)      | `torch.tensor(3)`                                   | Single value     |\n",
        "| Vector (1D)      | `torch.tensor([1,2])`                               | Feature vector   |\n",
        "| Matrix (2D)      | `torch.tensor([[1,2],[3,4]])`                       | Weight matrices  |\n",
        "| Higher Dim (3D+) | e.g. for images: `(Batch, Channels, Height, Width)` | CNNs, RNNs, etc. |\n",
        "\n",
        "Common Tensor Operations\n",
        "\n",
        "* `x.view()` or `x.reshape()`: reshape\n",
        "* `x.mean()`, `x.sum()`: reduce ops\n",
        "* `x.matmul(y)`: matrix multiplication\n",
        "* `x.to('cuda')`: move to GPU\n",
        "* `x.requires_grad_()`: enable gradient tracking"
      ],
      "metadata": {
        "id": "XLiiRuEqtSGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q14: What is the difference between torch. Tensor and torch.cuda.Tensor in PyTorch?\n",
        "Short Answer:\n",
        "\n",
        "| Aspect          | `torch.Tensor`                            | `torch.cuda.Tensor`                              |\n",
        "| --------------- | ----------------------------------------- | ------------------------------------------------ |\n",
        "| Location        | Stored in CPU memory                      | Stored in GPU memory (CUDA device)               |\n",
        "| Performance     | Slower for large computations             | Faster due to   GPU acceleration                 |\n",
        "| Use Case        | Default for most data loading & debugging | Needed for **training large models efficiently** |\n",
        "| Conversion      | `.to('cuda')`, `.cuda()`                  | `.to('cpu')`, `.cpu()`                           |\n",
        "\n",
        "Understanding with Example\n",
        "\n",
        "1. CPU Tensor (`torch.Tensor`)\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x_cpu = torch.tensor([1.0, 2.0, 3.0])  # Default on CPU\n",
        "print(x_cpu.device)  # Output: cpu\n",
        "```\n",
        "\n",
        "2. GPU Tensor (`torch.cuda.Tensor`)\n",
        "\n",
        "```python\n",
        "x_gpu = torch.tensor([1.0, 2.0, 3.0]).to('cuda')  # Move to GPU\n",
        "print(x_gpu.device)  # Output: cuda:0\n",
        "```\n",
        "\n",
        "> All operations must happen on tensors on the same device (CPU or GPU).\n",
        "\n",
        "Moving Between CPU and GPU\n",
        "\n",
        "```python\n",
        "# From CPU to GPU\n",
        "x_gpu = x_cpu.to('cuda')\n",
        "\n",
        "# From GPU to CPU\n",
        "x_cpu_again = x_gpu.to('cpu')\n",
        "```\n",
        "Common Error\n",
        "```python\n",
        "# This will raise an error\n",
        "result = x_cpu + x_gpu  #  Mixing devices\n",
        "\n",
        "# Fix by moving one tensor\n",
        "result = x_cpu.to('cuda') + x_gpu  #\n",
        "''\n",
        "* `torch.Tensor` = Default CPU tensor\n",
        "* `torch.cuda.Tensor` = GPU tensor for fast training\n",
        "* Use `.to('cuda')` or `.cuda()` to move tensors to GPU\n",
        "* All tensors in an operation **must be on the same device"
      ],
      "metadata": {
        "id": "CLrxphPstjhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q15: What is the purpose of the torch.optim module in PyTorch?\n",
        "\n",
        "The `torch.optim` module provides optimization algorithms (like SGD, Adam, etc.) used to update the weights of neural networks during training by minimizing the loss function.\n",
        "Why It’s Important\n",
        "\n",
        "In deep learning, after calculating the loss, we need to adjust model parameters (weights) to reduce that loss. `torch.optim` provides efficient implementations of popular gradient-based optimizers to do just that.\n",
        "\n",
        "How `torch.optim` Works\n",
        "1. Define the optimizer with model parameters\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "```\n",
        "\n",
        "2. Inside the training loop:\n",
        "\n",
        "```python\n",
        "# Forward pass\n",
        "output = model(inputs)\n",
        "loss = criterion(output, targets)\n",
        "\n",
        "# Backward pass\n",
        "optimizer.zero_grad()   # Clear previous gradients\n",
        "loss.backward()         # Backpropagation\n",
        "optimizer.step()        # Update weights\n",
        "```\n",
        "Common Optimizers in `torch.optim`\n",
        "\n",
        "| Optimizer | Description                                             |\n",
        "| --------- | ------------------------------------------------------- |\n",
        "| `SGD`     | Stochastic Gradient Descent (optionally with momentum)  |\n",
        "| `Adam`    | Adaptive optimizer combining momentum & RMSProp         |\n",
        "| `Adagrad` | Adapts learning rate based on past gradients            |\n",
        "| `RMSprop` | Like Adam but simpler, often used in RNNs               |\n",
        "| `AdamW`   | Adam with decoupled weight decay (used in Transformers) |\n",
        "\n",
        "Key Parameters in Optimizers\n",
        "\n",
        "* `lr`: Learning rate (step size)\n",
        "* `momentum`: For SGD, helps accelerate updates\n",
        "* `weight_decay`: Regularization (L2 penalty)\n",
        "* `betas`: For Adam, controls momentum factors\n",
        "\n",
        "Summary\n",
        "The `torch.optim` module in PyTorch is crucial for:\n",
        "\n",
        "* Adjusting model weights\n",
        "* Minimizing loss\n",
        "* Speeding up convergence with efficient algorithms\n",
        "\n",
        "It’s a central part of any training loop in PyTorch."
      ],
      "metadata": {
        "id": "nsuS8FxOuDG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q16: What are some common activation functions used in neural networks?\n",
        "What is an Activation Function?\n",
        "\n",
        "An activation function introduces non-linearity into a neural network, enabling it to learn complex patterns and relationships in data. Without activation functions, neural networks behave like simple linear models.\n",
        "Most Common Activation Functions\n",
        "\n",
        "| Name                                     | Formula / Function                            | Use Case / Description\n",
        "| ---------------------------------------- | --------------------------------------------- | ------------------------------------------------------------------\n",
        "| ReLU<br>(Rectified Linear Unit)          | `f(x) = max(0, x)`                            | ✅ Most widely used<br>🚀 Fast & simple<br>❌ Can cause \"dying ReLU\"\n",
        "| Leaky ReLU                               | `f(x) = x if x > 0 else αx`                   | 🔁 Variant of ReLU<br>⚠️ Allows small gradient when x < 0\n",
        "| Sigmoid                                  | `f(x) = 1 / (1 + e^-x)`                       | 🧠 Used in binary classification<br>❌ Vanishing gradients\n",
        "| Tanh                                     | `f(x) = (e^x - e^-x)/(e^x + e^-x)`            | 🔄 Outputs in \\[-1, 1]<br>🚫 Also suffers from vanishing gradients\n",
        "| Softmax                                  | `f(xi) = e^xi / Σ e^xj`                       | 🎯 Used in multi-class classification (output layer)\n",
        "| ELU<br>(Exponential Linear Unit)         | `f(x) = x if x > 0 else α(e^x - 1)`           | Smooth version of ReLU<br>Better convergence in some cases\n",
        "| Swish                                    | `f(x) = x * sigmoid(x)`                       | Developed by Google<br>Sometimes better than ReLU\n",
        "| GELU<br>(Gaussian Error Linear Unit)     | `f(x) ≈ 0.5x(1 + tanh(√(2/π)(x + 0.0447x³)))` | 🔬 Used in Transformers (BERT, GPT)\n",
        "| ReLU6                                    | `f(x) = min(max(0, x), 6)`                    | Used in MobileNets for bounded ReLU\n",
        "| Hard Sigmoid / Hard Swish                | Approximations for low-power devices          | 🤖 Efficient for mobile models\n",
        "\n",
        "Quick Comparison\n",
        "\n",
        "| Function | Output Range  | Use In                         |\n",
        "| -------- | ------------- | ------------------------------ |\n",
        "| ReLU     | \\[0, ∞)       | Hidden layers                  |\n",
        "| Sigmoid  | (0, 1)        | Binary classification (output) |\n",
        "| Tanh     | (-1, 1)       | Hidden layers (less common)    |\n",
        "| Softmax  | (0, 1), sum=1 | Multi-class output             |\n",
        "| GELU     | \\~(-∞, ∞)     | NLP & Transformers             |\n",
        "\n",
        "Activation Function Selection Guide\n",
        "\n",
        "| Situation                         | Recommended           |\n",
        "| --------------------------------- | --------------------- |\n",
        "| Most hidden layers                | `ReLU` or `LeakyReLU` |\n",
        "| Binary classification output      | `Sigmoid`             |\n",
        "| Multi-class classification output | `Softmax`             |\n",
        "| Transformer-based models          | `GELU`                |\n",
        "| Small models / edge devices       | `ReLU6`, `HardSwish`  |"
      ],
      "metadata": {
        "id": "k_nz272MuaWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q17: What is the difference between torch.nn.module and torch.nn.sequential in PyTorch?\n",
        "\n",
        "| Feature                  | `torch.nn.Module`                      | `torch.nn.Sequential`                      |\n",
        "| ------------------------ | -------------------------------------- | ------------------------------------------ |\n",
        "| Flexibility              |  Fully customizable                    |  Linear, layer-by-layer stacking only      |\n",
        "| Best for                 | Complex models with loops, conditions  | Simple feedforward networks                |\n",
        "| Forward method           | You define your own `forward()` method | Automatically defines `forward()`          |\n",
        "| Control over logic       | High — full programming control        | Low — fixed order execution                |\n",
        "| Use in real projects     | Almost always used for serious models  | Used for quick prototyping or small models |\n",
        "\n",
        "1. `torch.nn.Module` — Full Custom Model\n",
        "\n",
        "You subclass `nn.Module`, define layers in `__init__()` and computations in `forward()`:\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "Use when:\n",
        "\n",
        "* You need custom logic (like residual connections, multiple inputs/outputs)\n",
        "* You want full control over the forward pass\n",
        "\n",
        "2. `torch.nn.Sequential` — Quick Layer Stack\n",
        "You build the model by stacking layers in a sequence:\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "```\n",
        "\n",
        "Use when:\n",
        "\n",
        "* You have a straightforward, layer-by-layer model\n",
        "* No conditional logic is required\n",
        "* Prototyping or building small MLPs/CNNs\n",
        "\n",
        "Limitations of `nn.Sequential`\n",
        "\n",
        "* Can't handle branches or skip connections (e.g., ResNet)\n",
        "* Can't customize forward behavior\n",
        "* Not suited for RNNs with variable sequence lengths\n",
        "\n",
        "Summary\n",
        "\n",
        "| Use Case                                   | Choose          |\n",
        "| ------------------------------------------ | --------------- |\n",
        "| Simple stack of layers                     | `nn.Sequential` |\n",
        "| Custom computation, branches, conditionals | `nn.Module`     |\n"
      ],
      "metadata": {
        "id": "jXMfUtawu5v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q18: How can you monitor training progress in TensorFlow 2.0?\n",
        "\n",
        "Monitoring training progress helps you track performance, diagnose issues, and optimize model training. TensorFlow 2.0 offers several tools and techniques for this.\n",
        "1. Using `verbose` in `model.fit()`\n",
        "This gives console logs of training progress (loss, accuracy, etc.):\n",
        "```python\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), verbose=1)\n",
        "```\n",
        "* `verbose=0`: Silent\n",
        "* `verbose=1`: Progress bar per epoch\n",
        "* `verbose=2`: One line per epoch\n",
        "\n",
        "2. TensorBoard — Visual Monitoring Tool\n",
        "TensorBoard lets you visualize:\n",
        "* Training/validation loss and accuracy\n",
        "* Learning rates\n",
        "* Histograms of weights and gradients\n",
        "\n",
        "Step-by-step:\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "# Create log directory\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Fit model with TensorBoard callback\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), callbacks=[tensorboard_callback])\n",
        "```\n",
        "Launch TensorBoard:\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir=logs/fit\n",
        "```\n",
        "\n",
        "Open [http://localhost:6006](http://localhost:6006) in your browser.\n",
        "\n",
        "---\n",
        "3. Custom Callbacks\n",
        "You can create a custom callback to print or log any metric:\n",
        "```python\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"Epoch {epoch + 1}: val_loss = {logs['val_loss']:.4f}\")\n",
        "\n",
        "model.fit(..., callbacks=[MyCallback()])\n",
        "```\n",
        "4. Plotting Metrics Manually\n",
        "After training:\n",
        "\n",
        "```python\n",
        "history = model.fit(...)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "Summary\n",
        "\n",
        "| Method           | Use Case                              |\n",
        "| ---------------- | ------------------------------------- |\n",
        "| `verbose`        | Quick console logs                    |\n",
        "| `TensorBoard`    | Best for deep visual insights         |\n",
        "| Custom Callbacks | Advanced monitoring or early stopping |\n",
        "| Manual Plotting  | When using saved history              |"
      ],
      "metadata": {
        "id": "AbP88sLGvXGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q19: How does the keras API fit into TensorFlow 2.0?\n",
        "\n",
        "In TensorFlow 2.0, Keras is the default high-level API for building and training deep learning models. It’s fully integrated into TensorFlow, making it easier to use while retaining flexibility and power.\n",
        "Key Points on Keras in TensorFlow 2.0\n",
        "\n",
        "| Aspect          | Details                                                                                              |\n",
        "| --------------- | ---------------------------------------------------------------------------------------------------- |\n",
        "| Location        | Now accessed via `tensorflow.keras` (instead of standalone `keras`)                                  |\n",
        "| Ease of Use     | High-level, user-friendly API for defining and training models                                       |\n",
        "| Integration     | Seamlessly works with TensorFlow’s backend (e.g., `tf.data`, `tf.function`, distribution strategies) |\n",
        "| Modularity      | Model = layers + loss + optimizer + metrics + callbacks                                              |\n",
        "| Consistency     | Unified API for building both sequential and functional models                                       |\n",
        "\n",
        "How It Fits Together\n",
        "```python\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, validation_split=0.2)\n",
        "```\n",
        "\n",
        "This is pure TensorFlow 2.0 with Keras, under the hood.\n",
        "\n",
        "Keras APIs You Use in TensorFlow 2.0\n",
        "\n",
        "| API Component     | Example                                           |\n",
        "| ----------------- | ------------------------------------------------- |\n",
        "| Layers            | `tf.keras.layers.Dense`, `Conv2D`, `LSTM`         |\n",
        "| Models            | `tf.keras.Model`, `Sequential`                    |\n",
        "| Optimizers        | `tf.keras.optimizers.Adam`, `SGD`                 |\n",
        "| Losses            | `tf.keras.losses.MeanSquaredError`                |\n",
        "| Callbacks         | `tf.keras.callbacks.TensorBoard`, `EarlyStopping` |\n",
        "| Preprocessing     | `tf.keras.preprocessing.image`, `text`            |\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 **Why TensorFlow Adopted Keras Natively**\n",
        "\n",
        "* **Simplified model-building experience**\n",
        "* **Community adoption** — Keras is intuitive for beginners\n",
        "* **Scalable** — Works with advanced TF tools like `tf.function`, `TPU`, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Summary\n",
        "\n",
        "* Keras is **built into TensorFlow 2.0** as `tf.keras`\n",
        "* It's the **official high-level API** for building and training models\n",
        "* Combines ease-of-use with TensorFlow’s **performance and scalability**\n",
        "\n",
        "---\n",
        "\n",
        "Would you like a demo showing how to build a model using both `Sequential` and the functional API in `tf.keras`?\n"
      ],
      "metadata": {
        "id": "s7PYaHWLvvjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q20: What is an example of deep learning Projects that can be implemented using TensorFlow 2.0?\n",
        "\n",
        "Example Project: Handwritten Digit Recognition with MNIST Dataset\n",
        "This is a classic beginner-friendly deep learning project where a model is trained to recognize digits (0–9) from handwritten images.\n",
        "\n",
        "Technologies Used\n",
        "* TensorFlow 2.0 (with Keras API)\n",
        "* MNIST dataset (built-in)\n",
        "* CNN (Convolutional Neural Network)\n",
        "\n",
        "Project Overview\n",
        "\n",
        "| Component     | Description                      |\n",
        "| ------------- | -------------------------------- |\n",
        "| Input         | 28x28 grayscale images of digits |\n",
        "| Model         | Convolutional Neural Network     |\n",
        "| Output        | 10-class softmax (digit 0–9)     |\n",
        "| Objective     | Classify handwritten digits      |\n",
        "| Metric        | Accuracy                         |\n",
        "\n",
        "Step-by-Step Code\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1. Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Reshape data to include channel dimension\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "# 2. Build the model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Train the model\n",
        "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n",
        "```\n",
        "Possible Improvements\n",
        "* Add dropout for regularization\n",
        "* Use data augmentation\n",
        "* Deploy using TensorFlow Lite or TensorFlow\\.js\n"
      ],
      "metadata": {
        "id": "3EPFVmRCwFwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q21: What is the main advantage of using pre-trained models in TensorFlow and PyTorch?\n",
        "The main advantage of using pre-trained models is that they save time and resources by allowing you to leverage existing learned features from large datasets—so you don't have to train a model from scratch.\n",
        "| Benefit                         | Explanation                                                                                           |\n",
        "| ------------------------------- | ----------------------------------------------------------------------------------------------------- |\n",
        "| Faster Development              | No need to train from scratch—just fine-tune                                                          |\n",
        "| Lower Computational Cost        | Reduces need for powerful hardware or long training time                                              |\n",
        "| Better Performance              | Pre-trained on large datasets like ImageNet, COCO, etc., often outperform custom-trained small models |\n",
        "| Transfer Learning               | Can adapt to new but similar tasks with fewer data                                                    |\n",
        "| Ready for Deployment            | Many pre-trained models are production-ready (e.g., MobileNet, BERT, ResNet, YOLO)                    |\n",
        "\n",
        "Example Use Cases\n",
        "| Task                        | Pre-trained Model                  |\n",
        "| --------------------------- | ---------------------------------- |\n",
        "| Image Classification        | `ResNet`, `MobileNet`, `Inception` |\n",
        "| Object Detection            | `Faster R-CNN`, `YOLO`, `SSD`      |\n",
        "| Natural Language Processing | `BERT`, `GPT`, `RoBERTa`           |\n",
        "| Style Transfer              | `VGG`, `CycleGAN`                  |\n",
        "| Face Recognition            | `FaceNet`, `ArcFace`               |\n",
        "\n",
        "How to Use in Practice\n",
        "In TensorFlow:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
        "```\n",
        "\n",
        "In PyTorch:\n",
        "\n",
        "```python\n",
        "from torchvision import models\n",
        "\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "```\n",
        "Fine-Tuning vs Feature Extraction**\n",
        "\n",
        "| Mode                   | Description                                                      |\n",
        "| ---------------------- | ---------------------------------------------------------------- |\n",
        "| Feature Extraction     | Freeze all layers and use the model as a fixed feature extractor |\n",
        "| Fine-Tuning            | Unfreeze some layers and train the model further on your data    |\n"
      ],
      "metadata": {
        "id": "AEntHc3awewI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "hloXUbPywyku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q1: How do you install and verify that TensorFlow 2.0 was installed successfully?\n",
        "Step 1: Install TensorFlow 2.0\n",
        "\n",
        "You can install it using pip:\n",
        "\n",
        "```bash\n",
        "pip install tensorflow==2.0.0\n",
        "```\n",
        "\n",
        "Note: If you want the latest TensorFlow 2.x version, simply run:\n",
        "\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```\n",
        "Step 2: Verify Installation\n",
        "\n",
        "Open Python or Jupyter Notebook and run:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "```\n",
        "\n",
        "If it outputs something like `2.0.0` or higher (e.g., `2.15.0`), TensorFlow 2.x is installed correctly.\n",
        "\n",
        "Optional: Check for GPU Support\n",
        "\n",
        "```python\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "```\n",
        "\n",
        "If you have a GPU and TensorFlow is set up properly with CUDA, you'll see:\n",
        "\n",
        "```\n",
        "Num GPUs Available: 1\n",
        "```\n",
        " Troubleshooting Tips\n",
        "\n",
        "| Issue            | Solution                                                                   |\n",
        "| ---------------- | -------------------------------------------------------------------------- |\n",
        "| `ImportError`    | Ensure you're using the right Python environment (e.g., virtualenv, conda) |\n",
        "| Wrong version    | Use `pip install tensorflow==2.0.0 --upgrade`                              |\n",
        "| GPU not detected | Install `tensorflow-gpu==2.0.0` and set up CUDA & cuDNN properly           |\n"
      ],
      "metadata": {
        "id": "77g0Li6sw51U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q2: How can you define a simple function in TensorFlow 2.0 to perform addition?\n",
        "\n",
        "You can define a function for addition in TensorFlow 2.0 using both regular Python functions and the `@tf.function` decorator (for graph optimization).\n",
        "1. Basic Tensor Addition Function**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "def add_tensors(x, y):\n",
        "    return tf.add(x, y)\n",
        "\n",
        "# Example usage\n",
        "a = tf.constant(3)\n",
        "b = tf.constant(5)\n",
        "result = add_tensors(a, b)\n",
        "print(\"Result:\", result.numpy())  # Output: 8\n",
        "```\n",
        "2. Optimized with `@tf.function`\n",
        "\n",
        "To convert the function into a TensorFlow graph (for performance):\n",
        "\n",
        "```python\n",
        "@tf.function\n",
        "def add_tensors_graph(x, y):\n",
        "    return tf.add(x, y)\n",
        "\n",
        "# Example usage\n",
        "result = add_tensors_graph(10, 15)\n",
        "print(\"Graph Result:\", result.numpy())  # Output: 25\n",
        "```\n",
        "\n",
        "> `@tf.function` compiles the function into a TensorFlow **computational graph**, making it faster and suitable for deployment.\n",
        "\n",
        "| Method           | Use                       |\n",
        "| ---------------- | ------------------------- |\n",
        "| Regular function | Quick prototyping         |\n",
        "| `@tf.function`   | Optimized graph execution |\n"
      ],
      "metadata": {
        "id": "3SA7ZS0fxgtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3: How can you create a simple neural network in TensorFlow 2.0 with one hidden layer?\n",
        "You can create a simple neural network using the **Keras API** built into TensorFlow 2.0. Below is an example with **one hidden layer.\n",
        "Step-by-Step Example\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1. Build the model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(16, activation='relu', input_shape=(10,)),  # Hidden layer with 16 neurons\n",
        "    layers.Dense(1, activation='sigmoid')                    # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 3. Create dummy data\n",
        "import numpy as np\n",
        "x_train = np.random.rand(100, 10)      # 100 samples, 10 features\n",
        "y_train = np.random.randint(2, size=100)  # Binary labels (0 or 1)\n",
        "\n",
        "# 4. Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=10)\n",
        "```\n",
        "Explanation of Layers\n",
        "\n",
        "| Layer                            | Purpose                                |\n",
        "| -------------------------------- | -------------------------------------- |\n",
        "| `Dense(16, activation='relu')`   | Hidden layer with 16 units using ReLU  |\n",
        "| `Dense(1, activation='sigmoid')` | Output layer for binary classification |\n",
        "\n",
        "> For multiclass classification, change the output to `Dense(num_classes, activation='softmax')` and use `sparse_categorical_crossentropy`.\n"
      ],
      "metadata": {
        "id": "GUbwF2MVxwge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q4: How can you visualize the training progress using TensorFlow and Matplotlib?\n",
        "\n",
        "You can visualize training and validation loss/accuracy over epochs using Matplotlib by accessing the `history` object returned by `model.fit()`.\n",
        "Step-by-Step Example\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Dummy data\n",
        "import numpy as np\n",
        "x_train = np.random.rand(100, 10)\n",
        "y_train = np.random.randint(2, size=100)\n",
        "\n",
        "# Train the model and store training history\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=8, validation_split=0.2)\n",
        "```\n",
        "Plotting the Training Progress\n",
        "\n",
        "```python\n",
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "msJGB_E-yTJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q5: How do you install PyTorch and verify the PyTorch installation?\n",
        "Step 1: Install PyTorch\n",
        "Using pip (CPU-only version):\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision torchaudio\n",
        "```\n",
        "For GPU support:\n",
        "\n",
        "Visit the official [PyTorch Installation Guide](https://pytorch.org/get-started/locally/) and select your OS, package manager, and CUDA version. Example (for CUDA 11.8):\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "```\n",
        "Step 2: Verify Installation in Python\n",
        "\n",
        "```python\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "```\n",
        "Expected Output (for CPU):\n",
        "\n",
        "```\n",
        "PyTorch version: 2.x.x\n",
        "CUDA available: False\n",
        "CUDA device count: 0\n",
        "```\n",
        "Expected Output (for GPU):\n",
        "\n",
        "```\n",
        "PyTorch version: 2.x.x\n",
        "CUDA available: True\n",
        "CUDA device count: 1\n",
        "```\n",
        " Troubleshooting Tips\n",
        "\n",
        "| Issue                                          | Solution                                                           |\n",
        "| ---------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| `ModuleNotFoundError: No module named 'torch'` | Run `pip install torch` in the correct environment                 |\n",
        "| CUDA not available                             | Ensure proper NVIDIA driver, CUDA toolkit, and cuDNN are installed |\n",
        "| Wrong version                                  | Specify version with `pip install torch==2.x.x`                    |\n"
      ],
      "metadata": {
        "id": "CTghyae9yVvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q6: How do you create a simple neural network in PyTorch?\n",
        "\n",
        "You can create a simple neural network in PyTorch using the `torch.nn` module and the `forward()` method for forward propagation.\n",
        "Example: Simple Neural Network with One Hidden Layer\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Define the Neural Network class\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 16)  # Input layer → Hidden layer (10 → 16 neurons)\n",
        "        self.fc2 = nn.Linear(16, 1)   # Hidden layer → Output layer (16 → 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))      # ReLU activation\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid output for binary classification\n",
        "        return x\n",
        "\n",
        "# 2. Initialize the model\n",
        "model = SimpleNN()\n",
        "\n",
        "# 3. Define loss function and optimizer\n",
        "criterion = nn.BCELoss()              # Binary Cross Entropy\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 4. Dummy data (100 samples, 10 features)\n",
        "x_train = torch.randn(100, 10)\n",
        "y_train = torch.randint(0, 2, (100, 1)).float()\n",
        "\n",
        "# 5. Training loop (basic)\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(x_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
        "```\n",
        "Key Components\n",
        "\n",
        "| Component          | Description                    |\n",
        "| ------------------ | ------------------------------ |\n",
        "| `nn.Linear`        | Fully connected (dense) layer  |\n",
        "| `F.relu`           | Activation function            |\n",
        "| `nn.Module`        | Base class for neural networks |\n",
        "| `optimizer.step()` | Updates weights                |\n",
        "| `loss.backward()`  | Backpropagation                |\n"
      ],
      "metadata": {
        "id": "tm0KI5eiyovw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q7: How do you define a loss function and optimizer in PyTorch?\n",
        "\n",
        "In PyTorch, you define the **loss function** to measure prediction error, and the **optimizer** to update model weights during training.\n",
        "1. Loss Function\n",
        "\n",
        "PyTorch provides several built-in loss functions in `torch.nn`.\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "```\n",
        " Other common loss functions:\n",
        "\n",
        "| Task                      | Loss Function            | Code                    |\n",
        "| ------------------------- | ------------------------ | ----------------------- |\n",
        "| Regression                | Mean Squared Error (MSE) | `nn.MSELoss()`          |\n",
        "| Binary Classification     | Binary Cross Entropy     | `nn.BCELoss()`          |\n",
        "| Multiclass Classification | Cross Entropy            | `nn.CrossEntropyLoss()` |\n",
        "\n",
        "2. Optimizer\n",
        "\n",
        "PyTorch optimizers are in `torch.optim`. You pass the model’s parameters and a learning rate.\n",
        "Example: Adam Optimizer\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "```\n",
        "Other common optimizers:\n",
        "\n",
        "| Optimizer | Description                                             |\n",
        "| --------- | ------------------------------------------------------- |\n",
        "| `SGD`     | Stochastic Gradient Descent                             |\n",
        "| `Adam`    | Adaptive Moment Estimation (recommended for most tasks) |\n",
        "| `RMSprop` | Root Mean Square Propagation                            |\n",
        "\n",
        "Usage in Training Loop\n",
        "\n",
        "```python\n",
        "optimizer.zero_grad()       # Clear previous gradients\n",
        "outputs = model(x_train)    # Forward pass\n",
        "loss = criterion(outputs, y_train)  # Compute loss\n",
        "loss.backward()             # Backward pass\n",
        "optimizer.step()            # Update weights\n",
        "```"
      ],
      "metadata": {
        "id": "brvKpLcPy67a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q8: How do you implement a custom loss function in PyTorch?\n",
        "\n",
        "You can implement a custom loss function in PyTorch by either:\n",
        "\n",
        "1. Writing a simple Python function using tensor operations\n",
        "2. Creating a custom subclass of `torch.nn.Module`\n",
        "\n",
        "1. Simple Custom Loss Function (L1 Distance Example)\n",
        "```python\n",
        "import torch\n",
        "\n",
        "def custom_l1_loss(output, target):\n",
        "    return torch.mean(torch.abs(output - target))\n",
        "```\n",
        "Example usage:\n",
        "\n",
        "```python\n",
        "output = torch.tensor([0.5, 0.7], requires_grad=True)\n",
        "target = torch.tensor([1.0, 0.0])\n",
        "loss = custom_l1_loss(output, target)\n",
        "loss.backward()\n",
        "print(\"Loss:\", loss.item())\n",
        "```\n",
        "2. Advanced Custom Loss Using `nn.Module`\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss = torch.mean((output - target) ** 2)\n",
        "        return loss\n",
        "```\n",
        "Example:\n",
        "\n",
        "```python\n",
        "criterion = CustomMSELoss()\n",
        "output = torch.tensor([0.2, 0.4], requires_grad=True)\n",
        "target = torch.tensor([0.0, 1.0])\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "```\n",
        "Why Use Custom Loss Functions?\n",
        "\n",
        "* Tailor the loss to your domain (e.g., weighted losses for class imbalance)\n",
        "* Combine multiple objectives\n",
        "* Implement domain-specific constraints\n"
      ],
      "metadata": {
        "id": "XlN2YCIEzLBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q9: How do you save and load a TensorFlow model?\n",
        "TensorFlow makes it easy to **save** and **load** models using either the SavedModel format or HDF5 (.h5) format.\n",
        "\n",
        "Saving a Model\n",
        "A. Save in `SavedModel` Format (Recommended)\n",
        "\n",
        "```python\n",
        "model.save('my_model')  # Saves as a folder (my_model/)\n",
        "```\n",
        "B. Save in `HDF5` Format\n",
        "\n",
        "```python\n",
        "model.save('my_model.h5')  # Saves as a single .h5 file\n",
        "```\n",
        "2. Loading a Saved Model\n",
        "A. Load from `SavedModel` Folder\n",
        "\n",
        "```python\n",
        "loaded_model = tf.keras.models.load_model('my_model')\n",
        "```\n",
        "B. Load from `.h5` File\n",
        "\n",
        "```python\n",
        "loaded_model = tf.keras.models.load_model('my_model.h5')\n",
        "```\n",
        "Example\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Save the model\n",
        "model.save(\"saved_model\")\n",
        "\n",
        "# Load it back\n",
        "new_model = tf.keras.models.load_model(\"saved_model\")\n",
        "\n",
        "# Verify it's the same\n",
        "new_model.summary()\n",
        "```"
      ],
      "metadata": {
        "id": "_zYts4aBzeSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}