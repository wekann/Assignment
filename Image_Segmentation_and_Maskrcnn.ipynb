{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnDQ9HpRigkus2mi2/J4Xr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wekann/Assignment/blob/main/Image_Segmentation_and_Maskrcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory"
      ],
      "metadata": {
        "id": "Ivvj9fS8DuBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czMLTIhpDXOl"
      },
      "outputs": [],
      "source": [
        "'''Q1. What is Image Segmentation and Why Is It Important?\n",
        "Definition:\n",
        "Image Segmentation is the process of partitioning an image into multiple segments (sets of pixels) to simplify or change the representation of an image into something more meaningful and easier to analyze.\n",
        "Each segment corresponds to specific objects or regions, such as cars, humans, sky, or background.\n",
        "\n",
        "Types of Image Segmentation:\n",
        "| Type                  | Description                                                                                                         |\n",
        "| ----------------------| ------------------------------------------------------------------------------------------------------------------- |\n",
        "| Semantic Segmentation | Labels each pixel with a class (e.g., all pixels of \"car\" as class 1). No distinction between individual objects.   |\n",
        "| Instance Segmentation | Labels each pixel with both class and instance (e.g., \"car 1\", \"car 2\" separately). Used in Mask R-CNN.             |\n",
        "| Panoptic Segmentation | Combination of both ‚Äî semantic + instance segmentation in one framework.                                            |\n",
        "\n",
        "Why Is Image Segmentation Important?\n",
        "| Use Case / Benefit    | Explanation                                                                   |\n",
        "| ----------------------| ----------------------------------------------------------------------------- |\n",
        "| Precision Detection   | Goes beyond bounding boxes‚Äîdetects exact shape of objects at pixel level.     |\n",
        "| Medical Imaging       | Segmenting tumors, organs, etc., with high accuracy.                          |\n",
        "| Autonomous Driving    | Detecting lanes, pedestrians, traffic signs on a pixel level.                 |\n",
        "| Industrial Automation | Quality control, defect detection in manufacturing lines.                     |\n",
        "| Robotics              | Precise object manipulation using segmented regions.                          |\n",
        "| Image Editing & AR    | Background removal, virtual try-on, scene understanding.                      |"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2. Difference Between Image Classification, Object Detection, and Image Segmentation\n",
        "\n",
        "These three are core tasks in computer vision, each offering a different level of understanding:\n",
        "1. Image Classification\n",
        "Goal:\n",
        "Determine what is in an image.\n",
        "**üñºÔ∏è Input: An image\n",
        "**üßæ Output: A label (e.g., `cat`, `car`, `dog`)\n",
        "\n",
        "| Feature     | Value                           |\n",
        "| ----------- | ------------------------------- |\n",
        "| Granularity | Whole image                     |\n",
        "| Output      | Single class (or multi-label)   |\n",
        "| Example     | ‚ÄúThis image contains a dog‚Äù     |\n",
        "\n",
        "2 Object Detection\n",
        "Goal:\n",
        "Detect what objects are in an image and where they are using bounding boxes.\n",
        "Input:An image\n",
        "Output:Classes + bounding boxes + confidence scores\n",
        "| Feature     | Value                                   |\n",
        "| ----------- | --------------------------------------- |\n",
        "| Granularity | Object-level                            |\n",
        "| Output      | Coordinates + class labels              |\n",
        "| Example     | ‚ÄúThere‚Äôs a dog at (x1, y1, x2, y2)‚Äù |\n",
        "\n",
        "3. Image Segmentation\n",
        "Goal:\n",
        "Classify each pixel of the image.\n",
        "a. semantic Segmentation\n",
        "Classify pixels by class only (e.g., all \"cats\" are one class).\n",
        "\n",
        "b. Instance Segmentation\n",
        "Classify pixels by both class and object instance (e.g., two separate \"cats\" have two masks).\n",
        "| Feature     | Value                                                   |\n",
        "| ----------- | ------------------------------------------------------- |\n",
        "| Granularity | Pixel-level                                             |\n",
        "| Output      | Mask per object or class                                |\n",
        "| Example     | ‚ÄúEvery pixel in the image is dog or background‚Äù |"
      ],
      "metadata": {
        "id": "RXYSdHtyE9gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3. What is Mask R-CNN and How Is It Different from Traditional Object Detection Models?\n",
        "What is Mask R-CNN?\n",
        "Mask R-CNN is a deep learning model that extends Faster R-CNN to perform instance segmentation ‚Äî it not only detects objects and their bounding boxes, but also generates pixel-level segmentation masks for each individual object.\n",
        "\n",
        "It is one of the most powerful models for tasks where understanding the exact shape and location of each object is important.\n",
        "\n",
        "#Mask R-CNN Architecture Overview\n",
        "\n",
        "Mask R-CNN is built on top of Faster R-CNN with an additional branch:\n",
        "\n",
        "| Component                     | Purpose                                                                 |\n",
        "| ------------------------------| ----------------------------------------------------------------------- |\n",
        "| Backbone (e.g., ResNet + FPN) | Feature extraction from input image                                     |\n",
        "| Region Proposal Network (RPN) | Proposes candidate object regions (RoIs)                                |\n",
        "| RoI Align                     | Precisely aligns features from regions for further processing           |\n",
        "| Classification Head           | Predicts class label of each object                                     |\n",
        "| Bounding Box Regressor        | Refines object location                                                 |\n",
        "| Mask Head (NEW)               | Predicts a binary mask for each detected object ‚Äî pixel-wise region     |\n",
        "\n",
        "Traditional Object Detection vs. Mask R-CNN\n",
        "\n",
        "| Feature                 | Traditional Detectors (e.g., Faster R-CNN, YOLO) | Mask R-CNN                               |\n",
        "| ------------------------| ------------------------------------------------ | ---------------------------------------- |\n",
        "|  Bounding Boxes         |  Yes                                             |  Yes                                     |\n",
        "|  Class Labels           |  Yes                                             |  Yes                                     |\n",
        "|  Segmentation Masks     | No                                               |  Yes (per instance!)                     |\n",
        "|  Accuracy of Shape Info | Low ‚Äî box only                                   | High ‚Äî per-pixel prediction              |\n",
        "|  Task Type              | Object Detection                                 | Instance Segmentation (detection + mask) |\n",
        "\n",
        "Use Cases of Mask R-CNN\n",
        "\n",
        "| Industry       | Use Case                                  |\n",
        "| -------------- | ----------------------------------------- |\n",
        "|  Medical       | Tumor segmentation in MRIs                |\n",
        "|  Automotive    | Lane/vehicle segmentation in self-driving |\n",
        "|  Manufacturing | Defect detection and shape analysis       |\n",
        "|  Biology       | Cell instance detection                   |\n"
      ],
      "metadata": {
        "id": "WzqmsgsiFbIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q4. What Role Does the `RoIAlign` Layer Play in Mask R-CNN?\n",
        "\n",
        "In Mask R-CNN, the model needs to extract features from each proposed region (RoI: Region of Interest) and align them accurately for mask prediction and classification.\n",
        "\n",
        "What Is `RoIAlign`?\n",
        "`RoIAlign` (Region of Interest Align) is a crucial layer in Mask R-CNN that accurately extracts fixed-size feature maps from variable-size regions (RoIs) on the input feature map ‚Äî without losing spatial alignment.\n",
        "\n",
        "Solution: RoIAlign\n",
        "\n",
        "| Feature             | Description                                                                      |\n",
        "| ------------------- | -------------------------------------------------------------------------------- |\n",
        "|  No Quantization    | Uses floating-point coordinates to keep exact alignment.                         |\n",
        "|  Interpolation      | Applies bilinear interpolation to get pixel values ‚Äî ensures high precision.     |\n",
        "|  Consistent Size    | Outputs fixed-size (e.g., 7√ó7 or 14√ó14) feature maps for each RoI.               |\n",
        "\n",
        "Role in Mask R-CNN:\n",
        "| Component                | Purpose                                                              |\n",
        "| ------------------------ | -------------------------------------------------------------------- |\n",
        "| RoIAlign                 | Extracts feature map for each RoI accurately                         |\n",
        "| Mask Head                | Uses RoI-aligned features to generate precise segmentation masks     |\n",
        "| Classification/Box Heads | Also use RoI-aligned features for better predictions                 |"
      ],
      "metadata": {
        "id": "MSXEnDTjF3TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q5. What Are Semantic, Instance, and Panoptic Segmentation?\n",
        "\n",
        "These are three main types of image segmentation techniques in computer vision, each providing a different level of detail and understanding of objects in an image.\n",
        "1. Semantic Segmentation\n",
        "Definition: Assigns each pixel in the image a class label (e.g., road, sky, car), but does not distinguish between different instances of the same class.\n",
        "Eg:An image with 3 cars will label all their pixels as \"car\" ‚Äî no distinction between Car 1, Car 2, etc.\n",
        "\n",
        "2.Instance Segmentation\n",
        "Definition: Assigns each pixel a class label and an object instance ID. This means it differentiates between individual objects of the same class.\n",
        "Eg:The same 3 cars will each get separate masks: Car 1, Car 2, Car 3\n",
        "\n",
        "3. Panoptic Segmentation\n",
        "Definition:\n",
        "Combines semantic and instance segmentation into a single, unified output.\n",
        "Eg:\n",
        "* \"Stuff\" classes (like sky, road) ‚Üí labeled semantically\n",
        "* \"Thing\" classes (like people, cars) ‚Üí labeled with instance masks\n"
      ],
      "metadata": {
        "id": "ZKDMjKZ1GUgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q6. Describe the Role of Bounding Boxes and Masks in Image Segmentation Models\n",
        "\n",
        "In image segmentation models ‚Äî especially instance segmentation models like Mask R-CNN ‚Äî both **bounding boxes and masks play complementary roles in detecting and understanding objects.\n",
        "\n",
        "1. Bounding Boxes\n",
        "What They Are:Bounding boxes are rectangular regions that tightly enclose an object in the image.\n",
        "Role in Image Segmentation:\n",
        "* Locate objects spatially within the image.\n",
        "* Define the region of interest (RoI) where the model should focus to generate masks.\n",
        "* Serve as a coarse-level detection before detailed mask prediction.\n",
        "\n",
        "2. Segmentation Masks\n",
        "What They Are: Masks are pixel-level binary maps that indicate the exact shape of an object.\n",
        "Role in Image Segmentation:\n",
        "* Classify each pixel as belonging to an object (1) or not (0).\n",
        "* Provide fine-grained details ‚Äî e.g., ears of a cat, fingers of a hand.\n",
        "* Enable precise object extraction and manipulation.\n"
      ],
      "metadata": {
        "id": "z2PTfUmrGomh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q7. What Is the Purpose of Data Annotation in Image Segmentation?**\n",
        "\n",
        "What Is Data Annotation?\n",
        "Data annotation in image segmentation refers to the process of labeling each pixel in an image with a class (semantic) or a class + object instance ID (instance).\n",
        "This annotated data serves as ground truth for training machine learning models ‚Äî especially segmentation models like Mask R-CNN, UNet, or DeepLab.\n",
        "\n",
        "Why Data Annotation Is Important in Segmentation:\n",
        "| Purpose                         | Explanation                                                                  |\n",
        "| ------------------------------- | ---------------------------------------------------------------------------- |\n",
        "| Supervised Learning             | Segmentation models need labeled data to learn how to identify objects.  |\n",
        "| Pixel-Level Accuracy            | High-quality masks help the model learn precise boundaries of objects.   |\n",
        "| Model Evaluation                | Annotations serve as the baseline for comparing model predictions.       |\n",
        "| Class + Instance Recognition    | Helps the model understand what the object is and which one it is.   |\n",
        "| Transfer Learning & Fine-Tuning | Pretrained models can be fine-tuned on new, annotated segmentation datasets. |"
      ],
      "metadata": {
        "id": "2m04Zr6eHcTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q8. How Does Detectron2 Simplify Model Training for Object Detection and Segmentation Tasks?\n",
        "\n",
        "Detectron2 is a powerful, modular open-source library by Facebook AI Research (FAIR) for training and deploying object detection, instance segmentation, and semantic segmentation models.\n",
        "It is built on PyTorch, and it significantly simplifies the entire deep learning pipeline ‚Äî from model setup to training and evaluation.\n",
        "\n",
        "Key Ways Detectron2 Simplifies Model Training:\n",
        "1. Pretrained Model Zoo (Plug-and-Play)\n",
        "* Provides ready-to-use pretrained models for:\n",
        "  * Faster R-CNN\n",
        "  * Mask R-CNN\n",
        "  * RetinaNet\n",
        "  * Panoptic FPN, etc.\n",
        "\n",
        "2.Modular Configuration System\n",
        "* Uses `.yaml` config files (and Python overrides) to control:\n",
        "  * Model architecture\n",
        "  * Dataset\n",
        "  * Optimizer and learning rate\n",
        "  * Batch size, augmentation, etc.\n",
        "\n",
        "3.Built-in Dataset Support\n",
        "* Supports popular datasets like COCO, Pascal VOC, Cityscapes by default.\n",
        "* Easily registers custom datasets using a few lines of code.\n",
        "Just provide annotations in COCO format or register a custom loader.\n",
        "\n",
        "4. Automatic Logging, Evaluation, and Visualization\n",
        "* Logs training/validation loss, AP, learning rate, etc.\n",
        "* Built-in support for **COCO-style metrics** like mAP.\n",
        "* Includes tools to visualize:\n",
        "  * Ground truth\n",
        "  * Model predictions\n",
        "  * Segmentation masks\n",
        "Reduces manual debugging and improves experiment tracking.\n",
        "\n",
        "5.Easy Inference API\n",
        "* Run inference on images/videos with a single call.\n",
        "* Automatically draws bounding boxes, labels, masks.\n",
        "\n",
        "6. Advanced Features Out-of-the-Box\n",
        "* Mixed precision (FP16) training\n",
        "* Multi-GPU and distributed training\n",
        "* Support for panoptic segmentation, keypoints, and densepose\n",
        "* Custom model extensions (e.g., new backbones or heads)"
      ],
      "metadata": {
        "id": "wlzOo7ngHxTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q9. Why Is Transfer Learning Valuable in Training Segmentation Models?\n",
        "\n",
        "What is Transfer Learning?\n",
        "Transfer learning is a machine learning technique where a model pretrained on a large dataset (like COCO or ImageNet) is fine-tuned on a smaller, task-specific dataset.\n",
        "In segmentation, this means using a model like Mask R-CNN with pretrained weights, and adapting it to your own custom segmentation task.\n",
        "Why Transfer Learning Is Valuable in Segmentation:\n",
        "1.Faster Training\n",
        "* Pretrained models already ‚Äúknow‚Äù low-level features like edges, shapes, textures.\n",
        "* Your model starts from a strong base, requiring fewer training epochs.\n",
        "You don't need to train from scratch (which can take days or weeks).\n",
        "\n",
        "2.Better Accuracy with Less Data\n",
        "* Segmentation requires pixel-level annotations, which are costly and time-consuming.\n",
        "* Transfer learning enables high performance even with small annotated datasets.\n",
        "COCO-pretrained models bring rich visual understanding to your domain.\n",
        "\n",
        "3.Reduces Overfitting\n",
        "* Small datasets = high risk of overfitting.\n",
        "* A pretrained backbone reduces that risk by using generalized features learned from large datasets.\n",
        "\n",
        "4. Efficient Use of Resources\n",
        "* Saves GPU hours and engineering effort.\n",
        "* You can focus on data quality and fine-tuning hyperparameters rather than full training.\n",
        "\n",
        "5.Customizability\n",
        "* You can fine-tune:\n",
        "  * Only the mask head (for segmentation)\n",
        "  * Or the entire model (for domain-specific tasks)\n",
        "* Great for domain adaptation (e.g., medical, satellite, manufacturing).\n"
      ],
      "metadata": {
        "id": "wnOBmSqyIOGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q10. How Does Mask R-CNN Improve Upon the Faster R-CNN Model Architecture?\n",
        "\n",
        "Faster R-CNN is an object detection model that identifies bounding boxes and class labels for objects.\n",
        "Mask R-CNN extends Faster R-CNN to perform instance segmentation ‚Äî i.e., detecting each object and outlining its exact shape at the pixel level.\n",
        "\n",
        "1. Adds a Mask Head\n",
        "* Mask R-CNN introduces a third branch in the network that:\n",
        "  * Takes RoI-aligned features\n",
        "  * Outputs a binary mask (segmentation map) for each object\n",
        "* The mask is predicted in parallel with class label and bounding box.\n",
        "Each RoI gets a 28√ó28 mask (default), which is then resized to fit the object in the original image.\n",
        "\n",
        "2. Uses RoIAlign Instead of RoIPool\n",
        "* RoIPool in Faster R-CNN rounds off floating-point coordinates, which can misalign object features.\n",
        "* RoIAlign in Mask R-CNN:\n",
        "  * Uses bilinear interpolation\n",
        "  * Preserves exact spatial locations\n",
        "  * Leads to higher-quality masks and better performance\n",
        "\n",
        "3. Improved Performance with Minimal Extra Cost\n",
        "* Despite the added segmentation head, Mask R-CNN is:\n",
        "  * Fast\n",
        "  * Modular(can be plugged into any detection framework)\n",
        "  * Flexible(used for panoptic segmentation, keypoints, etc.)\n"
      ],
      "metadata": {
        "id": "mrMeBcicJMQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q11. What Is Meant by \"From Bounding Box to Polygon Masks\" in Image Segmentation?\n",
        "\n",
        "\"From bounding box to polygon masks\" refers to the evolution of object detection models from using simple rectangular boxes to representing objects with precise, flexible shapes ‚Äî typically using polygons or binary masks.\n",
        "1. Bounding Boxes (Traditional Object Detection)\n",
        "* A rectangle defined by `(x_min, y_min, x_max, y_max)` that **roughly encloses** an object.\n",
        "* Used in models like:\n",
        "  * YOLO\n",
        "  * Faster R-CNN\n",
        "Pros:\n",
        "* Simple\n",
        "* Fast\n",
        "* Good for coarse object localization\n",
        "\n",
        "Cons:\n",
        "* Can include a lot of background pixels\n",
        "* Not precise for non-rectangular objects\n",
        "* Not suitable for pixel-level tasks\n",
        "\n",
        "2. Polygon Masks / Binary Masks (Segmentation)\n",
        "* A mask defines the exact shape of the object.\n",
        "* Can be stored as:\n",
        "  * Polygon coordinates (e.g., COCO format)\n",
        "  * Pixel-level binary mask (1 = object, 0 = background)\n"
      ],
      "metadata": {
        "id": "yugoIyCXJoi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q12. How Does Data Augmentation Benefit Image Segmentation Model Training?\n",
        "\n",
        "What is Data Augmentation?\n",
        "Data augmentation is the process of creating modified versions of existing training images to artificially increase the size and diversity of a dataset ‚Äî without collecting new data.\n",
        "In image segmentation, this involves augmenting both the image and its corresponding segmentation mask** so the model sees more variety during training.\n",
        "\n",
        "Why Is Data Augmentation Important for Segmentation?\n",
        "| Challenge            | Solution via Augmentation                                |\n",
        "| ---------------------| -------------------------------------------------------- |\n",
        "| Limited Labeled Data | Generate more training samples from the same data        |\n",
        "| Overfitting          | Expose model to diverse conditions, reduce memorization  |\n",
        "| Poor Generalization  | Learn invariance to changes in scale, rotation, lighting |\n",
        "| Unbalanced Classes   | Improve training on rare object appearances              |\n",
        "\n",
        "Common Augmentations Used in Segmentation\n",
        "Both image and mask must be transformed identically!\n",
        "| Technique                | What It Does                    | Why It Helps                               |\n",
        "| -------------------------| ------------------------------- | ------------------------------------------ |\n",
        "| Horizontal/Vertical Flip | Flips image + mask              | Helps learn symmetrical patterns           |\n",
        "| Rotation                 | Rotates object and mask         | Improves orientation robustness            |\n",
        "| Zoom/Scale               | Zooms in/out on object          | Teaches size/scale invariance              |\n",
        "| Random Crop              | Crops parts of image            | Improves local context recognition         |\n",
        "| Brightness/Contrast      | Alters image lighting           | Makes model lighting-invariant             |\n",
        "| Color Jitter             | Adds random color variations    | Helps model ignore irrelevant color shifts |\n",
        "| Elastic Transform        | Deforms shapes smoothly         | Improves robustness to shape distortions   |\n",
        "| Noise Injection          | Adds Gaussian/salt-pepper noise | Makes model robust to real-world noise     |\n"
      ],
      "metadata": {
        "id": "7o-M01y1KlZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q13. Describe the architecture of mask R_CNN, focusing on the backbone region proposasl network(RPN) and segmentation mask head\n",
        "\n",
        "Overview of Mask R-CNN Architecture\n",
        "Mask R-CNN is an instance segmentation model that extends Faster R-CNN by adding a third branch to predict pixel-wise masks.\n",
        "\n",
        "It has three main components:\n",
        "1. Backbone ‚Äî Feature extraction\n",
        "2. Region Proposal Network (RPN) ‚Äî Object region proposals\n",
        "3. Heads:\n",
        "   * Classification + Bounding Box Head\n",
        "   * Mask Head (new in Mask R-CNN)\n",
        "\n",
        "1. Backbone Network\n",
        "Used to extract deep feature maps from the input image.\n",
        "* Common choices: ResNet-50, ResNet-101, ResNeXt\n",
        "* Often combined with Feature Pyramid Network (FPN) for multiscale feature representation.\n",
        "\n",
        "Role:\n",
        "* Converts the input image into a compact, high-level feature map\n",
        "* Supports detecting objects at multiple scales and resolutions\n",
        "\n",
        "2. Region Proposal Network (RPN)\n",
        "> Proposes regions in the image that are likely to contain objects.\n",
        "How it Works:\n",
        "* Slides a small window (3√ó3) across the backbone feature map.\n",
        "* At each position, it generates anchor boxes of different scales/aspect ratios.\n",
        "* Predicts:\n",
        "  * Objectness score (is there an object?)\n",
        "  * Coordinates of refined boxes\n",
        "\n",
        "3. RoIAlign (instead of RoIPool)\n",
        "> Fixes misalignment issues in Faster R-CNN caused by quantization in RoIPool.\n",
        "* Uses bilinear interpolation to precisely align RoI features\n",
        "* Ensures better spatial accuracy, especially important for segmentation\n",
        "\n",
        "4. Heads (Fully Connected Layers)\n",
        "a.Classification & Bounding Box Regression Head\n",
        "* For each RoI, predicts:\n",
        "  * Class label\n",
        "  * Refined bounding box coordinates\n",
        "\n",
        "b.Segmentation Mask Head (New in Mask R-CNN)\n",
        "Adds a pixel-wise binary mask output for each object.\n",
        "* A small FCN (Fully Convolutional Network):\n",
        "  * Usually: 4 convolution layers + 1 deconvolution (upsample) + sigmoid\n",
        "* Predicts a binary mask (e.g., 28√ó28) per class, for each object instance\n",
        "The mask is only produced for the predicted class (not all classes).\n"
      ],
      "metadata": {
        "id": "M7Anr1iENN1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q14. Explain the process of registering a custom dataset in Detectron2 for model training.\n",
        "\n",
        "In Detectron2, registering a custom dataset is the first step to train a model on your own images. Detectron2 expects data to be in a format it understands ‚Äî such as COCO JSON, or custom dictionaries with bounding boxes/masks.\n",
        "\n",
        "Step-by-Step Process\n",
        "\n",
        "Step 1: Install Detectron2\n",
        "Step 2: Prepare Your Dataset\n",
        "Step 3: Register the Dataset\n",
        "Step 4: Visualize a Sample (Optional but Recommended)\n",
        "Step 5: Update Config to Use Your Dataset\n",
        "\n",
        "Custom Dataset Registration Flow\n",
        "\n",
        "| Step     | Description                                        |\n",
        "| ---------| -------------------------------------------------- |\n",
        "| Dataset  | Organize into image folders + annotation JSON      |\n",
        "| Register | Use `register_coco_instances()` or custom function |\n",
        "| Inspect  | Visualize sample image with annotations            |\n",
        "| Config   | Set dataset names in config file before training   |"
      ],
      "metadata": {
        "id": "hfkRCaojOTwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Q15. What Challenges Arise in Scene Understanding for Image Segmentation, and How Can Mask R-CNN Address Them?\n",
        "\n",
        "What Is Scene Understanding in Image Segmentation?\n",
        "Scene understanding means identifying what objects are in an image, where they are, and what role they play in context ‚Äî often involving:\n",
        "* Precise localization\n",
        "* Separation of overlapping objects\n",
        "* Understanding small or complex shapes\n",
        "* Maintaining spatial consistency\n",
        "\n",
        "Key Challenges in Scene Understanding\n",
        "| Challenge                    | Description                                                         | Why It‚Äôs Hard                                               |\n",
        "| -----------------------------| ------------------------------------------------------------------- | ----------------------------------------------------------- |\n",
        "| Instance Overlap             | Multiple objects of the same class (e.g., people standing together) | Bounding boxes often overlap ‚Äî hard to tell them apart      |\n",
        "| Complex or Irregular Shapes  | Objects like clothes, animals, trees                                | Bounding boxes don‚Äôt capture fine edges or holes            |\n",
        "| Small Object Detection       | E.g., bottles, remote controls                                      | Easy to miss or misclassify, especially in cluttered scenes |\n",
        "| Pixel-Level Precision        | Needed in medical, autonomous driving, etc.                         | Bounding boxes are too coarse                               |\n",
        "| Occlusion & Background Noise | Objects partially hidden or visually similar to background          | Confuses detectors without deep context                     |\n",
        "| Scale Variability            | Objects appear at vastly different sizes                            | Needs multi-scale feature learning                          |\n",
        "\n",
        "How Mask R-CNN Addresses These Challenges\n",
        "1. Instance Segmentation (Not Just Detection)\n",
        "* Unlike detection models (YOLO, Faster R-CNN), Mask R-CNN outputs a binary mask for each object.\n",
        "* This enables it to distinguish individual instances, even if:\n",
        "  * They overlap\n",
        "  * Belong to the same class\n",
        "  * Have complex shapes\n",
        "\n",
        "2. RoIAlign for Precision\n",
        "* Replaces RoIPool with RoIAlign, which avoids pixel misalignment.\n",
        "* Results in high-quality, spatially accurate masks ‚Äî crucial for small or tightly packed objects.\n",
        "\n",
        "3. Feature Pyramid Networks (FPN)\n",
        "* Enhances the backbone (e.g., ResNet-50) with multi-scale features.\n",
        "* Helps detect objects of varying sizes and preserves contextual detail.\n",
        "\n",
        "4.Multiple Output Heads\n",
        "* Predicts:\n",
        "  * Bounding box\n",
        "  * Class label\n",
        "  * Segmentation mask\n",
        "* Combines local object info with global scene understanding.\n",
        "\n",
        "5.Transfer Learning Friendly\n",
        "* Pretrained on large datasets (like COCO), Mask R-CNN learns common visual patterns.\n",
        "* Speeds up training for domain-specific scenes (medical, aerial, etc.)\n"
      ],
      "metadata": {
        "id": "BkXquOovO1v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q16. How is the \"IoU(intersection over union)\" metric used in evaluating segmentation models?\n",
        "\n",
        "What is IoU?\n",
        "IoU (Intersection over Union) is a standard evaluation metric used in object detection and image segmentation to measure how well a predicted region (bounding box or mask) overlaps with the ground truth (actual region).\n",
        "\n",
        "IoU Formula:\n",
        "$$\n",
        "\\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
        "$$\n",
        "\n",
        "For Image Segmentation:\n",
        "In segmentation tasks (like with Mask R-CNN), IoU compares the predicted mask (binary mask of pixels marked as \"object\") with the ground truth mask:\n",
        "|       | Predicted Mask  | Ground Truth Mask |\n",
        "| ----- | --------------- | ----------------- |\n",
        "| Shape | Pixel-wise      | Pixel-wise        |\n",
        "| Type  | Binary (0 or 1) | Binary (0 or 1)   |\n",
        "\n",
        "Eg\n",
        "* Total overlap area = 80 pixels\n",
        "* Total union area = 120 pixels\n",
        "* IoU = 80 / 120 = 0.67\n",
        "\n",
        "#Why IoU Matters\n",
        "| IoU Value | Interpretation                           |\n",
        "| --------- | ---------------------------------------- |\n",
        "| 1.0       | Perfect overlap (ideal prediction)       |\n",
        "| > 0.5     | Generally acceptable in object detection |\n",
        "| \\~0.0     | No overlap at all                        |\n",
        "\n",
        "#Performance Metrics Using IoU\n",
        "| Metric           | Description                                      |\n",
        "| -----------------| ------------------------------------------------ |\n",
        "| IoU per class    | IoU for each object class                        |\n",
        "| mean IoU (mIoU)  | Average of IoUs across all classes               |\n",
        "| mAP\\@IoU=0.5     | Mean Average Precision with IoU threshold of 0.5 |\n",
        "| IoU thresholding | Set cutoff (e.g., 0.5) to decide success/failure |\n"
      ],
      "metadata": {
        "id": "SNCIWBIPPhp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q17. Discuss the Use of Transfer Learning in Mask R-CNN for Improving Segmentation on Custom Datasets\n",
        "\n",
        "What is Transfer Learning?\n",
        "Transfer learning is a machine learning technique where a model pretrained on a large dataset (like COCO or ImageNet) is fine-tuned on a smaller, task-specific dataset.\n",
        "\n",
        "In the Context of Mask R-CNN:\n",
        "Mask R-CNN models pretrained on COCO or LVIS datasets learn:\n",
        "* Strong feature extractors via the backbone (e.g., ResNet)\n",
        "* General object shapes, patterns, and contexts\n",
        "* Segmentation head behaviors\n",
        "\n",
        "Why Use Transfer Learning for Mask R-CNN?\n",
        "| Problem When Training from Scratch  | Solved by Transfer Learning               |\n",
        "| ----------------------------------- | ------------------------------------------|\n",
        "| Need huge dataset                   |  Pretrained knowledge fills gaps          |\n",
        "| Long training time                  |  Faster convergence                       |\n",
        "| Overfitting on small data           |  Uses robust feature extractor            |\n",
        "| Poor performance on complex tasks   |  Already learned general object structure |\n",
        "\n",
        "How Transfer Learning Works in Mask R-CNN\n",
        "Step-by-Step:\n",
        "1. Start with Pretrained Model\n",
        "   * COCO-trained model (`mask_rcnn_R_50_FPN_3x.yaml`)\n",
        "   * Includes pretrained weights for:\n",
        "     * Backbone (e.g., ResNet50)\n",
        "     * RPN\n",
        "     * Classifier\n",
        "     * Segmentation mask head\n",
        "\n",
        "2. Replace Class Head\n",
        "   * Replace final classification and mask prediction layers with your own number of classes\n",
        "\n",
        "3. Fine-Tune\n",
        "   * Train the model on your custom dataset\n",
        "   * Usually **freeze backbone** for initial epochs (optional)\n",
        "   * Lower learning rate for pretrained layers, higher for new heads\n",
        "\n",
        "Benefits of Using Transfer Learning in Mask R-CNN\n",
        "\n",
        "| Benefit              | Description                                       |\n",
        "| -------------------- | ------------------------------------------------- |\n",
        "|  Faster Training     | Already-learned visual features speed up learning |\n",
        "|  Better Accuracy     | Better generalization on small or noisy datasets  |\n",
        "|  Less Data Required  | Can work well even with a few hundred images      |\n",
        "|  Domain Adaptability | Easily adapt to medical, industrial, aerial, etc. |\n",
        "\n",
        "When Is Transfer Learning Most Useful?\n",
        "| Scenario                                  | Transfer Learning Benefit                         |\n",
        "| ----------------------------------------- | ------------------------------------------------- |\n",
        "| Custom segmentation (e.g., tools, plants) | Leverages generic object structure knowledge      |\n",
        "| Medical imaging (X-ray, MRI)              | Adapts to specialized data without large dataset  |\n",
        "| Industrial defects, satellite images      | Recognizes unfamiliar shapes via learned patterns |\n"
      ],
      "metadata": {
        "id": "ap8qzctbVojj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q18. What Is the Purpose of Evaluation Curves, Such as Precision-Recall Curves in Segmentation Model Assessment?\n",
        "\n",
        "What Are Evaluation Curves?\n",
        "Evaluation curves are graphical tools used to analyze the performance of segmentation models by visualizing trade-offs between different metrics (e.g., precision vs. recall). These help developers:\n",
        "* Understand model behavior beyond a single metric\n",
        "* Tune thresholds for optimal performance\n",
        "* Identify overfitting or poor generalization\n",
        "Key Curve: Precision-Recall (PR) Curve\n",
        "\n",
        "Why PR Curves Matter in Segmentation\n",
        "\n",
        "In segmentation (especially instance or semantic), we want accurate pixel-level predictions. PR curves tell us:\n",
        "| Question                                   | Answered by PR Curve   |\n",
        "| ------------------------------------------ | -----------------------|\n",
        "| Is the model over-predicting masks?        |  Precision drops       |\n",
        "| Is the model missing objects?              |  Recall drops          |\n",
        "| What's the best threshold to filter masks? |  Optimal balance point |\n",
        "\n",
        "How It's Generated:\n",
        "1. Set a range of IoU thresholds (e.g., 0.5 to 0.95).\n",
        "2. For each threshold, compute Precision and Recall.\n",
        "3. Plot them to visualize how changes in threshold affect performance.\n",
        "\n",
        "What to Look For in PR Curves:\n",
        "| Curve Shape         | Meaning                                               |\n",
        "| ------------------- | ----------------------------------------------------- |\n",
        "|  Sharp high curve   | High precision & recall ‚Äî excellent segmentation      |\n",
        "|  Falling curve      | High recall but low precision ‚Äî many false positives  |\n",
        "|  Flat low curve     | Poor model ‚Äî misses many or predicts irrelevant areas |\n"
      ],
      "metadata": {
        "id": "EYMiLgMjVu4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q19. How Do Mask R-CNN Models Handle Occlusions or Overlapping Objects in Segmentation?\n",
        "\n",
        "The Challenge: Occlusion & Overlapping Objects\n",
        "In real-world images, objects often overlap or partially occlude one another:\n",
        "* People standing close together\n",
        "* Vehicles in traffic\n",
        "* Fruits in a basket\n",
        "\n",
        "This poses difficulty for traditional segmentation models, which may:\n",
        "* Merge multiple objects into one mask\n",
        "* Fail to detect partially visible objects\n",
        "* Assign incorrect classes\n",
        "\n",
        "#Mask R-CNN: Designed for Instance Segmentation\n",
        "Unlike semantic segmentation (which assigns a label per pixel regardless of object instance), Mask R-CNN separates and segments each object instance individually, even if overlapping.\n",
        "\n",
        "How Mask R-CNN Handles Occlusion & Overlap\n",
        "| Component                     | Role in Handling Overlaps                                                      |\n",
        "| ----------------------------- | ------------------------------------------------------------------------------ |\n",
        "| Region Proposal Network (RPN) | Proposes multiple overlapping object regions, including partially visible ones |\n",
        "| RoIAlign                      | Aligns proposed regions precisely for accurate masks                           |\n",
        "| Classification + Mask Heads   | Predicts label and pixel-wise mask per instance, not per class                 |\n",
        "| Non-Max Suppression (NMS)     | Filters overlapping boxes based on confidence and IoU thresholds               |\n",
        "| Softmax (multi-class)         | Allows detection of **each** object individually even if class is repeated     |\n",
        "\n",
        "---\n",
        "\n",
        "### üñºÔ∏è Example: People in a Crowd\n",
        "\n",
        "* 5 people standing close together\n",
        "* All proposals are generated (even for partially seen people)\n",
        "* Each is classified as \"person\"\n",
        "* 5 separate masks are generated with **individual boundaries**\n",
        "\n",
        "This is how instance segmentation differs from semantic segmentation.\n",
        "\n",
        "Visualization Result\n",
        "In output:\n",
        "* Each object has:\n",
        "  * A unique bounding box\n",
        "  * A separate colored mask\n",
        "  * An associated class label and confidence score\n",
        "\n",
        "Even if two people are touching or partially hidden, the model:\n",
        "Predicts distinct masks"
      ],
      "metadata": {
        "id": "O0LQeqSWW4Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q20. Explain the Impact of Batch Size and Learning Rate on Mask R-CNN Model Training\n",
        "\n",
        "Training a mask R-CNN model involves careful tuning of hyperparameters, especially batch size and learning rate, as they directly impact:\n",
        "* Convergence speed\n",
        "* Stability of training\n",
        "* Model accuracy\n",
        "* GPU memory consumption\n",
        "\n",
        "---\n",
        "1.Batch Size\n",
        "What is Batch Size?\n",
        "The number of images processed together before updating model weights.\n",
        "#Effects of Batch Size:\n",
        "| Batch Size    | Impact                                                                                      |\n",
        "| ------------- | ------------------------------------------------------------------------------------------- |\n",
        "| Small (1‚Äì4)   | - Lower memory usage <br> - Noisy gradients (unstable updates) <br> - Can generalize better |\n",
        "| Medium (8‚Äì16) | - Balanced trade-off between noise and stability                                            |\n",
        "| Large (32+)   | - Smooth gradients <br> - Requires more memory <br> - Risk of overfitting                   |\n",
        "\n",
        "#In Mask R-CNN:\n",
        "* Due to high-resolution images + multiple heads (bbox + class + mask), large batch sizes may exceed memory limits.\n",
        "* On most GPUs (e.g., 8‚Äì16 GB), batch size 2‚Äì4 is common.\n",
        "\n",
        "2. Learning Rate\n",
        "What is Learning Rate?\n",
        "A scalar that controls how much the model updates weights during backpropagation.\n",
        "\n",
        "Effects of Learning Rate:\n",
        "| Learning Rate         | Impact                                                                        |\n",
        "| --------------------- | ----------------------------------------------------------------------------- |\n",
        "| **Too High** (>0.01)  | - Training may diverge  <br> - Loss oscillates <br> - Skips optimal weights   |\n",
        "| **Too Low** (<0.0001) | - Slow convergence  <br> - Gets stuck in local minima <br> - Underfits        |\n",
        "| **Optimal** (\\~0.001) | - Smooth convergence  <br> - Learns effectively without overshooting          |\n",
        "\n",
        "In Mask R-CNN:\n",
        "* Suggested initial learning rates (using Detectron2):\n",
        "  * 0.00025 to 0.001\n",
        "  * Lower if fine-tuning (to preserve pretrained knowledge)\n",
        "* Use learning rate decay (e.g., step or cosine schedule) for stability\n",
        "\n",
        "#Interaction Between Batch Size and Learning Rate\n",
        "A larger batch size often allows for a higher learning rate, because gradients are more stable.\n",
        "Linear scaling rule (approximate):\n",
        "\n",
        "$$\n",
        "\\text{LR}_{\\text{new}} = \\text{LR}_{\\text{base}} \\times \\left(\\frac{\\text{batch size}_{\\text{new}}}{\\text{batch size}_{\\text{base}}}\\right)\n",
        "$$\n",
        "\n",
        "Example:\n",
        "* Trained with batch size 2 and LR = 0.00025\n",
        "* If increasing to batch size 4 ‚Üí try LR = 0.0005"
      ],
      "metadata": {
        "id": "KLQyOFiVXcUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q21. Challenges of Training Segmentation Models on Custom Datasets (Especially in Detectron2)\n",
        "\n",
        "Training segmentation models like Mask R-CNN on a custom dataset (using frameworks like Detectron2) is highly powerful‚Äîbut comes with a unique set of challenges. Below is a structured breakdown of these challenges and how they specifically affect the Detectron2 workflow.\n",
        "\n",
        "1. Data Annotation Issues\n",
        "#Challenges:\n",
        "* Manual annotation for masks (polygons) is time-consuming and error-prone.\n",
        "* Inconsistent labels, missing masks, or overlapping annotations can break training.\n",
        "* Incorrect COCO format structure or category ID mismatch can cause crashes.\n",
        "#Solutions:\n",
        "* Use tools like CVAT, Labelme, or makesense.ai for annotation.\n",
        "* Validate COCO JSON format using [coco-analyzer](https://github.com/philferriere/coco-analyzer).\n",
        "\n",
        "2. Dataset Format and Registration\n",
        "#Challenges:\n",
        "* Detectron2 expects COCO-style datasets (JSON + images) or to be registered manually.\n",
        "* Dataset registration (with `DatasetCatalog`) must be carefully done.\n",
        "* Categories must match both in number and naming in the metadata and annotations.\n",
        "#### Solutions:\n",
        "* Register dataset properly using:\n",
        "  ```python\n",
        "  from detectron2.data.datasets import register_coco_instances\n",
        "  register_coco_instances(\"my_dataset\", {}, \"path/to/annotations.json\", \"path/to/images\")\n",
        "  ```\n",
        "* Validate the number of classes:\n",
        "  ```python\n",
        "  cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "  ```\n",
        "  3. Insufficient or Imbalanced Data\n",
        "#Challenges:\n",
        "* Small datasets ‚Üí model overfits and underperforms on generalization.\n",
        "* Some classes may appear far more often than others, leading to bias.\n",
        "#Solutions:\n",
        "* Data augmentation (flipping, scaling, color jittering, cropping).\n",
        "* Use class weighting or focal loss (custom implementation).\n",
        "* Use transfer learning with COCO-pretrained weights.\n",
        "\n",
        "4. Configuration Complexity\n",
        "#Challenges:\n",
        "* Detectron2 uses YAML-based configs that must match:\n",
        "  * Number of classes\n",
        "  * Dataset names\n",
        "  * Input formats\n",
        "* A misconfigured setting may silently fail or train incorrectly.\n",
        "#Solutions:\n",
        "* Use a base config (`mask_rcnn_R_50_FPN_3x.yaml`) and override carefully.\n",
        "* Check these keys:\n",
        "\n",
        "  ```python\n",
        "  cfg.DATASETS.TRAIN\n",
        "  cfg.MODEL.WEIGHTS\n",
        "  cfg.INPUT.MASK_FORMAT\n",
        "  cfg.SOLVER.*\n",
        "  ```\n",
        "  5. Hyperparameter Sensitivity\n",
        "#Challenges:\n",
        "* Learning rate, batch size, and training steps need careful tuning.\n",
        "* Default values are often suited for COCO-sized datasets, not custom ones.\n",
        "#Solutions:\n",
        "* Reduce `BASE_LR` (e.g., 0.00025).\n",
        "* Use early stopping or checkpoint monitoring.\n",
        "* Reduce `IMS_PER_BATCH` to avoid GPU memory errors.\n",
        "\n",
        "6. Evaluation and Debugging\n",
        "#challenges:\n",
        "* mAP (mean Average Precision) or IoU scores may be misleading on small test sets.\n",
        "* No obvious error, but bad predictions if mask quality is poor.\n",
        "#Solutions:\n",
        "* Visualize predictions on validation images using `Visualizer`.\n",
        "* Track metrics like precision, recall, IoU, confusion matrix per class.\n",
        "\n",
        "7. Visualizing Segmentation Masks\n",
        "#challenges:\n",
        "* Visual output is critical, but masks can overlap or be incorrectly colored.\n",
        "* Needs `Visualizer` + metadata + proper class IDs.\n",
        "#### Solutions:\n",
        "```python\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "v = Visualizer(image[..., ::-1], metadata=my_metadata)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "``"
      ],
      "metadata": {
        "id": "HSvHWg9UYE28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q22.How Does Mask R-CNN's Segmentation Head Output Differ from a Traditional Object Detector‚Äôs Output?\n",
        "\n",
        "Traditional Object Detector Output (e.g., Faster R-CNN, YOLO)\n",
        "Traditional object detectors focus on:\n",
        "1. Bounding Boxes ‚Äì Rectangular boxes enclosing objects.\n",
        "2. Class Labels ‚Äì What object is inside the box.\n",
        "3. Confidence Scores ‚Äì How sure the model is.\n",
        "\n",
        "Mask R-CNN Output ‚Äì Adds a Segmentation Head\n",
        "Mask R-CNN extends Faster R-CNN by adding a third branch for pixel-wise segmentation (masks):\n",
        "1. Bounding Boxes\n",
        "2. Class Labels\n",
        "3. Confidence Scores\n",
        "4. Segmentation Mask (Pixel-level) ‚Äì A binary mask (e.g., 28√ó28) for each detected object.\n",
        "\n",
        "#Comparison Table\n",
        "| Feature                        | Traditional Detector     | Mask R-CNN                       |\n",
        "| ------------------------------ | -------------------------| ---------------------------------|\n",
        "| Bounding Box                   |  Yes                     |  Yes                             |\n",
        "| Class Prediction               |  Yes                     |  Yes                             |\n",
        "| Confidence Score               |  Yes                     |  Yes                             |\n",
        "| **Instance Segmentation Mask** |  No                      |  Yes (per object)                |\n",
        "| Handles Overlap                |  Bounding boxes overlap  |  Instance masks separate objects |\n",
        "| Spatial Detail                 |  Coarse                  |  Fine-grained pixel-level masks  |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "J99H_ovBZCJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "RNg5XWQZaHU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q1: Perform Basic Color-Based Segmentation to Separate the Blue Color in an Image\n",
        "\n",
        "simple implementation using OpenCV in Python to segment out blue regions from an image.\n",
        "\n",
        "#Step-by-Step Code\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread(\"path/to/your/image.jpg\")  # Replace with your image path\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Convert image to HSV color space\n",
        "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "# Define range for blue color in HSV\n",
        "lower_blue = np.array([100, 150, 0])  # lower bound of blue hue\n",
        "upper_blue = np.array([140, 255, 255])  # upper bound of blue hue\n",
        "\n",
        "# Create mask to extract blue areas\n",
        "blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "# Bitwise-AND mask and original image to segment blue regions\n",
        "blue_segment = cv2.bitwise_and(image_rgb, image_rgb, mask=blue_mask)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Blue Mask\")\n",
        "plt.imshow(blue_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Blue Segment\")\n",
        "plt.imshow(blue_segment)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Output\n",
        "* Original Image: The input photo\n",
        "* Blue Mask: Binary mask where blue areas are white\n",
        "* Blue Segment: Only the blue parts retained, rest turned black\n"
      ],
      "metadata": {
        "id": "9WBRJnqFaJhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2: Use Edge Detection with Canny to Highlight Object Edges in an Image\n",
        "Here's a Python example using OpenCV to load an image and apply Canny edge detection to highlight the object boundaries.\n",
        "\n",
        "Canny Edge Detection: Step-by-Step Code\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the image\n",
        "image = cv2.imread(\"path/to/your/image.jpg\")  # Replace with your image path\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Step 2: Apply Gaussian Blur (recommended before Canny)\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 1.4)\n",
        "\n",
        "# Step 3: Apply Canny edge detection\n",
        "edges = cv2.Canny(blurred, threshold1=50, threshold2=150)\n",
        "\n",
        "# Step 4: Plot the result\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Grayscale\")\n",
        "plt.imshow(gray, cmap='gray')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Canny Edges\")\n",
        "plt.imshow(edges, cmap='gray')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "How It Works\n",
        "\n",
        "| Step                  | Description                                               |\n",
        "| --------------------- | --------------------------------------------------------- |\n",
        "|  Convert to grayscale | Reduces color noise for better edge detection             |\n",
        "|  Gaussian blur        | Smoothens the image to reduce noise before edge detection |\n",
        "|  Canny detector       | Detects edges using gradient intensity and direction      |"
      ],
      "metadata": {
        "id": "tiNsd_M7asDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3: Load a Pretrained Mask R-CNN Model from PyTorch and Use It for Object Detection and Segmentation\n",
        "PyTorch's `torchvision` library provides a pretrained Mask R-CNN model that can perform both bounding box detection and pixel-wise segmentation.\n",
        "\n",
        "Step-by-Step Code\n",
        "```python\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# 1. Load an image\n",
        "image_path = \"path/to/your/image.jpg\"  # Replace with your image\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# 2. Transform the image to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# 3. Load pretrained Mask R-CNN model\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# 4. Perform inference\n",
        "with torch.no_grad():\n",
        "    prediction = model(image_tensor)[0]\n",
        "\n",
        "# 5. Visualize the output\n",
        "image_np = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for i in range(len(prediction[\"boxes\"])):\n",
        "    score = prediction[\"scores\"][i].item()\n",
        "    if score > 0.5:\n",
        "        box = prediction[\"boxes\"][i].int().numpy()\n",
        "        mask = prediction[\"masks\"][i, 0].mul(255).byte().cpu().numpy()\n",
        "        label = prediction[\"labels\"][i].item()\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(image_np, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "\n",
        "        # Apply mask\n",
        "        colored_mask = cv2.merge([mask, mask, mask])\n",
        "        image_np = cv2.addWeighted(image_np, 1, colored_mask, 0.5, 0)\n",
        "\n",
        "# 6. Show the result\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image_np)\n",
        "plt.title(\"Mask R-CNN Detection & Segmentation\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "```\n",
        "Output\n",
        "\n",
        "* Bounding boxes around detected objects.\n",
        "* Segmented masks overlayed on each object.\n",
        "* Filters out low-confidence predictions (score < 0.5).\n"
      ],
      "metadata": {
        "id": "VmFpN-ybbVEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q4: Generate Bounding Boxes for Each Object Detected by Mask R-CNN in an Image (PyTorch)\n",
        "\n",
        "You can easily extract and visualize bounding boxes from a pretrained Mask R-CNN model using PyTorch. Below is a practical script to do that.\n",
        "\n",
        "Full Code to Display Bounding Boxes\n",
        "```python\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# Step 1: Load and preprocess image\n",
        "image_path = \"path/to/your/image.jpg\"  #  Replace with your image path\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Step 2: Load pretrained Mask R-CNN model\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Step 3: Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(image_tensor)[0]\n",
        "\n",
        "# Step 4: Draw bounding boxes\n",
        "image_cv = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for idx in range(len(outputs[\"boxes\"])):\n",
        "    score = outputs[\"scores\"][idx].item()\n",
        "    if score > 0.5:  # Confidence threshold\n",
        "        box = outputs[\"boxes\"][idx].cpu().numpy().astype(int)\n",
        "        label = outputs[\"labels\"][idx].item()\n",
        "\n",
        "        # Draw the bounding box\n",
        "        cv2.rectangle(image_cv, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "\n",
        "        # Optionally, put class label & score (requires COCO label map)\n",
        "        cv2.putText(image_cv, f\"Class {label}, {score:.2f}\",\n",
        "                    (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.5, (255, 0, 0), 1)\n",
        "\n",
        "# Step 5: Display the result\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(image_cv)\n",
        "plt.title(\"Bounding Boxes from Mask R-CNN\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "```\n"
      ],
      "metadata": {
        "id": "qLFd70CMGpJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q5: Convert an Image to Grayscale and Apply Otsu's Thresholding for Segmentation\n",
        "\n",
        "Otsu's method automatically determines the optimal threshold to separate foreground from background, commonly used in **binarization and segmentation tasks**.\n",
        "\n",
        "Step-by-Step Python Code Using OpenCV\n",
        "```python\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the image\n",
        "image_path = \"path/to/your/image.jpg\"  #  Replace with your image path\n",
        "image = cv2.imread(image_path)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Step 2: Apply Gaussian Blur to reduce noise (recommended)\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# Step 3: Apply Otsu's thresholding\n",
        "# cv2.THRESH_BINARY + cv2.THRESH_OTSU tells OpenCV to compute the best threshold\n",
        "_, otsu_mask = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "# Step 4: Display the result\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Grayscale\")\n",
        "plt.imshow(gray, cmap='gray')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Otsu's Threshold\")\n",
        "plt.imshow(otsu_mask, cmap='gray')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        " What It Does\n",
        "* Converts the image to grayscale.\n",
        "* Applies Gaussian Blur for noise reduction.\n",
        "* Uses Otsu's method to find the optimal threshold value automatically.\n",
        "* Outputs a binary segmented mask (white = object, black = background).\n",
        "\n",
        "Use Cases\n",
        "* Background removal\n",
        "* Simple object segmentation\n",
        "* Preprocessing for contour detection or OCR\n"
      ],
      "metadata": {
        "id": "NrzJOEAmHc2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q6: Perform Contour Detection in an Image to Detect Distinct Objects or Shapes\n",
        "Contours help in detecting boundaries of objects in binary or thresholded images, commonly used in shape analysis and object detection.\n",
        "\n",
        "Step-by-Step Python Code Using OpenCV\n",
        "```python\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the image\n",
        "image_path = \"path/to/your/image.jpg\"  #  Replace with your image path\n",
        "image = cv2.imread(image_path)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Step 2: Threshold the image (you can also use Otsu's here)\n",
        "_, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Step 3: Find contours\n",
        "contours, hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Step 4: Draw contours on a copy of the original image\n",
        "contour_image = image.copy()\n",
        "cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)  # Green color\n",
        "\n",
        "# Step 5: Display results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Binary Image\")\n",
        "plt.imshow(binary, cmap='gray')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Contours Detected\")\n",
        "plt.imshow(cv2.cvtColor(contour_image, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "What This Does\n",
        "* Thresholds the grayscale image to get a binary image.\n",
        "* Uses `cv2.findContours()` to detect boundaries of distinct shapes.\n",
        "* Draws contours on the original image using `cv2.drawContours()`.\n"
      ],
      "metadata": {
        "id": "D7iTgCmgH0ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q7: Apply Mask R-CNN to Detect Objects and Their Segmentation Masks in a Custom Image and Display Them\n",
        "\n",
        "A pretrained Mask R-CNN model from `torchvision` (with a ResNet-50 backbone), apply it to a custom image, and overlay the segmentation masks and bounding boxes.\n",
        "\n",
        "Step-by-Step Code Using PyTorch & OpenCV\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load and preprocess the image\n",
        "image_path = \"path/to/your/image.jpg\"  #  Replace with your custom image\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "input_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "# Step 2: Load pretrained Mask R-CNN model\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Step 3: Perform inference\n",
        "with torch.no_grad():\n",
        "    predictions = model(input_tensor)[0]\n",
        "\n",
        "# Step 4: Load original image using OpenCV for drawing\n",
        "image_cv = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Step 5: Apply masks, bounding boxes, and labels\n",
        "for i in range(len(predictions[\"scores\"])):\n",
        "    score = predictions[\"scores\"][i].item()\n",
        "    if score > 0.5:\n",
        "        box = predictions[\"boxes\"][i].int().numpy()\n",
        "        mask = predictions[\"masks\"][i, 0].mul(255).byte().cpu().numpy()\n",
        "        label = predictions[\"labels\"][i].item()\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(image_cv, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "\n",
        "        # Random color for mask\n",
        "        color = np.random.randint(0, 255, (3,), dtype=np.uint8)\n",
        "        mask_color = np.stack([mask]*3, axis=-1)\n",
        "        mask_color = (mask_color > 0) * color\n",
        "\n",
        "        # Overlay mask on the image\n",
        "        image_cv = np.where(mask_color > 0, image_cv * 0.5 + mask_color * 0.5, image_cv).astype(np.uint8)\n",
        "\n",
        "# Step 6: Show the result\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image_cv)\n",
        "plt.title(\"Mask R-CNN: Detected Objects and Masks\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "```\n",
        "What we Get:\n",
        "* Bounding Boxes in green\n",
        "* Instance Segmentation Masks in random colors\n",
        "* High-confidence detections (score > 0.5)"
      ],
      "metadata": {
        "id": "t36RFO3QIQLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q8: Apply K-Means Clustering for Segmenting Regions in an Image\n",
        "\n",
        "K-Means clustering groups pixels based on their color similarity, often used for unsupervised image segmentation.\n",
        "\n",
        "Python Code Using OpenCV and scikit-learn\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Load the image\n",
        "image_path = \"path/to/your/image.jpg\"  #  Replace with your image path\n",
        "image = cv2.imread(image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Step 2: Reshape the image to (num_pixels, 3)\n",
        "pixel_values = image_rgb.reshape((-1, 3))\n",
        "pixel_values = np.float32(pixel_values)\n",
        "\n",
        "# Step 3: Apply K-Means clustering\n",
        "k = 4  # Number of clusters (you can change this)\n",
        "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "kmeans.fit(pixel_values)\n",
        "labels = kmeans.labels_\n",
        "centers = np.uint8(kmeans.cluster_centers_)\n",
        "\n",
        "# Step 4: Recreate segmented image\n",
        "segmented_img = centers[labels.flatten()]\n",
        "segmented_img = segmented_img.reshape(image_rgb.shape)\n",
        "\n",
        "# Step 5: Visualize the result\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(f\"K-Means Segmented (k={k})\")\n",
        "plt.imshow(segmented_img)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "Explanation\n",
        "\n",
        "* Input: RGB image reshaped to 2D array (each row = 1 pixel)\n",
        "* KMeans: Clusters similar colors into groups\n",
        "* Output: Segmented image with `k` distinct color regions"
      ],
      "metadata": {
        "id": "eYVfqwg4IeK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}