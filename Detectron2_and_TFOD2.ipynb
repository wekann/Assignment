{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlKuk7BJeA39PTaf/iTCoE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wekann/Assignment/blob/main/Detectron2_and_TFOD2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory"
      ],
      "metadata": {
        "id": "NPxPRtl0YLK0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEYe6Fw9XcB8"
      },
      "outputs": [],
      "source": [
        "'''Q1. What Types of Tasks Does Detectron2 Support?\n",
        "\n",
        "Detectron2, developed by Facebook AI Research (FAIR), is a modular and high-performance object detection library built on PyTorch. It supports a wide range of computer vision tasks, primarily focused on visual recognition.\n",
        "Key Tasks Supported by Detectron2:\n",
        "1. Object Detection\n",
        "* Detects multiple objects in an image.\n",
        "* Outputs: bounding boxes, class labels, and confidence scores.\n",
        "* Example: Detecting cars, people, animals, etc.\n",
        "\n",
        "2. Instance Segmentation\n",
        "* Detects each object and provides a pixel-wise mask.\n",
        "* Example: Separating overlapping people in an image.\n",
        "\n",
        "3. Semantic Segmentation\n",
        "* Assigns a class label to each pixel, treating all instances of a class as one.\n",
        "* Example: Coloring the road, sky, buildings differently.\n",
        "\n",
        "4. Panoptic Segmentation\n",
        "* Combines semantic + instance segmentation.\n",
        "* Example: Separates each person + labels background classes like sky or road.\n",
        "\n",
        "5. Keypoint Detection (Pose Estimation)\n",
        "* Detects human body keypoints (joints like elbows, knees, etc.).\n",
        "* Example: Pose estimation in sports analytics or AR.\n",
        "\n",
        "6. DensePose\n",
        "* Maps all human pixels to a 3D surface of the human body.\n",
        "* Example: Augmented reality or motion capture.\n",
        "\n",
        "Optional Tasks via Extensions:\n",
        "* Zero-shot object detection\n",
        "* Depth prediction (with additional plugins)\n",
        "* Object tracking (via integration)\n",
        "\n",
        "Table:\n",
        "| Task Type             | Output                       | Use Case Example                   |\n",
        "| --------------------- | ---------------------------- | ---------------------------------- |\n",
        "| Object Detection      | Boxes + labels               | Face, vehicle, animal detection    |\n",
        "| Instance Segmentation | Boxes + masks                | Medical imaging, self-driving cars |\n",
        "| Semantic Segmentation | Class per pixel              | Land cover, road scenes            |\n",
        "| Panoptic Segmentation | Combined semantic + instance | Scene understanding                |\n",
        "| Keypoint Detection    | Human joint locations        | Pose estimation, sports tracking   |\n",
        "| DensePose             | 3D mapping of human surface  | Animation, AR/VR                   |"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2. Why Is Data Annotation Important When Training Object Detection Models?\n",
        "\n",
        "What Is Data Annotation in Object Detection?\n",
        "Data annotation involves labeling images with the correct information so that an object detection model can learn to identify and locate objects. This typically includes:\n",
        "\n",
        "* Drawing bounding boxes around objects\n",
        "* Assigning class labels to those boxes\n",
        "* (Sometimes) providing segmentation masks or keypoints\n",
        "\n",
        "Why It's Important\n",
        "1. Supervised Learning Requires Labeled Data\n",
        "Object detection models are trained using supervised learning, which means they need annotated (labeled) data to learn:\n",
        "* What objects look like\n",
        "* Where objects are located in the image\n",
        "\n",
        "2. Teaches the Model What to Detect\n",
        "Without annotations, the model has no idea:\n",
        "* What part of the image is the object\n",
        "* What class each object belongs to\n",
        "Annotations provide the ground truth the model compares itself to during training.\n",
        "\n",
        "3. Affects Model Accuracy\n",
        "Poor or inconsistent annotations lead to:\n",
        "* Lower accuracy\n",
        "* More false positives/negatives\n",
        "* Poor generalization\n",
        "Well-annotated datasets = better model performance\n",
        "\n",
        "4. Used to Calculate the Loss Function\n",
        "During training, object detection models compare their predictions to the annotated ground truth to calculate:\n",
        "* Classification loss (wrong label)\n",
        "* Localization loss (wrong box position)\n",
        "* Objectness/confidence loss\n",
        "No annotations ‚Üí no loss calculation ‚Üí no learning.\n",
        "\n",
        "5. Essential for Real-World Applications\n",
        "In domains like:\n",
        "\n",
        "* Autonomous driving\n",
        "* Surveillance\n",
        "* Medical imaging\n",
        "Incorrect or missing annotations can result in dangerous or costly failures.\n",
        "\n",
        "Table:\n",
        "| Reason                          | Importance                                    |\n",
        "| ------------------------------- | --------------------------------------------- |\n",
        "| Supervised learning requirement | Models need labels to learn                   |\n",
        "| Teaches what and where          | Guides the model to detect correct objects    |\n",
        "| Affects training quality        | Bad annotations = bad model                   |\n",
        "| Enables loss computation        | Used in backpropagation                       |\n",
        "| Critical in real-world settings | Mislabeling can cause major real-world issues |"
      ],
      "metadata": {
        "id": "qbrlCLS-YmUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3. What Does Batch Size Refer to in the Context of Model Training?\n",
        "\n",
        "Definition:\n",
        "Batch size is the number of training samples (images, data points) that are processed together in one forward and backward pass of the model during training.\n",
        "Example:\n",
        "If you have:\n",
        "* A dataset with 1,000 images\n",
        "* A batch size of 32\n",
        "\n",
        "The model processes 32 images at a time, and it takes:\n",
        "$$\n",
        "\\frac{1000}{32} \\approx 32 \\text{ iterations (batches) to complete one epoch}\n",
        "$$\n",
        "\n",
        "Why Is Batch Size Important?\n",
        "| üîπ Parameter         | üîç Meaning                                                                                          |\n",
        "| -------------------- | ---------------------------------------------------------------------------------------------------- |\n",
        "| Small batch size     | Uses fewer samples per step (e.g., 8, 16) ‚Üí slower training but more generalization                  |\n",
        "| Large batch size     | Uses more samples per step (e.g., 64, 128, 256) ‚Üí faster training but may overfit or use more memory |\n",
        "\n",
        "Impacts of Batch Size:\n",
        "| Aspect             | Small Batch Size      | Large Batch Size             |\n",
        "| ------------------ | --------------------- | ---------------------------- |\n",
        "| Memory usage       | Low                   | High                         |\n",
        "| Training stability | Noisy updates         | Smoother updates             |\n",
        "| Training speed     | Slower per epoch      | Faster per epoch             |\n",
        "| Model performance  | May generalize better | Can overfit or plateau early |\n",
        "| Learning dynamics  | More stochastic       | More deterministic           |\n",
        "\n",
        "Summary:\n",
        "\n",
        " Batch size controls how many data samples the model sees at once before updating the weights.\n",
        " It affects training speed, stability, and memory usage."
      ],
      "metadata": {
        "id": "gXLIjfdWY35s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q4. What Is the Purpose of Pretrained Weights in Object Detection Models?\n",
        "Definition:\n",
        "Pretrained weights are the saved parameters (like filters and biases) from a model that has already been trained on a large benchmark dataset, such as ImageNet or COCO.\n",
        "In object detection, they are used to initialize a model before training it on a new dataset.\n",
        "\n",
        "Purpose of Using Pretrained Weights:\n",
        "1.Faster Convergence (Training is Faster)\n",
        "\n",
        "* Instead of starting from random weights, you start from a model that already ‚Äúknows‚Äù basic features (edges, shapes, textures).\n",
        "* Result: The model learns the new task much quicker.\n",
        "\n",
        "2.Transfer Learning\n",
        "* Allows you to reuse knowledge learned on a large dataset and apply it to a smaller, domain-specific dataset.\n",
        "* Example: A model pretrained on COCO can be fine-tuned to detect medical tools or industrial parts.\n",
        "\n",
        "3.Better Accuracy with Less Data\n",
        "* Pretrained models often perform better than training from scratch, especially when you have limited labeled data.\n",
        "\n",
        "4. Avoid Overfitting\n",
        "* Since the model starts with generalized features, it‚Äôs less likely to overfit early on when training with small datasets.\n",
        "\n",
        "5. Modularity & Flexibility\n",
        "* You can freeze some layers and fine-tune others, depending on your task complexity and dataset size.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "# Load Faster R-CNN with pretrained weights on COCO\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "```\n",
        "\n",
        "This model:\n",
        "\n",
        "* Already knows how to detect 80 object categories from COCO.\n",
        "* Can be fine-tuned to learn custom classes like \"helmet\", \"mask\", etc.\n",
        "\n",
        "Table:\n",
        "| Benefit                     | Description                                   |\n",
        "| ----------------------------| --------------------------------------------- |\n",
        "|  Faster training            | Reuses learned features ‚Üí quick convergence   |\n",
        "|  Better accuracy            | Especially on small datasets                  |\n",
        "|  Generalization             | Learns high-quality representations           |\n",
        "|  Supports transfer learning | Use on custom datasets with fewer labels      |\n",
        "|  Reduces overfitting        | Model is not learning everything from scratch |"
      ],
      "metadata": {
        "id": "byLE53SaZPKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q5. How Can You Verify That Detectron2 Was Installed Correctly?\n",
        "After installing Detectron2, it‚Äôs important to make sure the installation works as expected.\n",
        "Here‚Äôs a step-by-step checklist to verify your setup:\n",
        "\n",
        "1. Import Detectron2 in Python\n",
        "```python\n",
        "import detectron2\n",
        "```\n",
        "If this runs without error, Detectron2 is installed.\n",
        "\n",
        "2. Check Version and Environment\n",
        "```python\n",
        "import detectron2\n",
        "print(\"Detectron2 version:\", detectron2.__version__)\n",
        "```\n",
        "You can also verify CUDA compatibility:\n",
        "```bash\n",
        "python -m detectron2.utils.collect_env\n",
        "```\n",
        "This prints details like:\n",
        "* PyTorch version\n",
        "* CUDA version\n",
        "* GPU availability\n",
        "* Compiler and OS info\n",
        "\n",
        "3. Run a Basic Inference Script\n",
        "```python\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "import cv2\n",
        "\n",
        "# Load a sample config and model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # confidence threshold\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Load an image and run prediction\n",
        "image = cv2.imread(\"your_image.jpg\")\n",
        "outputs = predictor(image)\n",
        "\n",
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)\n",
        "```\n",
        "If this runs and returns predictions (even just boxes and labels), your Detectron2 is working.\n",
        "\n",
        "4. Visualize Predictions (Optional)\n",
        "```python\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2.imshow(\"Result\", out.get_image()[:, :, ::-1])\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "```\n",
        "If You See Errors:\n",
        "* Reinstall Detectron2 with version matching your **CUDA** and **PyTorch** version.\n",
        "* Use official installation guide:\n",
        "   [https://detectron2.readthedocs.io/](https://detectron2.readthedocs.io/)\n",
        "\n",
        "Summary\n",
        "\n",
        "| Step                    | Command/Output                           |\n",
        "| ----------------------- | ---------------------------------------- |\n",
        "| Import test             | `import detectron2`                      |\n",
        "| Version check           | `print(detectron2.__version__)`          |\n",
        "| Environment check       | `python -m detectron2.utils.collect_env` |\n",
        "| Run simple prediction   | Use `DefaultPredictor` from model zoo    |\n",
        "| Visual check (optional) | Display results using `Visualizer`       |"
      ],
      "metadata": {
        "id": "Cspp3rv1Zrl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q6. What is TFOD2, and Why Is It Widely Used?\n",
        "\n",
        "What is TFOD2?\n",
        "TFOD2 stands for TensorFlow Object Detection API v2 ‚Äî a powerful, open-source framework developed by Google for:\n",
        "\n",
        "Training, evaluating, and deploying object detection models using TensorFlow 2.x.\n",
        "It Includes:\n",
        "* Pre-trained detection models (like SSD, Faster R-CNN, EfficientDet, CenterNet)\n",
        "* Tools for data preprocessing, training, evaluation, and exporting models\n",
        "* Support for custom datasets using TFRecord format\n",
        "* Integration with TensorFlow Lite, TFJS, and Coral EdgeTPU for deployment\n",
        "\n",
        "Why TFOD2 Is Widely Used:\n",
        "1. End-to-End Pipeline\n",
        "* Provides everything from data preparation ‚Üí training ‚Üí deployment.\n",
        "* No need to build everything manually.\n",
        "\n",
        "2. Wide Range of Pretrained Models\n",
        "* Offers a Model Zoo with optimized detection models like:\n",
        "  * SSD MobileNet (lightweight)\n",
        "  * Faster R-CNN (high accuracy)\n",
        "  * EfficientDet (accuracy vs. speed balance)\n",
        "\n",
        "3. Custom Training Support\n",
        "* You can train models on your own labeled data using:\n",
        "  * COCO format\n",
        "  * Pascal VOC\n",
        "  * TFRecord (native support)\n",
        "\n",
        "4. TensorFlow 2.x Compatibility\n",
        "* Fully supports **TensorFlow 2.x (eager execution)** and **Keras models**.\n",
        "\n",
        "5. Deployment Ready\n",
        "* Easily convert models to:\n",
        "  * TensorFlow Lite (for mobile)\n",
        "  * TensorFlow\\.js (for browser)\n",
        "  * TensorFlow Serving (for cloud APIs)\n",
        "\n",
        "6. Strong Community & Documentation\n",
        "* Maintained by Google\n",
        "* Huge community = easier to troubleshoot, find tutorials, and get updates\n",
        "\n",
        "Typical Use Cases:\n",
        "| Use Case             | Example                           |\n",
        "| -------------------- | --------------------------------- |\n",
        "| Retail Analytics     | People & product detection        |\n",
        "| Smart Farming        | Detect crop diseases or animals   |\n",
        "| Traffic Surveillance | Vehicle detection                 |\n",
        "| Industrial Safety    | Helmet/vest detection             |\n",
        "| Wildlife Monitoring  | Animal detection via camera traps |\n",
        "\n",
        "Table:\n",
        "\n",
        "| Feature                 | Why It Matters                     |\n",
        "| ------------------------| ---------------------------------- |\n",
        "|  All-in-one pipeline    | Easy training, testing, deployment |\n",
        "|  Pretrained models      | Train faster with better accuracy  |\n",
        "|  Custom dataset support | Works with your own labeled images |\n",
        "|  Multi-platform deploy  | Use on web, mobile, edge devices   |"
      ],
      "metadata": {
        "id": "2qBmXEr9aD0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q7. How Does Learning Rate Affect Model Training in Detectron2?\n",
        "\n",
        "What is Learning Rate?\n",
        "The learning rate controls how much the model updates its weights in response to the loss gradient during training.\n",
        "In Detectron2 (like all deep learning frameworks), it's a critical hyperparameter that directly affects:\n",
        "* How quickly the model learns\n",
        "* Whether the model converges to a good solution\n",
        "* Whether training becomes unstable\n",
        "\n",
        "Effects of Learning Rate on Model Training\n",
        "| Learning Rate Type | Effect on Training                                              |\n",
        "| ------------------ | --------------------------------------------------------------- |\n",
        "|  Optimal Rate      | Fast, stable convergence ‚Üí good accuracy                        |\n",
        "|  Too High          | Model may overshoot minima ‚Üí unstable, oscillating or diverging |\n",
        "|  Too Low           | Model trains very slowly or gets stuck in local minima          |\n",
        "\n",
        "In Detectron2: How It's Used\n",
        "Learning rate is set in the config file (`cfg.SOLVER.BASE_LR`) and is typically coupled with:\n",
        "* Warmup steps\n",
        "* Learning rate schedules (step decay, cosine annealing, etc.)\n",
        "```python\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 10000\n",
        "cfg.SOLVER.STEPS = [7000, 9000]  # LR decay steps\n",
        "```\n",
        "Example Scenarios\n",
        "| Learning Rate              | Observed Behavior in Detectron2                         |\n",
        "| -------------------------- | ------------------------------------------------------- |\n",
        "| `0.001` (default for COCO) | Stable, good baseline for pretrained models             |\n",
        "| `0.01`                     | May work with strong regularization or for small models |\n",
        "| `0.0001`                   | May require more iterations, useful when fine-tuning    |\n",
        "| `> 0.1`                    | Usually too high ‚Äî causes loss to spike or diverge      |\n",
        "\n",
        "Note: Monitor These While Training\n",
        "\n",
        "*  Loss curve: Should go down smoothly\n",
        "*  Validation accuracy: Should improve consistently\n",
        "*  Sudden spikes or flat lines = learning rate issue\n",
        "\n",
        "How to Tune It:\n",
        "1. Start with `0.00025` if fine-tuning from pretrained weights\n",
        "2. Use `WarmupFactor` and `WarmupSteps` to slowly ramp up LR\n",
        "3. Apply `STEP` or `COSINE` schedule to reduce LR during training\n",
        "\n",
        "Table:\n",
        "| Factor              | Impact                                   |\n",
        "| --------------------| ---------------------------------------- |\n",
        "|  High learning rate | Fast training, may destabilize learning  |\n",
        "|  Low learning rate  | Stable but slow training                 |\n",
        "|  Scheduled LR decay | Helps model settle into a better minimum |\n",
        "|  Monitored via loss | Loss drop = good; spikes = bad config    |"
      ],
      "metadata": {
        "id": "PQPXqkrTa0Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q8. Why Might Detectron2 Use PyTorch as Its Backend Framework?\n",
        "\n",
        "Detectron2 is built on PyTorch for several compelling reasons, both technical and practical. Here's a breakdown of why PyTorch is an ideal backend for a high-performance object detection framework like Detectron2:\n",
        "1. Dynamic Computation Graph (Eager Execution)\n",
        "* PyTorch allows dynamic graph creation ‚Äî the graph is built as the code runs.\n",
        "* This makes it easier to debug, modify, and customize models.\n",
        "Useful for research and prototyping complex detection models.\n",
        "\n",
        "2. High Performance with GPU Support\n",
        "* PyTorch provides built-in CUDA support and fast tensor operations.\n",
        "* Detectron2 benefits from PyTorch‚Äôs optimized operations for training and inference.\n",
        "Enables Detectron2 to run **efficiently on modern GPUs** for large-scale datasets like COCO.\n",
        "\n",
        "3. Modularity & Flexibility\n",
        "* PyTorch makes it easy to define custom layers, loss functions, and models.\n",
        "* Detectron2 leverages this to offer:\n",
        "  * Pluggable backbones (e.g., ResNet, Swin)\n",
        "  * Custom heads (e.g., Mask R-CNN, Keypoint R-CNN)\n",
        "Great for researchers and developers who want to tweak architecture.\n",
        "\n",
        "4. Ease of Integration\n",
        "* PyTorch integrates well with tools like:\n",
        "  * TensorBoard (for visualization)\n",
        "  * ONNX (for model export)\n",
        "  * TorchServe (for deployment)\n",
        "* This enhances Detectron2‚Äôs ability to go from research ‚Üí production smoothly.\n",
        "\n",
        "5. Strong Developer Community & Ecosystem\n",
        "* PyTorch has a large, active community with:\n",
        "  * Extensive tutorials\n",
        "  * Open-source projects\n",
        "  * Well-maintained libraries (e.g., torchvision, timm)\n",
        "Detectron2 can build on top of existing PyTorch tools and resources.\n",
        "\n",
        "6. Better for Research Use Cases\n",
        "Detectron2 is designed for research and experimentation. PyTorch:\n",
        "* Encourages readable code\n",
        "* Makes experimental modeling easier than static frameworks like TensorFlow 1.x\n",
        "\n",
        "Table\n",
        "| Reason                 | Benefit for Detectron2                  |\n",
        "| ---------------------- | --------------------------------------- |\n",
        "|  Dynamic graph         | Easier debugging, prototyping           |\n",
        "|  GPU acceleration      | High-performance training/inference     |\n",
        "|  Modular design        | Custom layers, heads, and backbones     |\n",
        "|  Ecosystem integration | Export to ONNX, TensorBoard, TorchServe |\n",
        "|  Active community      | Easier learning, faster development     |"
      ],
      "metadata": {
        "id": "e7lhwDrKa2IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q9. What Types of Pretrained Models Does TFOD2 Support?\n",
        "\n",
        "The TensorFlow Object Detection API v2 (TFOD2) supports a wide range of pretrained models for various object detection tasks. These models are available in the Model Zoo and come with pretrained weights (typically trained on the COCO dataset), making it easy to fine-tune them on your own custom datasets.\n",
        "\n",
        "Types of Pretrained Models in TFOD2\n",
        "1.Single Shot Detector (SSD) Models\n",
        "* Lightweight & Fast\n",
        "* Suitable for real-time applications\n",
        "* Good trade-off between speed and accuracy\n",
        "Examples:\n",
        "* `ssd_mobilenet_v2_fpnlite_320x320`\n",
        "* `ssd_mobilenet_v1_fpn_640x640`\n",
        "* `ssd_resnet50_v1_fpn_640x640`\n",
        "\n",
        "2.Faster R-CNN Models\n",
        "* Two-stage detectors with high accuracy\n",
        "* Slower than SSD but better for complex tasks\n",
        "Examples:\n",
        "* `faster_rcnn_resnet50_v1_640x640`\n",
        "* `faster_rcnn_inception_resnet_v2_640x640`\n",
        "* `faster_rcnn_nas`\n",
        "\n",
        "3.EfficientDet Models\n",
        "* Scalable models that balance speed and accuracy across various sizes\n",
        "* Efficient for deployment on multiple platforms\n",
        "Examples:\n",
        "* `efficientdet_d0_coco`\n",
        "* `efficientdet_d1_coco` to `d7` (higher = more accurate, slower)\n",
        "\n",
        "4.CenterNet Models\n",
        "* Keypoint-based detection architecture\n",
        "* Good for detecting objects and keypoints (pose)\n",
        "Examples:\n",
        "* `centernet_hourglass_512x512`\n",
        "* `centernet_resnet50_v2`\n",
        "\n",
        "5.RetinaNet Models\n",
        "* Single-stage detectors using focal loss for better class imbalance handling\n",
        "* High accuracy for dense object detection\n",
        "Examples:\n",
        "* `retinanet_resnet50_fpn_640x640`\n",
        "\n",
        "6.Mask R-CNN Models (Instance Segmentation)\n",
        "* Detects objects and generates pixel-level masks\n",
        "Examples:\n",
        "* `mask_rcnn_inception_resnet_v2`\n",
        "* `mask_rcnn_resnet50_fpn_1024x1024`\n",
        "\n",
        "Table\n",
        "| Model Type   | Key Feature                     | Example                            |\n",
        "| ------------ | ------------------------------- | ---------------------------------- |\n",
        "| SSD          | Fast, lightweight               | `ssd_mobilenet_v2_fpnlite_320x320` |\n",
        "| Faster R-CNN | Accurate, slower                | `faster_rcnn_resnet50`             |\n",
        "| EfficientDet | Scalable accuracy-speed balance | `efficientdet_d0` to `d7`          |\n",
        "| CenterNet    | Keypoint-based detection        | `centernet_hourglass`              |\n",
        "| RetinaNet    | Handles class imbalance well    | `retinanet_resnet50`               |\n",
        "| Mask R-CNN   | Instance segmentation           | `mask_rcnn_resnet50_fpn`           |"
      ],
      "metadata": {
        "id": "pEEluCCBbc8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q10. How Can Data Path Errors Impact Detectron2?\n",
        "In Detectron2, like most deep learning frameworks, correct data paths are crucial. If they are misconfigured or invalid, the training or inference process will fail or behave incorrectly.\n",
        "\n",
        "What Is a Data Path Error?\n",
        "A data path error happens when:\n",
        "* The path to image files, annotations, or config files is incorrect or missing\n",
        "* The specified dataset name in the config doesn‚Äôt match any registered dataset\n",
        "* Paths to pretrained weights or output folders are invalid or inaccessible\n",
        "\n",
        "How These Errors Impact Detectron2\n",
        "1.Training Fails to Start\n",
        "If the dataset path is wrong:\n",
        "```python\n",
        "KeyError: 'Dataset not registered'\n",
        "```\n",
        "If image files or annotations can't be loaded:\n",
        "```python\n",
        "FileNotFoundError: [Errno 2] No such file or directory: 'images/train/image1.jpg'\n",
        "```\n",
        "2.Evaluation Crashes or Returns Zero Results\n",
        "If Detectron2 can‚Äôt find or read the validation data, it can‚Äôt evaluate model performance:\n",
        "* No predictions\n",
        "* No AP/mAP scores\n",
        "Incorrect Learning (Garbage In, Garbage Out)\n",
        "\n",
        "* If image and annotation paths are mismatched, the model may learn incorrect relationships.\n",
        "* If wrong file extensions or corrupt files are used, training can behave unpredictably.\n",
        "\n",
        "4. Checkpoints & Outputs Not Saved Properly\n",
        "If output directories aren‚Äôt correctly set:\n",
        "```python\n",
        "cfg.OUTPUT_DIR = \"outputs/my_model\"\n",
        "```\n",
        "Then:\n",
        "* Training logs, checkpoints, or final weights might not be saved.\n",
        "* TensorBoard might not work.\n",
        "\n",
        "How to Prevent Data Path Errors:\n",
        "| Checkpoint            | How to Handle                                     |\n",
        "| ----------------------| ------------------------------------------------- |\n",
        "|  Dataset Registration | Use `register_coco_instances()` correctly         |\n",
        "|  Absolute Paths       | Avoid relative paths unless you're sure they work |\n",
        "|  Verify Files         | Ensure images and annotations actually exist      |\n",
        "|  Print Paths          | Use `print(os.path.abspath(...))` for debugging   |\n",
        "|  Use `os.path.join()` | Ensures cross-platform compatibility              |\n",
        "\n",
        "Example: Registering Dataset Correctly\n",
        "```python\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "register_coco_instances(\n",
        "    \"my_dataset_train\",\n",
        "    {},\n",
        "    \"annotations/instances_train.json\",\n",
        "    \"images/train\"\n",
        ")\n",
        "```\n",
        "* Make sure `\"images/train\"` and the JSON file are valid and synced.\n",
        "\n",
        "#Table\n",
        "| Impact                   | Caused By                                   |\n",
        "| -------------------------| ------------------------------------------- |\n",
        "| Training/inference crash | Missing or incorrect image/annotation paths |\n",
        "| 0% Accuracy              | Wrong annotation/image mapping              |\n",
        "| No model output          | Output path not defined or invalid          |\n",
        "|  Wrong labels or boxes   | Bad or mismatched annotation files          |"
      ],
      "metadata": {
        "id": "AsxDeYJ-ced9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "e5338183-6ba4-4e2c-f620-720594d558bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-1-500292782.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1-500292782.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    '''Q10. How Can Data Path Errors Impact Detectron2?\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q11. What is Detectron2?\n",
        "\n",
        "Detectron2 is a next-generation object detection and segmentation library developed by Facebook AI Research (FAIR).\n",
        "It is built on PyTorch and provides a modular, flexible, and scalable framework for a wide range of **computer vision tasks.\n",
        "\n",
        "Key Features of Detectron2\n",
        "| Feature                  | Description                                                                 |\n",
        "| -------------------------| --------------------------------------------------------------------------- |\n",
        "|  Built on PyTorch        | Modern deep learning framework with dynamic graph support                   |\n",
        "|  Model Zoo               | Includes state-of-the-art pretrained models like Faster R-CNN, Mask R-CNN   |\n",
        "|  Highly Modular          | Easy to plug in custom datasets, models, loss functions, or backbones       |\n",
        "|  Supports Many Tasks     | Detection, instance segmentation, semantic segmentation, keypoint detection |\n",
        "|  GPU Acceleration        | Optimized for high-speed training and inference using CUDA                  |\n",
        "|  Strong Evaluation Tools | Built-in COCO-style evaluation metrics like mAP                             |\n",
        "|  Research-Grade          | Designed to support rapid prototyping and cutting-edge vision research      |\n",
        "\n",
        "Tasks Supported by Detectron2\n",
        "1.  Object Detection\n",
        "2.  Instance Segmentation\n",
        "3.  Semantic Segmentation\n",
        "4.  Keypoint Detection (e.g., human pose)\n",
        "5.  Panoptic Segmentation\n",
        "6.  DensePose (3D human surface mapping)\n",
        "\n",
        "What Can You Do with Detectron2?\n",
        "* Train custom object detectors (e.g., helmet, vehicle, defect detection)\n",
        "* Fine-tune existing models on new datasets\n",
        "* Deploy models for inference on images, video, or webcam\n",
        "* Evaluate detection results with detailed metrics\n",
        "* Extend and experiment with novel architectures or tasks\n",
        "\n",
        "Example Usage:\n",
        "```python\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(cv2.imread(\"image.jpg\"))\n",
        "``\n",
        "Summary:\n",
        "Detectron2 is a powerful, flexible, and research-grade deep learning framework for object detection and segmentation, built on PyTorch and widely used in academia and industry."
      ],
      "metadata": {
        "id": "QyY1SbObcyCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q12. What Are TFRecord Files, and Why Are They Used in TFOD2?\n",
        "\n",
        "What Is a TFRecord File?\n",
        "A TFRecord is a binary file format used by TensorFlow to efficiently store large datasets ‚Äî especially for tasks like object detection and image classification.\n",
        "\n",
        "It contains a series of serialized `tf.train.Example` protocol buffers, which represent individual data points (e.g., an image + annotations).\n",
        "\n",
        "Why Are TFRecord Files Used in TFOD2?\n",
        "The TensorFlow Object Detection API v2 (TFOD2) expects training data to be in TFRecord format for several important reasons:\n",
        "\n",
        "1. Efficiency & Performance\n",
        "* TFRecords are compact and binary, making them faster to read than individual JPEG/PNG and JSON/XML files.\n",
        "* Useful when training on large datasets with thousands of images.\n",
        "Faster data loading ‚Üí faster training\n",
        "\n",
        "2. Unified Storage of Image + Annotations\n",
        "* Stores image data, bounding boxes, labels, and even masks/keypoints in one record.\n",
        "* No need to sync separate annotation files (like COCO JSON or Pascal VOC XML).\n",
        "Prevents mismatch errors during training.\n",
        "\n",
        "3. Native Compatibility with TFOD2 Pipelines\n",
        "* TFOD2 uses input pipelines (`tf.data.TFRecordDataset`) that work seamlessly with TFRecord files.\n",
        "* Required for training, validation, and even evaluation steps.\n",
        "Eliminates need for custom data loaders.\n",
        "\n",
        " 4. Supports Streaming and Prefetching\n",
        "* TFRecord + TensorFlow‚Äôs `tf.data` API enables:\n",
        "  * Shuffling\n",
        "  * Prefetching\n",
        "  * Parallel loading\n",
        "   resource usage (CPU/GPU), especially during multi-epoch training.\n",
        "\n",
        "What Can a TFRecord Contain?\n",
        "Each example in the TFRecord might include:\n",
        "\n",
        "* `image/encoded`: raw image bytes\n",
        "* `image/filename`: name of the image\n",
        "* `image/object/bbox/xmin`, `ymin`, `xmax`, `ymax`\n",
        "* `image/object/class/label`\n",
        "* (Optional) `mask`, `keypoints`, `segmentation`, etc.\n",
        "\n",
        "How to Create a TFRecord File?\n",
        "Typically using tools like:\n",
        "* `generate_tfrecord.py` (custom scripts)\n",
        "* [Roboflow](https://roboflow.com) (automated conversion)\n",
        "* TensorFlow‚Äôs dataset preparation notebooks\n",
        "\n",
        "Table:\n",
        "| Aspect               | TFRecord Format Advantage               |\n",
        "| ---------------------| --------------------------------------- |\n",
        "|  Format type         | Binary (compact and fast)               |\n",
        "|  Stores              | Image + annotations in one place        |\n",
        "|  Performance         | Faster read and training times          |\n",
        "|  Required for        | TFOD2 training & evaluation             |\n",
        "|  Integration with TF | Works natively with `tf.data` pipelines |\n"
      ],
      "metadata": {
        "id": "xqB66FU1c1lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q13. What Evaluation Metrics Are Typically Used with Detectron2?\n",
        "\n",
        "Detectron2 uses COCO-style evaluation metrics, which are industry-standard for object detection and segmentation tasks. These metrics are designed to assess how accurately the model detects, classifies, and localizes objects in images.\n",
        "Main Evaluation Metrics Used in Detectron2:\n",
        "\n",
        "1. AP (Average Precision)\n",
        "* Primary metric used in COCO evaluation\n",
        "* Measures precision across different IoU thresholds\n",
        "* Reported as:\n",
        "  * `AP@[.5:.95]` (average over IoU = 0.5 to 0.95 in 0.05 steps)\n",
        "  * Higher is better\n",
        "Used to summarize model accuracy across a range of detection qualities.\n",
        "\n",
        "2. AP50 and AP75\n",
        "* AP50= Average Precision at IoU = 0.5 (more lenient)\n",
        "* AP75= Average Precision at IoU = 0.75 (stricter)\n",
        "| Metric | Interpretation                           |\n",
        "| ------ | ---------------------------------------- |\n",
        "| `AP50` | Did the model detect the object at all?  |\n",
        "| `AP75` | Did it detect the object **accurately**? |\n",
        "\n",
        "3. AP (Small, Medium, Large)\n",
        "* AP is also broken down by object size:\n",
        "  * `AP_small` ‚Äì objects < 32¬≤ px\n",
        "  * `AP_medium` ‚Äì objects 32¬≤ to 96¬≤ px\n",
        "  * `AP_large` ‚Äì objects > 96¬≤ px\n",
        "Helps understand performance on different object scales.\n",
        "\n",
        "4. AR (Average Recall)\n",
        "* Measures how well the model finds all objects\n",
        "* Reported as:\n",
        "  * `AR@1`, `AR@10`, `AR@100` ‚Üí max detections per image\n",
        "  * `AR_small`, `AR_medium`, `AR_large` ‚Üí recall per object size\n",
        "High recall = fewer missed objects.\n",
        "\n",
        "Example Output from Detectron2:\n",
        "```bash\n",
        "Evaluation results for bbox:\n",
        "AP50:  0.78\n",
        "AP75:  0.63\n",
        "AP:    0.60\n",
        "AP_small:  0.42\n",
        "AP_medium: 0.57\n",
        "AP_large:  0.71\n",
        "```"
      ],
      "metadata": {
        "id": "NFUxS3nYd423"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q14. How Do You Perform Inference with a Trained Detectron2 Model?\n",
        "Run inference (object detection or segmentation) on images using a trained Detectron2 model.\n",
        "Step-by-Step Process\n",
        "Step 1: Import Required Modules\n",
        "```python\n",
        "import cv2\n",
        "import torch\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "```\n",
        "\n",
        "Step 2: Load Config and Trained Weights\n",
        "```python\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # or your custom config\n",
        "cfg.MODEL.WEIGHTS = \"output/model_final.pth\"  # path to your trained model weights\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # confidence threshold\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "```\n",
        "Step 3: Load an Image for Inference\n",
        "```python\n",
        "image = cv2.imread(\"your_test_image.jpg\")\n",
        "```\n",
        "\n",
        "Step 4: Run Inference\n",
        "```python\n",
        "outputs = predictor(image)\n",
        "# Access prediction results\n",
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)\n",
        "```\n",
        "\n",
        "Step 5: Visualize the Results (Optional)\n",
        "```python\n",
        "v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2.imshow(\"Result\", out.get_image()[:, :, ::-1])\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "```"
      ],
      "metadata": {
        "id": "IRkicD4ReG6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q15. What Does TFOD2 Stand For, and What Is It Designed For?\n",
        "\n",
        "TFOD2 stands for:\n",
        "TensorFlow Object Detection API v2\n",
        "What Is It?\n",
        "\n",
        "TFOD2 is a powerful open-source framework developed by Google for building, training, evaluating, and deploying object detection models using TensorFlow 2.x.\n",
        "It‚Äôs widely used in both research and industry due to its flexibility, ease of use, and strong performance.\n",
        "\n",
        "What Is TFOD2 Designed For?\n",
        "TFOD2 is specifically designed to:\n",
        "1.Train Object Detection Models\n",
        "* On custom datasets using TFRecord format\n",
        "* With support for bounding boxes, masks, keypoints, etc.\n",
        "\n",
        "2. Perform Inference on Images/Videos\n",
        "* Load pretrained or custom models to detect objects in real-world images\n",
        "\n",
        "3. Fine-tune Pretrained Models\n",
        "* Offers a large Model Zoo with pretrained models like:\n",
        "  * SSD\n",
        "  * Faster R-CNN\n",
        "  * EfficientDet\n",
        "  * CenterNet\n",
        "  * Mask R-CNN\n",
        "\n",
        "4. Evaluate Model Performance\n",
        "* Built-in tools for calculating COCO-style metrics like mAP, precision, recall\n",
        "\n",
        "5. Deploy Trained Models\n",
        "* Export to TensorFlow Lite, TensorFlow\\.js, or TensorFlow Serving for:\n",
        "  * Mobile\n",
        "  * Web\n",
        "  * Cloud APIs"
      ],
      "metadata": {
        "id": "bMTG6muGe2sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q16. What Does Fine-Tuning Pretrained Weights Involve?\n",
        "Fine-tuning involves taking a model that was already trained on a large dataset (like COCO or ImageNet) and retraining it on your own custom dataset for a similar task (e.g., object detection).\n",
        "Instead of training a model from scratch, you start with a pretrained model and adapt it to your new data ‚Äî saving time and improving performance, especially when your dataset is small.\n",
        "What Fine-Tuning Involves (Step-by-Step)\n",
        "\n",
        "1. Load Pretrained Weights\n",
        "* These are the base weights learned from large-scale datasets (e.g., COCO 2017).\n",
        "* In TFOD2, you set this in the pipeline config:\n",
        "```protobuf\n",
        "fine_tune_checkpoint: \"pretrained_model/model.ckpt\"\n",
        "```\n",
        "2. Freeze Some Layers (Optional)\n",
        "You can freeze early layers of the backbone (e.g., first few ResNet blocks) to:\n",
        "* Preserve low-level feature extraction\n",
        "* Avoid overfitting\n",
        "\n",
        "```protobuf\n",
        "trainable_variables: [\"Conv_6\", \"BoxPredictor\"]\n",
        "```\n",
        "3. Update Detection Head for Custom Classes\n",
        "* Replace the classification and regression heads to match the number of your own classes.\n",
        "* Modify:\n",
        "\n",
        "```protobuf\n",
        "num_classes: 5  # for 5 custom object classes\n",
        "```\n",
        "4. Train on Custom Dataset\n",
        "* Use your annotated TFRecord files and label map.\n",
        "* Continue training using the pretrained weights as a base.\n",
        "\n",
        "5. Use Lower Learning Rate\n",
        "* Fine-tuning typically uses a smaller learning rate (e.g., `0.0001`) to avoid \"destroying\" the pretrained knowledge too quickly.\n",
        "\n",
        "6. Evaluate and Save Checkpoints\n",
        "* Monitor mAP, loss, and overfitting.\n",
        "* Use checkpoints to resume or deploy the model."
      ],
      "metadata": {
        "id": "q1Wkc_ygfMHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q17. How Is Training Started in TFOD2 (TensorFlow Object Detection API v2)?\n",
        "To start training in TFOD2, you must complete several setup steps first. Here's the full workflow to launch training using the built-in tools.\n",
        "\n",
        "Prerequisites Before Training\n",
        "You need:\n",
        "1.  A labeled dataset converted to TFRecord format\n",
        "2.  A label map file (e.g., `label_map.pbtxt`)\n",
        "3.  A pipeline config file specifying model, paths, parameters\n",
        "4.  A pretrained model checkpoint (optional but recommended)\n",
        "\n",
        "Directory Structure Example\n",
        "```\n",
        "/workspace/\n",
        "‚îú‚îÄ‚îÄ annotations/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ train.record\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ val.record\n",
        "‚îú‚îÄ‚îÄ images/\n",
        "‚îú‚îÄ‚îÄ models/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ my_model/\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ pipeline.config\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ pre-trained-checkpoint/\n",
        "‚îú‚îÄ‚îÄ label_map.pbtxt\n",
        "```\n",
        "Step-by-Step: Start Training\n",
        "Step 1: Activate Your Environment (Optional)\n",
        "```bash\n",
        "conda activate tfod2_env\n",
        "```\n",
        "Step 2: Run the Training Command\n",
        "From your model directory:\n",
        "```bash\n",
        "python model_main_tf2.py \\\n",
        "  --model_dir=models/my_model/ \\\n",
        "  --pipeline_config_path=models/my_model/pipeline.config\n",
        "```\n",
        "Step 3: Monitor Progress with TensorBoard (Optional)\n",
        "```bash\n",
        "tensorboard --logdir=models/my_model/\n",
        "```\n",
        "\n",
        "This shows:\n",
        "* Training and validation loss\n",
        "* mAP (mean Average Precision)\n",
        "* Learning rate, steps, etc.\n",
        "\n",
        "Important Fields in `pipeline.config`\n",
        "```protobuf\n",
        "train_input_reader {\n",
        "  label_map_path: \"label_map.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"annotations/train.record\"\n",
        "  }\n",
        "}\n",
        "\n",
        "eval_input_reader {\n",
        "  label_map_path: \"label_map.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"annotations/val.record\"\n",
        "  }\n",
        "}\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 3\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config {\n",
        "  batch_size: 8\n",
        "  fine_tune_checkpoint: \"pre-trained-checkpoint/model.ckpt\"\n",
        "  num_steps: 10000\n",
        "}\n",
        "Table:\n",
        "|Stp | What You Do                               |\n",
        "| ---| ----------------------------------------- |\n",
        "| 1  | Prepare TFRecord files and label map      |\n",
        "| 2  | Choose and configure a pretrained model   |\n",
        "| 3  | Edit `pipeline.config` with correct paths |\n",
        "| 4  | Run `model_main_tf2.py` to start training |\n",
        "| 5  | Monitor with TensorBoard (optional)       |"
      ],
      "metadata": {
        "id": "2qid4dAIgSj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q18. What Does COCO Format Represent and Why Is It Popular in Detectron2?\n",
        "\n",
        "What is COCO Format?\n",
        "COCO stands for Common Objects in Context. It's a JSON-based annotation format used to describe image datasets for tasks like:\n",
        "* Object detection\n",
        "* Instance segmentation\n",
        "* Keypoint detection\n",
        "* Panoptic segmentation\n",
        "\n",
        "COCO Format Structure:\n",
        "A typical `annotations.json` file contains:\n",
        "| Key           | Description                                       |\n",
        "| ------------- | ------------------------------------------------- |\n",
        "| `images`      | List of images with filename, height, width, ID   |\n",
        "| `annotations` | Bounding boxes, segmentation, category, image\\_id |\n",
        "| `categories`  | List of class labels with IDs                     |\n",
        "\n",
        "Example (Simplified):\n",
        "```json\n",
        "{\n",
        "  \"images\": [\n",
        "    {\"id\": 1, \"file_name\": \"image1.jpg\", \"height\": 600, \"width\": 800}\n",
        "  ],\n",
        "  \"annotations\": [\n",
        "    {\"id\": 101, \"image_id\": 1, \"category_id\": 3, \"bbox\": [100, 150, 200, 100], \"iscrowd\": 0}\n",
        "  ],\n",
        "  \"categories\": [\n",
        "    {\"id\": 3, \"name\": \"person\"}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "* `bbox` = \\[x\\_min, y\\_min, width, height]\n",
        "* `category_id` matches the class name from `categories`\n",
        "* Supports **instance masks** via `segmentation` field (for segmentation tasks)\n",
        "\n",
        "Why Is COCO Format Popular in Detectron2?\n",
        "1. Official Format for Pretrained Models\n",
        "* Most Detectron2 pretrained models are trained on the COCO dataset.\n",
        "* To use them effectively, your dataset must be in the same format.\n",
        "\n",
        "2. Standardized and Extensible\n",
        "* Works across detection, segmentation, and keypoint tasks\n",
        "* Rich enough for multi-object, multi-label, and complex scenes\n",
        "\n",
        "3. Easily Integrated in Detectron2\n",
        "```python\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "register_coco_instances(\n",
        "    \"my_dataset_train\", {},\n",
        "    \"annotations/instances_train.json\",\n",
        "    \"images/train\"\n",
        ")\n",
        "``\n",
        "* Detectron2 reads COCO format natively, making it very convenient.\n",
        "4. Widely Supported by Tools\n",
        "* Annotation tools like:\n",
        "  * LabelMe\n",
        "  * CVAT\n",
        "  * Roboflow\n",
        "* Support export to COCO JSON format\n",
        "\n",
        "5. Built-in Evaluation with COCO Metrics\n",
        "* Metrics like `mAP@[.5:.95]`, `AP_small`, `AR@100` are all COCO-standard.\n",
        "* Detectron2's `COCOEvaluator` uses these metrics by default."
      ],
      "metadata": {
        "id": "Dr2jydJZ5I6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q19. Why Is Evaluation Curve Plotting Important in Detectron2?\n",
        "\n",
        "Evaluation curve plotting in Detectron2 is essential because it provides a visual and quantitative understanding of your model‚Äôs performance across different thresholds and conditions.\n",
        "What Are Evaluation Curves?\n",
        "In the context of object detection and Detectron2, common evaluation curves include:\n",
        "\n",
        "| Curve Type                      | Description                                                                       |\n",
        "| ------------------------------- | --------------------------------------------------------------------------------- |\n",
        "| Precision-Recall (PR) Curve     | Shows the trade-off between precision and recall at various confidence thresholds |\n",
        "| IoU vs. AP Curve                | Shows how AP changes across different IoU thresholds (e.g., 0.5 to 0.95)          |\n",
        "| mAP over Epochs                 | Tracks mean Average Precision (mAP) across training steps/epochs                  |\n",
        "| Loss Curves                     | Tracks training and validation loss over time                                     |\n",
        "\n",
        "Why Is Curve Plotting Important?\n",
        "\n",
        "1.Gives Deeper Insight Beyond a Single Metric\n",
        "* Instead of a single mAP score, curves reveal how performance changes with confidence thresholds.\n",
        "* You can detect overconfidence or underconfidence in predictions.\n",
        "\n",
        "2. Helps in Hyperparameter Tuning\n",
        "* Adjusting:\n",
        "  * Confidence thresholds\n",
        "  * NMS thresholds\n",
        "  * Learning rate, augmentation\n",
        "* You can see how changes impact precision/recall trade-offs.\n",
        "\n",
        "3.Detects Overfitting or Underfitting Early\n",
        "* Loss curves can indicate:\n",
        "  * Overfitting (training loss ‚Üì, validation loss ‚Üë)\n",
        "  * Underfitting (both losses high)\n",
        "Helps decide when to stop training or adjust the model.\n",
        "\n",
        "4.Improves Decision-Making for Deployment\n",
        "* Helps you choose:\n",
        "  * The best confidence threshold for production\n",
        "  * Whether to prioritize precision (fewer false positives) or recall (fewer missed detections)\n",
        "\n",
        "5. Visual Debugging Tool\n",
        "* If precision drops sharply at a threshold, your model may be predicting many false positives at that level.\n",
        "\n",
        "Example: Precision-Recall Curve (PR Curve)\n",
        "```python\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "precision, recall, _ = precision_recall_curve(true_labels, predicted_scores)\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "Example: Plotting Loss Curves in Detectron2\n",
        "Detectron2 logs training loss (e.g., in `metrics.json`):\n",
        "```python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_json(\"output/metrics.json\", lines=True)\n",
        "plt.plot(df[\"iteration\"], df[\"total_loss\"])\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "``"
      ],
      "metadata": {
        "id": "S3AzB_FM6dbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q20. How Do You Configure Data Paths in TFOD2 (TensorFlow Object Detection API v2)?\n",
        "\n",
        "In TFOD2, data paths are configured in the `pipeline.config` file. This file defines where your TFRecord files, label map, checkpoints, and model outputs are located.\n",
        "You must edit this file to point to the correct locations for your custom dataset.\n",
        "\n",
        "Main Sections That Require Data Paths in `pipeline.config`\n",
        "1. Training Data Path\n",
        "```protobuf\n",
        "train_input_reader {\n",
        "  label_map_path: \"annotations/label_map.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"annotations/train.record\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "2. Validation Data Path\n",
        "```protobuf\n",
        "eval_input_reader {\n",
        "  label_map_path: \"annotations/label_map.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"annotations/val.record\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "3. Checkpoint Path (Pretrained Model)\n",
        "```protobuf\n",
        "train_config {\n",
        "  fine_tune_checkpoint: \"pretrained_model/ckpt-0\"\n",
        "  fine_tune_checkpoint_type: \"detection\"\n",
        "}\n",
        "```\n",
        "4. Output Directory (when you run training)\n",
        "When you run training with this command:\n",
        "```bash\n",
        "python model_main_tf2.py \\\n",
        "  --model_dir=models/my_model/ \\\n",
        "  --pipeline_config_path=models/my_model/pipeline.config\n",
        "```\n",
        "The training outputs (checkpoints, logs) go to:\n",
        "```\n",
        "models/my_model/\n",
        "```\n",
        "Best Practices for Setting Paths\n",
        "| Task                          | Tip                                                         |\n",
        "| ----------------------------- | ----------------------------------------------------------- |\n",
        "| Use absolute paths (optional) | Helps avoid path errors when running from other directories |\n",
        "| Use consistent naming         | Example: `annotations/train.record`, `images/train/`        |\n",
        "| Avoid typos or missing files  | Common cause of TFOD2 crashing                              |\n",
        "| Keep label map in root        | Easy to reference in both train/eval sections               |\n",
        "\n",
        "Typical Folder Structure\n",
        "```\n",
        "project/\n",
        "‚îú‚îÄ‚îÄ annotations/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ train.record\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ val.record\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ label_map.pbtxt\n",
        "‚îú‚îÄ‚îÄ images/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
        "‚îú‚îÄ‚îÄ pretrained_model/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ckpt-0, .index, .data-00000-of-00001\n",
        "‚îú‚îÄ‚îÄ models/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ my_model/\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ pipeline.config\n",
        "```"
      ],
      "metadata": {
        "id": "0eYntC826jYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q21. Can You Run Detectron2 on a CPU?\n",
        "you can run Detectron2 on a CPU, although it will be much slower compared to running on a GPU.\n",
        "\n",
        "How to Run Detectron2 on a CPU\n",
        "1. Install Detectron2 with CPU Support\n",
        "Use the standard installation command ‚Äî no changes are required for CPU.\n",
        "\n",
        "```bash\n",
        "pip install detectron2 -f \\\n",
        "https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch2.0/index.html\n",
        "```\n",
        "> Make sure your PyTorch is installed with CPU support:\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision torchaudio\n",
        "```2. Set the Device to CPU in Your Code\n",
        "When configuring the model:\n",
        "\n",
        "```python\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "```\n",
        "\n",
        "3. Run Training or Inference\n",
        "Inference will work out of the box.\n",
        "Training will also work, but it will be very slow, especially with larger models (e.g., ResNet-101).\n",
        "\n"
      ],
      "metadata": {
        "id": "-GUGNa6D69RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q22. Why Are Label Maps Used in TFOD2 (TensorFlow Object Detection API v2)?\n",
        "\n",
        "What Is a Label Map?\n",
        "\n",
        "A label map in TFOD2 is a `.pbtxt` file that defines the mapping between class IDs (integers) and class names (strings) used in object detection.\n",
        "\n",
        "Example of a Label Map (`label_map.pbtxt`):\n",
        "\n",
        "```protobuf\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'person'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'helmet'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 3\n",
        "  name: 'gloves'\n",
        "}\n",
        "```\n",
        "Why Are Label Maps Important in TFOD2?\n",
        "1.Links Numeric Labels to Class Names\n",
        "* The training data uses numeric IDs (e.g., 1, 2, 3) to represent object classes.\n",
        "* The label map translates those IDs into human-readable names (e.g., \"helmet\", \"gloves\").\n",
        "\n",
        "2.Required for Model Training and Evaluation\n",
        "* Training: Used to match predictions with ground truth labels.\n",
        "* Evaluation: Required for COCO mAP calculation (to know which class is which).\n",
        "* Visualization: So that bounding boxes are labeled correctly in outputs.\n",
        "\n",
        "3. Supports Custom Class Definitions\n",
        "* Whether you're using 2 classes or 200, the label map lets you define your own.\n",
        "Essential for custom object detection projects.\n",
        "\n",
        "4. Ensures Consistency Across Components\n",
        "\n",
        "| Component           | Uses Label Map For           |\n",
        "| ------------------- | ---------------------------- |\n",
        "|  `train.record`     | Stores class IDs only        |\n",
        "|  Evaluation metrics | Match predictions to classes |\n",
        "|  Visualization      | Show correct class names     |\n",
        "|  TFOD2 model        | Internally references labels |\n",
        "\n",
        "5. Required in `pipeline.config`\n",
        "You must specify the path in both training and eval sections:\n",
        "\n",
        "```protobuf\n",
        "train_input_reader {\n",
        "  label_map_path: \"annotations/label_map.pbtxt\"\n",
        "  ...\n",
        "}\n",
        "\n",
        "eval_input_reader {\n",
        "  label_map_path: \"annotations/label_map.pbtxt\"\n",
        "  ...\n",
        "}"
      ],
      "metadata": {
        "id": "JrNwUUK67We7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q23. What Makes TFOD2 (TensorFlow Object Detection API v2) Popular for Real-Time Detection Tasks?\n",
        "\n",
        "TFOD2 is widely used for real-time object detection due to a combination of its performance, flexibility, and deployment capabilities.\n",
        "\n",
        "1. Support for Lightweight, Fast Models\n",
        "TFOD2 provides out-of-the-box support for real-time-capable models like:\n",
        "| Model         | Key Feature                             |\n",
        "| --------------| --------------------------------------- |\n",
        "| SSD MobileNet | Extremely fast, mobile-ready            |\n",
        "| EfficientDet  | Good balance between speed and accuracy |\n",
        "| CenterNet     | Efficient for small and large objects   |\n",
        "\n",
        "These models run in real-time on edge devices like mobile phones, Raspberry Pi, or Jetson Nano.\n",
        "\n",
        "2. Easy Access to Pretrained Models\n",
        "* TFOD2‚Äôs Model Zoo offers dozens of pretrained models.\n",
        "* You can fine-tune them quickly on custom data.\n",
        "Saves time and accelerates real-world deployment.\n",
        "\n",
        "3. Built-in Optimization for Deployment\n",
        "* Can export models to:\n",
        "  * TensorFlow Lite (TFLite) for mobile\n",
        "  * TensorFlow\\.js for browser-based apps\n",
        "  * TensorFlow Serving for cloud APIs\n",
        "\n",
        "4. End-to-End Pipeline\n",
        "TFOD2 offers a complete pipeline from:\n",
        "* Data preprocessing ‚ûù\n",
        "* Training ‚ûù\n",
        "* Evaluation ‚ûù\n",
        "* Inference ‚ûù\n",
        "* Export and Deployment\n",
        "Everything is integrated, which reduces development time and errors.\n",
        "\n",
        "5. Real-Time Performance Monitoring\n",
        "* TensorBoard integration helps monitor loss, mAP, and other metrics in real time.\n",
        "* Easy to identify performance bottlenecks and improve training speed.\n",
        "\n",
        "6. Highly Customizable\n",
        "* Supports custom models, datasets, loss functions, and architectures.\n",
        "* Scalable for both lightweight applications and large-scale deployment.\n",
        "\n",
        "7. COCO Evaluation Metrics Built-In\n",
        "* Standardized accuracy measurement using:\n",
        "  * mAP\n",
        "  * AR\n",
        "  * Precision/Recall\n",
        "* Ensures fair comparison across different model versions and tasks.\n",
        "\n",
        "8. Popular in Production and Industry\n",
        "* Used in applications like:\n",
        "  * Smart security cameras\n",
        "  * Real-time inventory scanning\n",
        "  * Vehicle detection\n",
        "  * Safety PPE detection\n",
        "  * Wildlife monitoring"
      ],
      "metadata": {
        "id": "Ari_DRsA7wsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q24. How Does Batch Size Impact GPU Memory Usage?\n",
        "\n",
        "Batch Size refers to the number of samples processed together in one forward/backward pass during model training or inference.\n",
        "\n",
        "1. Larger Batch Size = Higher GPU Memory Consumption\n",
        "Each sample in a batch requires:\n",
        "* Input data memory\n",
        "* Intermediate feature maps\n",
        "* Gradients and optimizer states\n",
        "\n",
        "So, if batch size = 32, you need 32√ó memory for each of those components.\n",
        "Memory usage grows roughly linearly** with batch size.\n",
        "\n",
        "2. Smaller Batch Size = Less GPU Memory, Slower Convergence\n",
        "Pros:\n",
        "* Less memory usage\n",
        "* Works on smaller GPUs\n",
        "Cons:\n",
        "* Noisy gradients (less stable training)\n",
        "* May require more steps for convergence\n",
        "\n",
        "TFOD2 Tip:\n",
        "You can control batch size in the `pipeline.config`:\n",
        "```protobuf\n",
        "train_config {\n",
        "  batch_size: 8\n",
        "}\n",
        "```\n",
        "\n",
        "If you're getting memory errors, reduce the batch size (e.g., from 16 ‚Üí 8 ‚Üí 4)."
      ],
      "metadata": {
        "id": "D7009SPV8s5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q25. What‚Äôs the Role of Intersection over Union (IoU) in Model Evaluation?\n",
        "\n",
        "What Is IoU (Intersection over Union)?\n",
        "Intersection over Union (IoU) is a metric used to evaluate how well the predicted bounding box from an object detection model matches the ground truth bounding box.\n",
        "\n",
        "1.Measures Localization Accuracy\n",
        "* A high IoU means the predicted box closely matches the actual object‚Äôs location.\n",
        "* A low IoU indicates poor object localization, even if the correct object class is predicted.\n",
        "\n",
        "2. Used to Define a ‚ÄúCorrect‚Äù Prediction\n",
        "In object detection tasks:\n",
        "* A prediction is considered correct (true positive) only if:\n",
        "  $$\n",
        "  \\text{IoU} \\geq \\text{threshold (e.g., 0.5)}\n",
        "  $$\n",
        "* Common threshold values:\n",
        "  * 0.5 (used in PASCAL VOC)\n",
        "  * 0.5‚Äì0.95 (used in COCO evaluation)\n",
        "\n",
        "3. Enables Calculation of Key Metrics\n",
        "| Metric        | Role of IoU                        |\n",
        "| ------------- | ---------------------------------- |\n",
        "| Precision     | TP / (TP + FP), based on IoU ‚â• 0.5 |\n",
        "| Recall        | TP / (TP + FN), based on IoU       |\n",
        "| mAP           | Uses IoU thresholds (0.5‚Äì0.95)     |\n",
        "\n",
        "> Without IoU, you can‚Äôt compute these detection-specific metrics.\n",
        "\n",
        "4. Used in Non-Maximum Suppression (NMS)\n",
        "* IoU helps suppress duplicate predictions for the same object.\n",
        "* If two boxes have high IoU, the one with lower confidence is removed.\n",
        "\n",
        "5. Supports Multi-Threshold Evaluation\n",
        "* COCO metric uses multiple IoU thresholds (e.g., 0.5, 0.55, ..., 0.95) to evaluate performance more rigorously.\n",
        "Example:\n",
        "```text\n",
        "Ground Truth:  Box A\n",
        "Prediction:    Box B\n",
        "\n",
        "Area of Overlap:  60%\n",
        "Area of Union:    100%\n",
        "\n",
        "IoU = 0.6\n",
        "``"
      ],
      "metadata": {
        "id": "wHa_gqNZ9I7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q26. What Is Faster R-CNN, and Does TFOD2 Support It?\n",
        "\n",
        "What Is Faster R-CNN?\n",
        "Faster R-CNN is a two-stage object detection model that detects objects in images with high accuracy. It improves upon R-CNN and Fast R-CNN by introducing a Region Proposal Network (RPN) for faster and more efficient region proposal generation.\n",
        "\n",
        "Architecture Overview:\n",
        "Stage 1: Region Proposal Network (RPN)\n",
        "* Scans the image using a convolutional backbone (e.g., ResNet-50).\n",
        "* Proposes candidate regions (ROIs) where objects might exist.\n",
        "Stage 2: Detection Head\n",
        "* Classifies each region and refines bounding boxes.\n",
        "* Combines classification + localization tasks.\n",
        "\n",
        "#Does TFOD2 Support Faster R-CNN?\n",
        "\n",
        "Yes, TensorFlow Object Detection API v2 (TFOD2) fully supports Faster R-CNN.\n",
        "Available Faster R-CNN Variants in TFOD2 Model Zoo:\n",
        "\n",
        "| Model Name                 | Backbone     | Notes                      |\n",
        "| -------------------------- | ------------ | -------------------------- |\n",
        "| `faster_rcnn_resnet50_v1`  | ResNet-50    | Strong performance         |\n",
        "| `faster_rcnn_resnet101_v1` | ResNet-101   | Higher accuracy, slower    |\n",
        "| `faster_rcnn_inception_v2` | Inception V2 | Lightweight, good accuracy |\n",
        "| `faster_rcnn_nas`          | NASNet       | Very high accuracy, large  |\n",
        "\n",
        "How to Use Faster R-CNN in TFOD2\n",
        "1. Download a Pretrained Faster R-CNN Model from the [TF2 Model Zoo]\n",
        "2. Edit the `pipeline.config` File\n",
        "   * Set your dataset paths, label map, number of classes\n",
        "   * Define training parameters like `batch_size`, `fine_tune_checkpoint`\n",
        "3. Train the Model\n",
        "```bash\n",
        "python model_main_tf2.py \\\n",
        "  --pipeline_config_path=path/to/pipeline.config \\\n",
        "  --model_dir=training/ \\\n",
        "  --alsologtostderr\n",
        "```\n",
        "4.Export the Trained Model for Inference"
      ],
      "metadata": {
        "id": "qzb5Lwx0-Uj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q27. How Does Detectron2 Use Pretrained Weights?\n",
        "Pretrained Weights in Detectron2 are used to:\n",
        "1. Initialize models with already-learned features (from large datasets like COCO)\n",
        "2. Transfer learning for custom tasks with limited data\n",
        "3. Reduce training time and improve performance\n",
        "\n",
        "Ways Detectron2 Uses Pretrained Weights\n",
        "1. Auto-Loading Weights from Model Zoo\n",
        "Detectron2 provides built-in Model Zoo support:\n",
        "```python\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "))\n",
        "\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        ")\n",
        "```\n",
        "\n",
        "This automatically loads COCO-pretrained weights for the specified model.\n",
        "\n",
        "2. Transfer Learning on Custom Dataset\n",
        "To fine-tune on your own dataset:\n",
        "```python\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = <your_num_classes>\n",
        "cfg.MODEL.WEIGHTS = \"path/to/model_final.pth\"  # Pretrained COCO model\n",
        "cfg.SOLVER.MAX_ITER = 5000\n",
        "```\n",
        "Detectron2 reuses backbone features (like ResNet), and retrains detection/classification heads.\n",
        "\n",
        "3.Continue Training from Custom Checkpoint\n",
        "You can also load your own pretrained checkpoint:\n",
        "```python\n",
        "cfg.MODEL.WEIGHTS = \"output/my_model/model_final.pth\"\n",
        "```\n",
        "4. Change Backbone While Retaining Pretrained Weights\n",
        "You can switch to a different backbone:\n",
        "```python\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\n",
        "    \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "s3-rpZPd-1Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q28. What File Format Is Typically Used to Store Training Data in TFOD2 (TensorFlow Object Detection API v2)?\n",
        "\n",
        "`TFRecord` (.record) is the standard file format used in TFOD2 to store training and validation data."
      ],
      "metadata": {
        "id": "yvXePVV0_WoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q29. What Is the Difference Between Semantic Segmentation and Instance Segmentation?\n",
        "\n",
        "Both semantic segmentation and instance segmentation are tasks in computer vision that involve pixel-level understanding of images‚Äîbut they differ in how they label objects.\n",
        "Key Differences:\n",
        "| Feature                | Semantic Segmentation  | Instance Segmentation                   |\n",
        "| -----------------------| ---------------------- | --------------------------------------- |\n",
        "|  Pixel-level classes   | Yes                    | Yes                                     |\n",
        "|  Distinguish instances |  No                    | Yes                                     |\n",
        "|  Output type           | Single class per pixel | Class + instance ID per pixel           |\n",
        "|  Complexity            | Lower                  | Higher (needs detection + segmentation) |\n",
        "|  Example model         | DeepLab, U-Net         | Mask R-CNN, Detectron2                  |\n",
        "\n",
        "#Visualization:\n",
        "| Task                  | Visual Output Example                |\n",
        "| --------------------- | ------------------------------------ |\n",
        "| Semantic Segmentation | All cars are one color (class-level) |\n",
        "| Instance Segmentation | Each car is a different color/mask   |\n",
        "\n",
        "\n",
        "| Task                  | Labels Pixels  | Distinguishes Objects   | Example Use                         |\n",
        "| --------------------- | -------------  | ---------------------   | --------------------------------    |\n",
        "| Semantic Segmentation | ‚úÖ             | ‚ùå                     | Road/lane detection\n",
        "| Instance Segmentation | ‚úÖ             | ‚úÖ                     | Counting people, object tracking    |\n"
      ],
      "metadata": {
        "id": "9emoql3F_umL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q30. Can Detectron2 Detect Custom Classes During Inference?\n",
        "Yes, Detectron2 can detect custom classes during inference ‚Äî but only if the model was trained or fine-tuned on those custom classes.\n",
        "\n",
        "What‚Äôs Required to Detect Custom Classes in Detectron2?\n",
        "To detect your own custom classes (like `helmet`, `gloves`, `machinery`, etc.), you must:\n",
        "1. Train or Fine-tune the Model on Your Custom Dataset\n",
        "Detectron2 must learn the custom classes via training or fine-tuning.\n",
        "You cannot use a pretrained COCO model directly unless your classes match the COCO classes.\n",
        "\n",
        "2. Define Your Custom Class Names\n",
        "```python\n",
        "MetadataCatalog.get(\"your_dataset\").thing_classes = [\"helmet\", \"gloves\", \"vest\"]\n",
        "```\n",
        "This tells Detectron2 what to display during inference.\n",
        "\n",
        "3. Use the Trained Custom Model for Inference\n",
        "Example:\n",
        "```python\n",
        "cfg.MODEL.WEIGHTS = \"output/custom_model/model_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # for 3 custom classes\n",
        "cfg.MODEL.DEVICE = \"cpu\"  # or \"cuda\"\n",
        "```\n",
        "Then perform inference using the `DefaultPredictor`.\n",
        "\n",
        "4.Visualize Predictions with Custom Labels\n",
        "```python\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "metadata = MetadataCatalog.get(\"your_dataset\")\n",
        "visualizer = Visualizer(image[:, :, ::-1], metadata=metadata)\n",
        "out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "```"
      ],
      "metadata": {
        "id": "ZmehFAA3AMu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q31. Why Is `pipeline.config` Essential in TFOD2 (TensorFlow Object Detection API v2)?\n",
        "What is `pipeline.config`?\n",
        "`pipeline.config` is a configuration file in Protocol Buffers (.config) format that defines the entire training, evaluation, and model architecture pipeline for TFOD2.\n",
        "It is the central control file for your object detection project.\n",
        "\n",
        "Why Is It Essential?\n",
        "1. Defines the Model Architecture\n",
        "```protobuf\n",
        "model {\n",
        "  faster_rcnn {\n",
        "    num_classes: 3\n",
        "    ...\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "* Specifies what type of model to use (e.g., Faster R-CNN, SSD, EfficientDet)\n",
        "* Sets the number of output classes\n",
        "\n",
        "2. Sets Input Data Sources\n",
        "```protobuf\n",
        "train_input_reader {\n",
        "  label_map_path: \"annotations/label_map.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"annotations/train.record\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "3. Controls Training Parameters\n",
        "```protobuf\n",
        "train_config {\n",
        "  batch_size: 8\n",
        "  fine_tune_checkpoint: \"pretrained/model.ckpt\"\n",
        "  num_steps: 10000\n",
        "  ...\n",
        "}\n",
        "* Batch size\n",
        "* Number of training steps\n",
        "* Learning rate and optimizer\n",
        "* Checkpoint path for transfer learning\n",
        "\n",
        "4. Manages Data Augmentation & Preprocessing\n",
        "```protobuf\n",
        "data_augmentation_options {\n",
        "  random_horizontal_flip {\n",
        "  }\n",
        "}\n",
        "```\n",
        "* Built-in options like flipping, scaling, cropping\n",
        "* Important for boosting model generalization\n",
        "\n",
        "5. Controls Evaluation Behavior\n",
        "```protobuf\n",
        "eval_config {\n",
        "  metrics_set: \"coco_detection_metrics\"\n",
        "  num_examples: 500\n",
        "}\n",
        "```\n",
        "* Specifies evaluation metrics (like mAP, IoU)\n",
        "* Sets how often and how many examples to evaluate\n",
        "\n",
        "6. Handles Export & Deployment Settings\n",
        "During model export, `pipeline.config` is reused to ensure:\n",
        "* Consistent architecture\n",
        "* Consistent label and input size\n",
        "* Compatibility with TF Lite / TensorFlow Serving\n"
      ],
      "metadata": {
        "id": "QE1oVGmoAkct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q33. What Happens If the Learning Rate Is Too High During Training?\n",
        "A too-high learning rate can cause the model to diverge, overshoot minima, or fail to converge ‚Äî resulting in poor training performance.\n"
      ],
      "metadata": {
        "id": "Hh7_5VvjBEpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q34. What Is COCO JSON Format?\n",
        "COCO JSON Format is a standardized annotation format used for image datasets in object detection, segmentation, and keypoint detection tasks. It originates from the COCO dataset (Common Objects in Context) and is widely supported by libraries like Detectron2, MMDetection, and YOLOv8 (via converters).\n",
        "\n",
        "What Does the COCO JSON Contain?\n",
        "The JSON file describes everything about your annotated dataset.\n"
      ],
      "metadata": {
        "id": "QwTOE2mxGaKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q35. Why Is TensorFlow Lite Compatibility Important in TFOD2?\n",
        "\n",
        "TensorFlow Lite (TFLite) is a lightweight version of TensorFlow designed to run machine learning models on Edge devices such as:\n",
        "* Mobile phones (Android/iOS)\n",
        "* Embedded systems (e.g., Raspberry Pi, NVIDIA Jetson)\n",
        "* IoT devices\n",
        "* Microcontrollers\n",
        "\n",
        "Why Is Compatibility with TensorFlow Lite Important in TFOD2?\n",
        "\n",
        "1.Enables Deployment on Edge Devices\n",
        "TFLite-compatible models from TFOD2 can be:\n",
        "* Deployed offline on phones or embedded hardware\n",
        "* Run without internet access or cloud infrastructure\n",
        "* Used in real-time scenarios like mobile apps or surveillance\n",
        "\n",
        "2.Reduces Model Size and Latency\n",
        "* TFLite uses quantization and model optimization to shrink size\n",
        "* Speeds up inference dramatically\n",
        "* Makes models suitable for battery-powered devices\n",
        "| Format     | Typical Size | Inference Speed          |\n",
        "| ---------- | ------------ | ------------------------ |\n",
        "| SavedModel | Large        | Slower on mobile         |\n",
        "| TFLite     | Small (MBs)  | Fast, hardware-optimized |\n",
        "\n",
        "3. TFOD2 Includes TFLite-Optimized Models\n",
        "TFOD2 supports special TFLite-optimized models, such as:\n",
        "| Model Name                 | Description                   |\n",
        "| -------------------------- | ----------------------------- |\n",
        "| `ssd_mobilenet_v2_fpnlite` | Lightweight SSD for TFLite    |\n",
        "| `efficientdet_lite0`       | Efficient and mobile-friendly |\n",
        "These are trained and exported specifically with TFLite in mind.\n",
        "\n",
        "4.Exporting to TFLite is Simple with TFOD2\n",
        "You can export a trained TFOD2 model to `.tflite` using:\n",
        "```bash\n",
        "python export_tflite_graph_tf2.py \\\n",
        "  --pipeline_config_path pipeline.config \\\n",
        "  --trained_checkpoint_dir checkpoint/ \\\n",
        "  --output_directory tflite_model/\n",
        "```\n",
        "Then convert the saved model to TFLite with the TensorFlow Lite Converter.\n",
        "\n",
        "5.Runs on Millions of Devices Globally\n",
        "TensorFlow Lite compatibility allows you to:\n",
        "* Build cross-platform AI apps\n",
        "* Scale your solution globally\n",
        "* Avoid server-side latency and cost"
      ],
      "metadata": {
        "id": "QyHYuI8kGtL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "oR4iewN6HMaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q1: How Do You Install Detectron2 Using `pip` and Check Its Version?\n",
        "\n",
        "Step 1: Install Required Dependencies\n",
        "Before installing Detectron2, ensure:\n",
        "* You have Python ‚â• 3.8\n",
        "* PyTorch is installed (compatible with your CUDA version, if using GPU)\n",
        "\n",
        "Step 2: Install Detectron2 via pip\n",
        "```bash\n",
        "pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.2/index.html\n",
        "```\n",
        "> Replace `cu118` and `torch2.2` in the URL with:\n",
        "* Your PyTorch version (e.g., `torch2.0`)\n",
        "* Your CUDA version (e.g., `cu117`, `cu113`, or `cpu`)\n",
        "\n",
        "Step 3: Verify Installation & Check Version\n",
        "Run the following in a Python terminal or script:\n",
        "```pyThon\n",
        "import detectron2\n",
        "print(detectron2.__version__)\n",
        "```\n",
        "\n",
        "You should see output like:\n",
        "\n",
        "```\n",
        "0.6\n",
        "`\n",
        "\n",
        "| Task                   | Command or Code                               |\n",
        "| -----------------------| --------------------------------------------- |\n",
        "|  Install PyTorch       | `pip install torch torchvision ...`           |\n",
        "|  Install Detectron2    | `pip install detectron2 -f ...`               |\n",
        "|  Check version         | `print(detectron2.__version__)`               |\n",
        "|  Validate installation | Import model zoo & config to ensure no errors |"
      ],
      "metadata": {
        "id": "S-74mIIFHOA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2. How Do You Perform Inference with Detectron2 Using an Online Image?\n",
        "Here‚Äôs a step-by-step guide to download an image from a URL and run object detection inference on it using Detectron2.\n",
        "\n",
        "Step-by-Step Code\n",
        "```python\n",
        "# Step 1: Import Required Libraries\n",
        "import cv2\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Detectron2 core\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2 import model_zoo\n",
        "```\n",
        "Step 2: Load an Image from a URL\n",
        "```python\n",
        "# Load online image\n",
        "image_url = \"https://images.pexels.com/photos/45170/kittens-cat-cat-puppy-rush-45170.jpeg\"\n",
        "response = requests.get(image_url)\n",
        "image_np = np.array(Image.open(BytesIO(response.content)).convert(\"RGB\"))\n",
        "```\n",
        "Step 3: Load Pretrained Model\n",
        "```python\n",
        "# Set up config and load model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Confidence threshold\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        ")\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "```\n",
        "Step 4: Run Inference\n",
        "```python\n",
        "# Inference\n",
        "outputs = predictor(image_np)\n",
        "```\n",
        "Step 5: Visualize Results\n",
        "```python\n",
        "# Visualize predictions\n",
        "v = Visualizer(image_np[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(out.get_image()[:, :, ::-1])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Detectron2 Inference on Online Image\")\n",
        "plt.show()\n",
        "```\n",
        "#Output\n",
        "\n",
        "* Detected objects will be shown with bounding boxes and class labels.\n",
        "* You can adjust the confidence threshold via `cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST`."
      ],
      "metadata": {
        "id": "IO03MSUeHoo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3. How Do You Visualize Evaluation Metrics in Detectron2, Such as Training Loss?\n",
        "\n",
        "Detectron2 does not plot metrics automatically, but it logs them using its built-in `EventStorage` and `TensorBoard`-compatible logging.\n",
        "You can visualize metrics like:\n",
        "* Training loss\n",
        "* Classification loss\n",
        "* Box regression loss\n",
        "* Learning rate\n",
        "* Validation mAP (if evaluation is configured)\n"
      ],
      "metadata": {
        "id": "JEz18kq7IDz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q4. How Do You Run Inference with TFOD2 (TensorFlow Object Detection API v2) on an Online Image?\n",
        "\n",
        "You can use a pre-trained TFOD2 model (e.g., SSD MobileNet or Faster R-CNN) to perform object detection on an image from a URL.\n",
        "\n",
        "Step-by-Step Code to Run Inference on an Online Image\n",
        "\n",
        "Make sure you have installed:\n",
        "\n",
        "```bash\n",
        "pip install tensorflow opencv-python\n",
        "```\n",
        "1. Load Pretrained TFOD2 Model from SavedModel Directory\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the pretrained saved_model\n",
        "detect_fn = tf.saved_model.load(\"PATH_TO_SAVED_MODEL\")  # e.g., 'ssd_mobilenet/saved_model'\n",
        "```\n",
        "2. Download Online Image\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "3. Prepare Image for Inference\n",
        "```python\n",
        "# Add batch dimension and convert to tensor\n",
        "input_tensor = tf.convert_to_tensor(image_np)\n",
        "input_tensor = input_tensor[tf.newaxis, ...]  # shape: [1, H, W, 3]\n",
        "```\n",
        "4. Run Inference\n",
        "```python\n",
        "# Run model\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "# Extract boxes, classes, and scores\n",
        "boxes = detections['detection_boxes'][0].numpy()\n",
        "classes = detections['detection_classes'][0].numpy().astype(int)\n",
        "scores = detections['detection_scores'][0].numpy()\n",
        "```\n",
        "5. Visualize Predictions\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from object_detection.utils import label_map_util, visualization_utils as viz_utils\n",
        "\n",
        "# Load label map (path to .pbtxt file used during training)\n",
        "category_index = label_map_util.create_category_index_from_labelmap(\"PATH_TO_LABEL_MAP.pbtxt\")\n",
        "\n",
        "# Visualize\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np,\n",
        "    boxes,\n",
        "    classes,\n",
        "    scores,\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    max_boxes_to_draw=20,\n",
        "    min_score_thresh=0.5,\n",
        "    agnostic_mode=False\n",
        ")\n",
        "\n",
        "# Display image\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image_np)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"TFOD2 Inference on Online Image\")\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "CoXrerycIgI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q5. How Do You Install TensorFlow Object Detection API (TFOD2) in Jupyter Notebook?\n",
        "\n",
        "The TensorFlow Object Detection API (TFOD2) is not directly available via `pip install` ‚Äî it needs to be cloned, built, and installed manually.\n",
        "Here‚Äôs a step-by-step setup process that works inside a Jupyter Notebook environment (e.g., Colab, local Jupyter, or VS Code).\n",
        "\n",
        "Step 1: Install Dependencies\n",
        "Run this in a Jupyter cell:\n",
        "\n",
        "```python\n",
        "!pip install tensorflow opencv-python pillow lxml Cython contextlib2 \\\n",
        "    matplotlib pandas tf-slim pycocotools\n",
        "```\n",
        "Step 2: Clone the TFOD GitHub Repository\n",
        "```python\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "```\n",
        "Step 3: Compile Protobufs\n",
        "```python\n",
        "%cd models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "```\n",
        "If `protoc` is not installed:\n",
        "```python\n",
        "!apt-get install protobuf-compiler\n",
        "```\n",
        "\n",
        "Step 4: Install TFOD2 as a Package\n",
        "```python\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!pip install .\n",
        "```\n",
        "Step 5: Test the Installation\n",
        "```python\n",
        "!python object_detection/builders/model_builder_tf2_test.py\n",
        "\n",
        "Step 6: Add TFOD to PYTHONPATH\n",
        "```python\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models:/content/models/research:/content/models/research/slim'\n",
        "```\n",
        "\n",
        "Replace `/content` with your notebook‚Äôs root directory if running locally.\n"
      ],
      "metadata": {
        "id": "iGdHmMs4I8lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q6. How Can You Load a Pretrained TensorFlow Object Detection Model (TFOD2)?\n",
        "TensorFlow Object Detection API (TFOD2) provides a simple way to load and run inference with pre-trained models using the `SavedModel` format.\n",
        "You can load models like:\n",
        "\n",
        "* `ssd_mobilenet_v2_fpnlite`\n",
        "* `faster_rcnn_resnet50`\n",
        "* `efficientdet_d0`\n",
        "\n",
        "These are typically downloaded from the TensorFlow 2 Detection Model Zoo.\n",
        "\n",
        "Step-by-Step Guide to Load a Pretrained TFOD2 Model\n",
        "\n",
        "1. Download a Pretrained Model\n",
        "2. Locate the `saved_model` Directory\n",
        "3. Load the SavedModel in Python\n",
        "4. Run Inference (Optional Test)\n",
        "\n",
        "```python\n",
        "# Example input image (NumPy array with shape [H, W, 3])\n",
        "import numpy as np\n",
        "image_np = np.random.randint(0, 255, (320, 320, 3), dtype=np.uint8)\n",
        "\n",
        "# Prepare image tensor\n",
        "input_tensor = tf.convert_to_tensor(image_np)[tf.newaxis, ...]\n",
        "\n",
        "# Run detection\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "# Print keys in output dictionary\n",
        "print(detections.keys())\n",
        "```\n"
      ],
      "metadata": {
        "id": "YxTeAA23JZrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q7. How Do You Preprocess an Image from the Web for TFOD2 Inference?\n",
        "#To perform inference using the TensorFlow Object Detection API (TFOD2), the image from the web must be:\n",
        "1. Downloaded and decoded\n",
        "2. Converted into a NumPy array\n",
        "3. Transformed into a 4D Tensor (`[1, height, width, 3]`)\n",
        "4. Pixel values in range `[0, 255]` and type `uint8`\n",
        "\n",
        "Step-by-Step Code to Preprocess a Web Image\n",
        "# Step 1: Load image from URL\n",
        "image_url = \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\"\n",
        "response = requests.get(image_url)\n",
        "image = Image.open(BytesIO(response.content)).convert(\"RGB\")  # Ensure 3 channels\n",
        "\n",
        "# Step 2: Convert to NumPy array\n",
        "image_np = np.array(image)\n",
        "\n",
        "# Step 3: Expand dimensions to match TFOD2 input shape: [1, height, width, 3]\n",
        "input_tensor = tf.convert_to_tensor(image_np)\n",
        "input_tensor = input_tensor[tf.newaxis, ...]  # Add batch dimension\n",
        "\n",
        "# (Optional) Check input shape\n",
        "print(\"Preprocessed shape:\", input_tensor.shape)\n",
        "```\n",
        "### Important Notes for TFOD2\n",
        "\n",
        "| Requirement          | Ensured by                                     |\n",
        "| -------------------- | ---------------------------------------------- |\n",
        "| Shape `[1, H, W, 3]` | `input_tensor = input_tensor[tf.newaxis, ...]` |\n",
        "| Data type: `uint8`   | `np.array(image)` from PIL handles this        |\n",
        "| RGB format           | `.convert(\"RGB\")` ensures 3 channels           |\n",
        "\n",
        "Use with Inference\n",
        "\n",
        "Once the image is preprocessed:\n",
        "\n",
        "```python\n",
        "# Load model\n",
        "detect_fn = tf.saved_model.load(\"path_to/saved_model\")\n",
        "\n",
        "# Run inference\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "# Example: View detection scores\n",
        "print(detections['detection_scores'][0].numpy())"
      ],
      "metadata": {
        "id": "V1VaN1ghJ-cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q8. How Do You Visualize Bounding Boxes for Detected Objects in TFOD2 Inference?\n",
        "After you run inference using a TensorFlow Object Detection (TFOD2) model, the output includes:\n",
        "* `detection_boxes` (normalized bounding box coordinates)\n",
        "* `detection_classes` (class IDs)\n",
        "* `detection_scores` (confidence scores)\n",
        "\n",
        "Step-by-Step: Full Code to Visualize TFOD2 Detections\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import requests\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "\n",
        "# TFOD2 visualization utilities\n",
        "from object_detection.utils import label_map_util, visualization_utils as viz_utils\n",
        "1. Load Image from Web\n",
        "```python\n",
        "image_url = \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\"\n",
        "response = requests.get(image_url)\n",
        "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "image_np = np.array(image)\n",
        "```\n",
        "2. Prepare Input Tensor and Run Inference\n",
        "```python\n",
        "# Expand dims and convert\n",
        "input_tensor = tf.convert_to_tensor(image_np)[tf.newaxis, ...]\n",
        "\n",
        "# Load model\n",
        "detect_fn = tf.saved_model.load(\"path_to_model/saved_model\")\n",
        "\n",
        "# Inference\n",
        "detections = detect_fn(input_tensor)\n",
        "```\n",
        "3. Load Label Map (.pbtxt)\n",
        "```python\n",
        "category_index = label_map_util.create_category_index_from_labelmap(\n",
        "    \"path_to/label_map.pbtxt\"\n",
        "\n",
        "4. Visualize Detections with Bounding Boxes\n",
        "```python\n",
        "# Remove batch dimension\n",
        "image_np_with_detections = image_np.copy()\n",
        "# Draw boxes\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np_with_detections,\n",
        "    detections['detection_boxes'][0].numpy(),\n",
        "    detections['detection_classes'][0].numpy().astype(np.int32),\n",
        "    detections['detection_scores'][0].numpy(),\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    max_boxes_to_draw=20,\n",
        "    min_score_thresh=0.5\n",
        ")\n",
        "\n",
        "5. Display Image\n",
        "```python\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"TFOD2 Inference with Bounding Boxes\")\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "UQEaBXeSKWdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q9. How Do You Define Classes for Custom Training in TensorFlow Object Detection API (TFOD2)?\n",
        "\n",
        "To train a custom object detection model using TFOD2, you must define your custom classes in a file called a label map (`.pbtxt`). This file tells the model what each class ID represents.\n",
        "\n",
        "Step-by-Step Guide to Define Classes\n",
        "Step 1: Create a Label Map File (`label_map.pbtxt`)\n",
        "Create a plain text file with the following format:\n",
        "\n",
        "```\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'apple'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'banana'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 3\n",
        "  name: 'grape'\n",
        "}\n",
        "```\n",
        "Step 2: Reference Label Map in `pipeline.config`\n",
        "In your model‚Äôs `pipeline.config` file, find this line:\n",
        "```bash\n",
        "label_map_path: \"training/label_map.pbtxt\"\n",
        "```\n",
        "Make sure it matches the path to your `.pbtxt` file.\n",
        "\n",
        "Step 3: During Inference or Evaluation\n",
        "Use it like this in Python:\n",
        "```python\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "# Load label map\n",
        "category_index = label_map_util.create_category_index_from_labelmap(\n",
        "    'training/label_map.pbtxt'\n",
        ")\n",
        "\n",
        "print(category_index)\n",
        "```"
      ],
      "metadata": {
        "id": "jmnLDvzuK1-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q10. How Do You Define Classes for Custom Training in TFOD2 (TensorFlow Object Detection API v2)?\n",
        "To train a custom object detection model using TFOD2, you must define your own classes in a label map file (`.pbtxt`). This file tells the model what object categories to detect.\n",
        "\n",
        "Step-by-Step: Define Classes for Custom Training\n",
        "\n",
        "Step 1: Create `label_map.pbtxt` File\n",
        "The label map file defines each class with a unique ID and name in the following format:\n",
        "\n",
        "```plaintext\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'helmet'\n",
        "}\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'vest'\n",
        "}\n",
        "item {\n",
        "  id: 3\n",
        "  name: 'boots'\n",
        "}\n",
        "\n",
        "Step 2: Save the File\n",
        "Save the file as:\n",
        "```bash\n",
        "label_map.pbtxt\n",
        "```\n",
        "Place it in your training folder or any accessible directory:\n",
        "```\n",
        "<project_root>/annotations/label_map.pbtxt\n",
        "```\n",
        "\n",
        "Step 3: Reference It in Your `pipeline.config`\n",
        "Edit your `pipeline.config` file and locate:\n",
        "```protobuf\n",
        "label_map_path: \"annotations/label_map.pbtxt\"\n",
        "```\n",
        "Ensure the path correctly points to your label map file.\n",
        "\n",
        "Step 4: Load the Label Map in Code (During Inference or Evaluation)\n",
        "```python\n",
        "from object_detection.utils import label_map_util\n",
        "label_map_path = 'annotations/label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(\n",
        "    label_map_path, use_display_name=True\n",
        ")\n",
        "\n",
        "print(category_index)\n",
        "```\n",
        "\n",
        "This will give a dictionary like:\n",
        "\n",
        "```python\n",
        "{1: {'id': 1, 'name': 'helmet'}, 2: {'id': 2, 'name': 'vest'}}\n"
      ],
      "metadata": {
        "id": "-J6PPXz7LbWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q11. How Do You Resize an Image Before Detecting Objects in TFOD2?\n",
        "\n",
        "Resizing an image before passing it to a TensorFlow Object Detection (TFOD2) model can help:\n",
        "* Match the input shape expected by the model (e.g., 320x320, 640x640)\n",
        "* Speed up inference\n",
        "* Control memory usage\n",
        "\n",
        "Step-by-Step: Resize an Image for TFOD2 Inference\n",
        "\n",
        "1.Import Required Libraries\n",
        "```python\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "```\n",
        "2. Open and Resize the Image\n",
        "```python\n",
        "# Load image from file (or use a URL + requests + BytesIO)\n",
        "image = Image.open(\"your_image.jpg\").convert(\"RGB\")\n",
        "\n",
        "# Resize image (example: 320x320 for SSD MobileNet)\n",
        "resized_image = image.resize((320, 320))\n",
        "```\n",
        "\n",
        "> üìù Replace `(320, 320)` with the target size required by your model.\n",
        "\n",
        "3. Convert to Tensor for Inference\n",
        "```python\n",
        "# Convert to NumPy array\n",
        "image_np = np.array(resized_image)\n",
        "\n",
        "# Add batch dimension and convert to tensor\n",
        "input_tensor = tf.convert_to_tensor(image_np)\n",
        "input_tensor = input_tensor[tf.newaxis, ...]  # shape: [1, height, width, 3]\n",
        "```\n",
        "4. Run Inference\n",
        "```python\n",
        "# Load the saved TFOD2 model\n",
        "detect_fn = tf.saved_model.load(\"path_to/saved_model\")\n",
        "\n",
        "# Perform detection\n",
        "detections = detect_fn(input_tensor)\n",
        "```"
      ],
      "metadata": {
        "id": "GuQKXEa2MDn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q12. How Can You Apply a Color Filter to an Image?\n",
        "Applying a color filter means enhancing or modifying specific color channels (Red, Green, Blue) to create a visual effect or highlight features.\n",
        "You can do this easily using OpenCV or Pillow in Python.\n",
        "\n",
        "Method 1: Using OpenCV (Recommended for Image Processing)\n",
        "#Example: Apply a Red Filter\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread(\"your_image.jpg\")   # Loads in BGR format\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "# Create red filter (keep only R channel)\n",
        "red_filter = image.copy()\n",
        "red_filter[:, :, 1] = 0  # Set G channel to 0\n",
        "red_filter[:, :, 2] = 0  # Set B channel to 0\n",
        "\n",
        "# Show filtered image\n",
        "plt.imshow(red_filter)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Red Filter Applied\")\n",
        "plt.show()\n",
        "\n",
        "Method 2: Using PIL (Pillow)\n",
        "```python\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "# Load image\n",
        "image = Image.open(\"your_image.jpg\").convert(\"RGB\")\n",
        "\n",
        "# Split channels\n",
        "r, g, b = image.split()\n",
        "\n",
        "# Enhance red channel\n",
        "r = r.point(lambda i: i * 1.5)  # Increase intensity\n",
        "\n",
        "# Merge back with original green and blue\n",
        "filtered_image = Image.merge(\"RGB\", (r, g, b))\n",
        "\n",
        "# Show image\n",
        "filtered_image.show()\n",
        "```"
      ],
      "metadata": {
        "id": "IORr2UpnMHtp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}